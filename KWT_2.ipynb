{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkat23-hub/VenkatKWT/blob/main/KWT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReYUN5MYj_T5",
        "outputId": "7cd23601-56de-46ee-fb20-aaae7e3a7e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "PL4WBgkukH8a",
        "outputId": "49fb79e6-5dc8-4beb-8c7f-4181bb0c0b23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio librosa numpy tqdm pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlMh_09IkpFo",
        "outputId": "cc7028cc-af90-4e5b-9ef4-ede42f3eed50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset found!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define dataset path\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/speech_commands_dataset\"\n",
        "\n",
        "# Check if dataset exists\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(\"✅ Dataset found!\")\n",
        "else:\n",
        "    print(\"❌ Dataset not found. Check the path!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LokXuEJlkRs",
        "outputId": "4dc0ed37-9b58-4c4d-bd80-e08e3a565638"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train files: 30\n",
            "Validation files: 30\n",
            "Test files: 30\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check if preprocessing was successful\n",
        "print(\"Train files:\", len(os.listdir(\"/content/drive/MyDrive/Keyword_spotting_transformer/data/train/\")))\n",
        "print(\"Validation files:\", len(os.listdir(\"/content/drive/MyDrive/Keyword_spotting_transformer/data/valid\")))\n",
        "print(\"Test files:\", len(os.listdir(\"/content/drive/MyDrive/Keyword_spotting_transformer/data/test\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yWYcfIflrQT",
        "outputId": "8d7315c6-cb0b-4fdc-dceb-97940e7de552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train augmented : 51318\n",
            "Valid sugmented : 6798\n",
            "Test augmented: 6835\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/data\"\n",
        "\n",
        "print(\"Train augmented :\", len(os.listdir(f\"{DATA_PATH}/processed_train_augmented_exercise_bike\")))\n",
        "print(\"Valid sugmented :\", len(os.listdir(f\"{DATA_PATH}/processed_valid_augmented_exercise_bike\")))\n",
        "print(\"Test augmented:\", len(os.listdir(f\"{DATA_PATH}/processed_test_augmented_exercise_bike\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vGYqOdBqmXY",
        "outputId": "1f68c4e2-2c1e-4669-dbe7-4a35ea261052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X9oMKhl_qzSh",
        "outputId": "5e901b7b-c84f-43d7-fe5f-fc292a2fc320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  Batch 2020/3208 | Loss: 1.9157\n",
            "  Batch 2030/3208 | Loss: 2.1002\n",
            "  Batch 2040/3208 | Loss: 2.5725\n",
            "  Batch 2050/3208 | Loss: 2.4860\n",
            "  Batch 2060/3208 | Loss: 2.0282\n",
            "  Batch 2070/3208 | Loss: 2.0279\n",
            "  Batch 2080/3208 | Loss: 1.5339\n",
            "  Batch 2090/3208 | Loss: 1.6622\n",
            "  Batch 2100/3208 | Loss: 1.8719\n",
            "  Batch 2110/3208 | Loss: 2.2075\n",
            "  Batch 2120/3208 | Loss: 1.8760\n",
            "  Batch 2130/3208 | Loss: 1.7333\n",
            "  Batch 2140/3208 | Loss: 1.9376\n",
            "  Batch 2150/3208 | Loss: 2.2111\n",
            "  Batch 2160/3208 | Loss: 1.4735\n",
            "  Batch 2170/3208 | Loss: 2.0551\n",
            "  Batch 2180/3208 | Loss: 2.1155\n",
            "  Batch 2190/3208 | Loss: 2.2156\n",
            "  Batch 2200/3208 | Loss: 1.4753\n",
            "  Batch 2210/3208 | Loss: 2.4756\n",
            "  Batch 2220/3208 | Loss: 2.1923\n",
            "  Batch 2230/3208 | Loss: 1.8062\n",
            "  Batch 2240/3208 | Loss: 2.2184\n",
            "  Batch 2250/3208 | Loss: 2.2011\n",
            "  Batch 2260/3208 | Loss: 2.2556\n",
            "  Batch 2270/3208 | Loss: 2.0609\n",
            "  Batch 2280/3208 | Loss: 2.0938\n",
            "  Batch 2290/3208 | Loss: 1.7200\n",
            "  Batch 2300/3208 | Loss: 1.7878\n",
            "  Batch 2310/3208 | Loss: 2.2899\n",
            "  Batch 2320/3208 | Loss: 1.9238\n",
            "  Batch 2330/3208 | Loss: 2.1126\n",
            "  Batch 2340/3208 | Loss: 1.7278\n",
            "  Batch 2350/3208 | Loss: 2.3110\n",
            "  Batch 2360/3208 | Loss: 1.9046\n",
            "  Batch 2370/3208 | Loss: 1.8752\n",
            "  Batch 2380/3208 | Loss: 1.7990\n",
            "  Batch 2390/3208 | Loss: 1.9267\n",
            "  Batch 2400/3208 | Loss: 1.7387\n",
            "  Batch 2410/3208 | Loss: 2.2300\n",
            "  Batch 2420/3208 | Loss: 1.9481\n",
            "  Batch 2430/3208 | Loss: 1.6834\n",
            "  Batch 2440/3208 | Loss: 1.8542\n",
            "  Batch 2450/3208 | Loss: 1.7546\n",
            "  Batch 2460/3208 | Loss: 1.8657\n",
            "  Batch 2470/3208 | Loss: 2.2089\n",
            "  Batch 2480/3208 | Loss: 1.7392\n",
            "  Batch 2490/3208 | Loss: 1.5179\n",
            "  Batch 2500/3208 | Loss: 2.0618\n",
            "  Batch 2510/3208 | Loss: 2.1015\n",
            "  Batch 2520/3208 | Loss: 1.8141\n",
            "  Batch 2530/3208 | Loss: 2.2749\n",
            "  Batch 2540/3208 | Loss: 1.7671\n",
            "  Batch 2550/3208 | Loss: 1.7866\n",
            "  Batch 2560/3208 | Loss: 2.3043\n",
            "  Batch 2570/3208 | Loss: 2.0255\n",
            "  Batch 2580/3208 | Loss: 1.9843\n",
            "  Batch 2590/3208 | Loss: 2.0354\n",
            "  Batch 2600/3208 | Loss: 2.2271\n",
            "  Batch 2610/3208 | Loss: 2.2099\n",
            "  Batch 2620/3208 | Loss: 1.9371\n",
            "  Batch 2630/3208 | Loss: 2.3612\n",
            "  Batch 2640/3208 | Loss: 2.2797\n",
            "  Batch 2650/3208 | Loss: 2.3625\n",
            "  Batch 2660/3208 | Loss: 1.6591\n",
            "  Batch 2670/3208 | Loss: 2.2003\n",
            "  Batch 2680/3208 | Loss: 2.1323\n",
            "  Batch 2690/3208 | Loss: 1.9376\n",
            "  Batch 2700/3208 | Loss: 2.0739\n",
            "  Batch 2710/3208 | Loss: 1.9578\n",
            "  Batch 2720/3208 | Loss: 2.3897\n",
            "  Batch 2730/3208 | Loss: 2.0072\n",
            "  Batch 2740/3208 | Loss: 1.6426\n",
            "  Batch 2750/3208 | Loss: 2.0728\n",
            "  Batch 2760/3208 | Loss: 2.5487\n",
            "  Batch 2770/3208 | Loss: 2.1743\n",
            "  Batch 2780/3208 | Loss: 2.1160\n",
            "  Batch 2790/3208 | Loss: 1.7026\n",
            "  Batch 2800/3208 | Loss: 2.0383\n",
            "  Batch 2810/3208 | Loss: 2.0123\n",
            "  Batch 2820/3208 | Loss: 1.9088\n",
            "  Batch 2830/3208 | Loss: 2.0487\n",
            "  Batch 2840/3208 | Loss: 2.0072\n",
            "  Batch 2850/3208 | Loss: 2.0524\n",
            "  Batch 2860/3208 | Loss: 1.9198\n",
            "  Batch 2870/3208 | Loss: 1.6689\n",
            "  Batch 2880/3208 | Loss: 1.9588\n",
            "  Batch 2890/3208 | Loss: 2.4214\n",
            "  Batch 2900/3208 | Loss: 1.4794\n",
            "  Batch 2910/3208 | Loss: 1.9178\n",
            "  Batch 2920/3208 | Loss: 1.8100\n",
            "  Batch 2930/3208 | Loss: 1.7104\n",
            "  Batch 2940/3208 | Loss: 2.0705\n",
            "  Batch 2950/3208 | Loss: 2.2425\n",
            "  Batch 2960/3208 | Loss: 2.2091\n",
            "  Batch 2970/3208 | Loss: 1.8676\n",
            "  Batch 2980/3208 | Loss: 1.6870\n",
            "  Batch 2990/3208 | Loss: 2.1928\n",
            "  Batch 3000/3208 | Loss: 1.7667\n",
            "  Batch 3010/3208 | Loss: 2.0535\n",
            "  Batch 3020/3208 | Loss: 1.7902\n",
            "  Batch 3030/3208 | Loss: 2.5253\n",
            "  Batch 3040/3208 | Loss: 1.9482\n",
            "  Batch 3050/3208 | Loss: 1.9574\n",
            "  Batch 3060/3208 | Loss: 1.9864\n",
            "  Batch 3070/3208 | Loss: 2.0980\n",
            "  Batch 3080/3208 | Loss: 2.0753\n",
            "  Batch 3090/3208 | Loss: 2.2436\n",
            "  Batch 3100/3208 | Loss: 1.5930\n",
            "  Batch 3110/3208 | Loss: 2.0298\n",
            "  Batch 3120/3208 | Loss: 1.9777\n",
            "  Batch 3130/3208 | Loss: 1.6893\n",
            "  Batch 3140/3208 | Loss: 2.2048\n",
            "  Batch 3150/3208 | Loss: 2.4382\n",
            "  Batch 3160/3208 | Loss: 2.0819\n",
            "  Batch 3170/3208 | Loss: 2.0234\n",
            "  Batch 3180/3208 | Loss: 1.9130\n",
            "  Batch 3190/3208 | Loss: 2.3080\n",
            "  Batch 3200/3208 | Loss: 1.8357\n",
            " Epoch 5 Completed | Avg Loss: 2.1004 | Training Accuracy: 39.72%\n",
            " Validation Accuracy: 45.63%\n",
            " Best Model Saved (Epoch 5 | Accuracy: 45.63%)\n",
            "\n",
            " Epoch 6/20\n",
            "  Batch 10/3208 | Loss: 1.9651\n",
            "  Batch 20/3208 | Loss: 1.9651\n",
            "  Batch 30/3208 | Loss: 1.8236\n",
            "  Batch 40/3208 | Loss: 2.2042\n",
            "  Batch 50/3208 | Loss: 1.9983\n",
            "  Batch 60/3208 | Loss: 1.8724\n",
            "  Batch 70/3208 | Loss: 1.6871\n",
            "  Batch 80/3208 | Loss: 2.0821\n",
            "  Batch 90/3208 | Loss: 2.1163\n",
            "  Batch 100/3208 | Loss: 1.9186\n",
            "  Batch 110/3208 | Loss: 1.9554\n",
            "  Batch 120/3208 | Loss: 1.7637\n",
            "  Batch 130/3208 | Loss: 2.0664\n",
            "  Batch 140/3208 | Loss: 2.0286\n",
            "  Batch 150/3208 | Loss: 2.1659\n",
            "  Batch 160/3208 | Loss: 1.7888\n",
            "  Batch 170/3208 | Loss: 2.0859\n",
            "  Batch 180/3208 | Loss: 1.8872\n",
            "  Batch 190/3208 | Loss: 2.2537\n",
            "  Batch 200/3208 | Loss: 1.8075\n",
            "  Batch 210/3208 | Loss: 2.0407\n",
            "  Batch 220/3208 | Loss: 2.1048\n",
            "  Batch 230/3208 | Loss: 1.9518\n",
            "  Batch 240/3208 | Loss: 2.0260\n",
            "  Batch 250/3208 | Loss: 1.7677\n",
            "  Batch 260/3208 | Loss: 1.9931\n",
            "  Batch 270/3208 | Loss: 1.8030\n",
            "  Batch 280/3208 | Loss: 2.0631\n",
            "  Batch 290/3208 | Loss: 2.0743\n",
            "  Batch 300/3208 | Loss: 2.0974\n",
            "  Batch 310/3208 | Loss: 2.0829\n",
            "  Batch 320/3208 | Loss: 2.2684\n",
            "  Batch 330/3208 | Loss: 1.8948\n",
            "  Batch 340/3208 | Loss: 2.0051\n",
            "  Batch 350/3208 | Loss: 2.1320\n",
            "  Batch 360/3208 | Loss: 1.7580\n",
            "  Batch 370/3208 | Loss: 1.4323\n",
            "  Batch 380/3208 | Loss: 2.0410\n",
            "  Batch 390/3208 | Loss: 1.8523\n",
            "  Batch 400/3208 | Loss: 2.2402\n",
            "  Batch 410/3208 | Loss: 1.7546\n",
            "  Batch 420/3208 | Loss: 2.0285\n",
            "  Batch 430/3208 | Loss: 1.9975\n",
            "  Batch 440/3208 | Loss: 1.9138\n",
            "  Batch 450/3208 | Loss: 2.1016\n",
            "  Batch 460/3208 | Loss: 1.7103\n",
            "  Batch 470/3208 | Loss: 2.1287\n",
            "  Batch 480/3208 | Loss: 1.6313\n",
            "  Batch 490/3208 | Loss: 2.5202\n",
            "  Batch 500/3208 | Loss: 1.7372\n",
            "  Batch 510/3208 | Loss: 1.9246\n",
            "  Batch 520/3208 | Loss: 2.1927\n",
            "  Batch 530/3208 | Loss: 2.3951\n",
            "  Batch 540/3208 | Loss: 2.2314\n",
            "  Batch 550/3208 | Loss: 1.9055\n",
            "  Batch 560/3208 | Loss: 2.2400\n",
            "  Batch 570/3208 | Loss: 1.8168\n",
            "  Batch 580/3208 | Loss: 1.9931\n",
            "  Batch 590/3208 | Loss: 1.3333\n",
            "  Batch 600/3208 | Loss: 1.7223\n",
            "  Batch 610/3208 | Loss: 1.9792\n",
            "  Batch 620/3208 | Loss: 1.7308\n",
            "  Batch 630/3208 | Loss: 2.0747\n",
            "  Batch 640/3208 | Loss: 2.0175\n",
            "  Batch 650/3208 | Loss: 2.1771\n",
            "  Batch 660/3208 | Loss: 1.7647\n",
            "  Batch 670/3208 | Loss: 1.8612\n",
            "  Batch 680/3208 | Loss: 2.1961\n",
            "  Batch 690/3208 | Loss: 1.7958\n",
            "  Batch 700/3208 | Loss: 1.7060\n",
            "  Batch 710/3208 | Loss: 2.1374\n",
            "  Batch 720/3208 | Loss: 2.1210\n",
            "  Batch 730/3208 | Loss: 2.0421\n",
            "  Batch 740/3208 | Loss: 2.4704\n",
            "  Batch 750/3208 | Loss: 1.8792\n",
            "  Batch 760/3208 | Loss: 1.9295\n",
            "  Batch 770/3208 | Loss: 2.4273\n",
            "  Batch 780/3208 | Loss: 1.6348\n",
            "  Batch 790/3208 | Loss: 2.0532\n",
            "  Batch 800/3208 | Loss: 1.8630\n",
            "  Batch 810/3208 | Loss: 2.1311\n",
            "  Batch 820/3208 | Loss: 1.6684\n",
            "  Batch 830/3208 | Loss: 1.6535\n",
            "  Batch 840/3208 | Loss: 1.9867\n",
            "  Batch 850/3208 | Loss: 2.4348\n",
            "  Batch 860/3208 | Loss: 1.9931\n",
            "  Batch 870/3208 | Loss: 2.3219\n",
            "  Batch 880/3208 | Loss: 2.0738\n",
            "  Batch 890/3208 | Loss: 1.9700\n",
            "  Batch 900/3208 | Loss: 1.8559\n",
            "  Batch 910/3208 | Loss: 2.1699\n",
            "  Batch 920/3208 | Loss: 1.8328\n",
            "  Batch 930/3208 | Loss: 2.0249\n",
            "  Batch 940/3208 | Loss: 2.1752\n",
            "  Batch 950/3208 | Loss: 1.8122\n",
            "  Batch 960/3208 | Loss: 1.7048\n",
            "  Batch 970/3208 | Loss: 2.1305\n",
            "  Batch 980/3208 | Loss: 2.3239\n",
            "  Batch 990/3208 | Loss: 1.9753\n",
            "  Batch 1000/3208 | Loss: 2.1799\n",
            "  Batch 1010/3208 | Loss: 2.0558\n",
            "  Batch 1020/3208 | Loss: 2.0492\n",
            "  Batch 1030/3208 | Loss: 1.6637\n",
            "  Batch 1040/3208 | Loss: 2.1944\n",
            "  Batch 1050/3208 | Loss: 2.0953\n",
            "  Batch 1060/3208 | Loss: 1.6308\n",
            "  Batch 1070/3208 | Loss: 2.0041\n",
            "  Batch 1080/3208 | Loss: 1.6435\n",
            "  Batch 1090/3208 | Loss: 1.8726\n",
            "  Batch 1100/3208 | Loss: 1.9300\n",
            "  Batch 1110/3208 | Loss: 1.8688\n",
            "  Batch 1120/3208 | Loss: 1.9632\n",
            "  Batch 1130/3208 | Loss: 1.4253\n",
            "  Batch 1140/3208 | Loss: 2.0660\n",
            "  Batch 1150/3208 | Loss: 2.2342\n",
            "  Batch 1160/3208 | Loss: 2.1037\n",
            "  Batch 1170/3208 | Loss: 2.2382\n",
            "  Batch 1180/3208 | Loss: 2.2305\n",
            "  Batch 1190/3208 | Loss: 1.7493\n",
            "  Batch 1200/3208 | Loss: 1.9488\n",
            "  Batch 1210/3208 | Loss: 2.0521\n",
            "  Batch 1220/3208 | Loss: 1.6279\n",
            "  Batch 1230/3208 | Loss: 2.1488\n",
            "  Batch 1240/3208 | Loss: 1.7015\n",
            "  Batch 1250/3208 | Loss: 1.7279\n",
            "  Batch 1260/3208 | Loss: 1.6416\n",
            "  Batch 1270/3208 | Loss: 1.7499\n",
            "  Batch 1280/3208 | Loss: 2.0499\n",
            "  Batch 1290/3208 | Loss: 1.6275\n",
            "  Batch 1300/3208 | Loss: 1.5818\n",
            "  Batch 1310/3208 | Loss: 1.7404\n",
            "  Batch 1320/3208 | Loss: 2.2178\n",
            "  Batch 1330/3208 | Loss: 2.1772\n",
            "  Batch 1340/3208 | Loss: 2.0578\n",
            "  Batch 1350/3208 | Loss: 1.7030\n",
            "  Batch 1360/3208 | Loss: 2.1194\n",
            "  Batch 1370/3208 | Loss: 1.8326\n",
            "  Batch 1380/3208 | Loss: 1.8106\n",
            "  Batch 1390/3208 | Loss: 2.1617\n",
            "  Batch 1400/3208 | Loss: 1.7464\n",
            "  Batch 1410/3208 | Loss: 1.7671\n",
            "  Batch 1420/3208 | Loss: 2.0050\n",
            "  Batch 1430/3208 | Loss: 1.9327\n",
            "  Batch 1440/3208 | Loss: 1.8510\n",
            "  Batch 1450/3208 | Loss: 2.0845\n",
            "  Batch 1460/3208 | Loss: 1.9592\n",
            "  Batch 1470/3208 | Loss: 2.1178\n",
            "  Batch 1480/3208 | Loss: 1.9497\n",
            "  Batch 1490/3208 | Loss: 1.7815\n",
            "  Batch 1500/3208 | Loss: 2.1082\n",
            "  Batch 1510/3208 | Loss: 1.7373\n",
            "  Batch 1520/3208 | Loss: 1.6685\n",
            "  Batch 1530/3208 | Loss: 2.2383\n",
            "  Batch 1540/3208 | Loss: 2.1957\n",
            "  Batch 1550/3208 | Loss: 1.7175\n",
            "  Batch 1560/3208 | Loss: 2.1770\n",
            "  Batch 1570/3208 | Loss: 2.0807\n",
            "  Batch 1580/3208 | Loss: 1.7085\n",
            "  Batch 1590/3208 | Loss: 1.8935\n",
            "  Batch 1600/3208 | Loss: 2.1882\n",
            "  Batch 1610/3208 | Loss: 2.0127\n",
            "  Batch 1620/3208 | Loss: 1.8942\n",
            "  Batch 1630/3208 | Loss: 2.4547\n",
            "  Batch 1640/3208 | Loss: 1.9897\n",
            "  Batch 1650/3208 | Loss: 1.9691\n",
            "  Batch 1660/3208 | Loss: 2.0205\n",
            "  Batch 1670/3208 | Loss: 2.0530\n",
            "  Batch 1680/3208 | Loss: 1.8764\n",
            "  Batch 1690/3208 | Loss: 1.4806\n",
            "  Batch 1700/3208 | Loss: 1.6937\n",
            "  Batch 1710/3208 | Loss: 2.4016\n",
            "  Batch 1720/3208 | Loss: 2.3917\n",
            "  Batch 1730/3208 | Loss: 1.7109\n",
            "  Batch 1740/3208 | Loss: 2.0238\n",
            "  Batch 1750/3208 | Loss: 1.8184\n",
            "  Batch 1760/3208 | Loss: 2.2438\n",
            "  Batch 1770/3208 | Loss: 2.1234\n",
            "  Batch 1780/3208 | Loss: 1.9304\n",
            "  Batch 1790/3208 | Loss: 1.6261\n",
            "  Batch 1800/3208 | Loss: 1.7716\n",
            "  Batch 1810/3208 | Loss: 1.7733\n",
            "  Batch 1820/3208 | Loss: 1.5887\n",
            "  Batch 1830/3208 | Loss: 1.6313\n",
            "  Batch 1840/3208 | Loss: 2.2873\n",
            "  Batch 1850/3208 | Loss: 1.9673\n",
            "  Batch 1860/3208 | Loss: 1.7980\n",
            "  Batch 1870/3208 | Loss: 2.4534\n",
            "  Batch 1880/3208 | Loss: 1.9131\n",
            "  Batch 1890/3208 | Loss: 1.8134\n",
            "  Batch 1900/3208 | Loss: 1.7036\n",
            "  Batch 1910/3208 | Loss: 2.1675\n",
            "  Batch 1920/3208 | Loss: 1.8162\n",
            "  Batch 1930/3208 | Loss: 2.1745\n",
            "  Batch 1940/3208 | Loss: 1.6687\n",
            "  Batch 1950/3208 | Loss: 1.9664\n",
            "  Batch 1960/3208 | Loss: 2.4934\n",
            "  Batch 1970/3208 | Loss: 2.5311\n",
            "  Batch 1980/3208 | Loss: 1.6691\n",
            "  Batch 1990/3208 | Loss: 1.7031\n",
            "  Batch 2000/3208 | Loss: 2.1468\n",
            "  Batch 2010/3208 | Loss: 1.8665\n",
            "  Batch 2020/3208 | Loss: 1.8262\n",
            "  Batch 2030/3208 | Loss: 1.9688\n",
            "  Batch 2040/3208 | Loss: 1.6044\n",
            "  Batch 2050/3208 | Loss: 1.6335\n",
            "  Batch 2060/3208 | Loss: 1.8149\n",
            "  Batch 2070/3208 | Loss: 2.1147\n",
            "  Batch 2080/3208 | Loss: 2.1265\n",
            "  Batch 2090/3208 | Loss: 2.2408\n",
            "  Batch 2100/3208 | Loss: 2.3672\n",
            "  Batch 2110/3208 | Loss: 2.0337\n",
            "  Batch 2120/3208 | Loss: 1.8788\n",
            "  Batch 2130/3208 | Loss: 1.8217\n",
            "  Batch 2140/3208 | Loss: 2.1768\n",
            "  Batch 2150/3208 | Loss: 1.5109\n",
            "  Batch 2160/3208 | Loss: 2.0361\n",
            "  Batch 2170/3208 | Loss: 1.8366\n",
            "  Batch 2180/3208 | Loss: 2.2653\n",
            "  Batch 2190/3208 | Loss: 2.1687\n",
            "  Batch 2200/3208 | Loss: 1.9021\n",
            "  Batch 2210/3208 | Loss: 2.0567\n",
            "  Batch 2220/3208 | Loss: 1.8088\n",
            "  Batch 2230/3208 | Loss: 1.9624\n",
            "  Batch 2240/3208 | Loss: 2.3554\n",
            "  Batch 2250/3208 | Loss: 1.6703\n",
            "  Batch 2260/3208 | Loss: 1.6365\n",
            "  Batch 2270/3208 | Loss: 1.6530\n",
            "  Batch 2280/3208 | Loss: 1.7486\n",
            "  Batch 2290/3208 | Loss: 1.6501\n",
            "  Batch 2300/3208 | Loss: 2.0537\n",
            "  Batch 2310/3208 | Loss: 1.9956\n",
            "  Batch 2320/3208 | Loss: 1.6986\n",
            "  Batch 2330/3208 | Loss: 1.9290\n",
            "  Batch 2340/3208 | Loss: 1.8194\n",
            "  Batch 2350/3208 | Loss: 1.9052\n",
            "  Batch 2360/3208 | Loss: 1.8041\n",
            "  Batch 2370/3208 | Loss: 1.7551\n",
            "  Batch 2380/3208 | Loss: 1.8691\n",
            "  Batch 2390/3208 | Loss: 2.2421\n",
            "  Batch 2400/3208 | Loss: 2.4102\n",
            "  Batch 2410/3208 | Loss: 1.3757\n",
            "  Batch 2420/3208 | Loss: 1.6586\n",
            "  Batch 2430/3208 | Loss: 1.6309\n",
            "  Batch 2440/3208 | Loss: 2.0121\n",
            "  Batch 2450/3208 | Loss: 1.4765\n",
            "  Batch 2460/3208 | Loss: 1.6654\n",
            "  Batch 2470/3208 | Loss: 1.7554\n",
            "  Batch 2480/3208 | Loss: 2.0433\n",
            "  Batch 2490/3208 | Loss: 1.8478\n",
            "  Batch 2500/3208 | Loss: 1.8040\n",
            "  Batch 2510/3208 | Loss: 1.8807\n",
            "  Batch 2520/3208 | Loss: 2.1558\n",
            "  Batch 2530/3208 | Loss: 1.6948\n",
            "  Batch 2540/3208 | Loss: 2.1848\n",
            "  Batch 2550/3208 | Loss: 2.2180\n",
            "  Batch 2560/3208 | Loss: 2.2711\n",
            "  Batch 2570/3208 | Loss: 1.6864\n",
            "  Batch 2580/3208 | Loss: 1.8823\n",
            "  Batch 2590/3208 | Loss: 1.8067\n",
            "  Batch 2600/3208 | Loss: 2.0363\n",
            "  Batch 2610/3208 | Loss: 2.0787\n",
            "  Batch 2620/3208 | Loss: 1.7649\n",
            "  Batch 2630/3208 | Loss: 2.2849\n",
            "  Batch 2640/3208 | Loss: 2.2979\n",
            "  Batch 2650/3208 | Loss: 1.5275\n",
            "  Batch 2660/3208 | Loss: 1.9482\n",
            "  Batch 2670/3208 | Loss: 1.8688\n",
            "  Batch 2680/3208 | Loss: 1.5185\n",
            "  Batch 2690/3208 | Loss: 1.8586\n",
            "  Batch 2700/3208 | Loss: 2.2416\n",
            "  Batch 2710/3208 | Loss: 2.0278\n",
            "  Batch 2720/3208 | Loss: 2.0361\n",
            "  Batch 2730/3208 | Loss: 1.9369\n",
            "  Batch 2740/3208 | Loss: 1.7738\n",
            "  Batch 2750/3208 | Loss: 1.7970\n",
            "  Batch 2760/3208 | Loss: 2.0018\n",
            "  Batch 2770/3208 | Loss: 1.6448\n",
            "  Batch 2780/3208 | Loss: 1.3497\n",
            "  Batch 2790/3208 | Loss: 1.9895\n",
            "  Batch 2800/3208 | Loss: 1.9204\n",
            "  Batch 2810/3208 | Loss: 1.8688\n",
            "  Batch 2820/3208 | Loss: 1.8878\n",
            "  Batch 2830/3208 | Loss: 1.7314\n",
            "  Batch 2840/3208 | Loss: 2.3363\n",
            "  Batch 2850/3208 | Loss: 1.5439\n",
            "  Batch 2860/3208 | Loss: 1.7658\n",
            "  Batch 2870/3208 | Loss: 1.8495\n",
            "  Batch 2880/3208 | Loss: 2.0537\n",
            "  Batch 2890/3208 | Loss: 1.7548\n",
            "  Batch 2900/3208 | Loss: 2.1141\n",
            "  Batch 2910/3208 | Loss: 1.5699\n",
            "  Batch 2920/3208 | Loss: 1.9317\n",
            "  Batch 2930/3208 | Loss: 2.0003\n",
            "  Batch 2940/3208 | Loss: 1.8405\n",
            "  Batch 2950/3208 | Loss: 2.0345\n",
            "  Batch 2960/3208 | Loss: 1.6623\n",
            "  Batch 2970/3208 | Loss: 1.5520\n",
            "  Batch 2980/3208 | Loss: 1.9660\n",
            "  Batch 2990/3208 | Loss: 2.0258\n",
            "  Batch 3000/3208 | Loss: 2.2213\n",
            "  Batch 3010/3208 | Loss: 2.3652\n",
            "  Batch 3020/3208 | Loss: 2.0132\n",
            "  Batch 3030/3208 | Loss: 1.5392\n",
            "  Batch 3040/3208 | Loss: 2.1102\n",
            "  Batch 3050/3208 | Loss: 1.8170\n",
            "  Batch 3060/3208 | Loss: 2.1588\n",
            "  Batch 3070/3208 | Loss: 1.6085\n",
            "  Batch 3080/3208 | Loss: 1.7815\n",
            "  Batch 3090/3208 | Loss: 2.3521\n",
            "  Batch 3100/3208 | Loss: 2.2786\n",
            "  Batch 3110/3208 | Loss: 1.8905\n",
            "  Batch 3120/3208 | Loss: 2.0600\n",
            "  Batch 3130/3208 | Loss: 2.2939\n",
            "  Batch 3140/3208 | Loss: 1.9338\n",
            "  Batch 3150/3208 | Loss: 1.9800\n",
            "  Batch 3160/3208 | Loss: 2.0994\n",
            "  Batch 3170/3208 | Loss: 1.5955\n",
            "  Batch 3180/3208 | Loss: 1.3556\n",
            "  Batch 3190/3208 | Loss: 1.6985\n",
            "  Batch 3200/3208 | Loss: 1.5586\n",
            " Epoch 6 Completed | Avg Loss: 1.9238 | Training Accuracy: 44.93%\n",
            " Validation Accuracy: 50.99%\n",
            " Best Model Saved (Epoch 6 | Accuracy: 50.99%)\n",
            "\n",
            " Epoch 7/20\n",
            "  Batch 10/3208 | Loss: 1.8129\n",
            "  Batch 20/3208 | Loss: 2.0779\n",
            "  Batch 30/3208 | Loss: 1.7387\n",
            "  Batch 40/3208 | Loss: 1.8331\n",
            "  Batch 50/3208 | Loss: 1.5030\n",
            "  Batch 60/3208 | Loss: 2.0395\n",
            "  Batch 70/3208 | Loss: 1.9467\n",
            "  Batch 80/3208 | Loss: 1.8351\n",
            "  Batch 90/3208 | Loss: 1.3776\n",
            "  Batch 100/3208 | Loss: 1.4948\n",
            "  Batch 110/3208 | Loss: 2.2457\n",
            "  Batch 120/3208 | Loss: 2.1942\n",
            "  Batch 130/3208 | Loss: 1.7614\n",
            "  Batch 140/3208 | Loss: 1.2585\n",
            "  Batch 150/3208 | Loss: 1.7848\n",
            "  Batch 160/3208 | Loss: 1.7377\n",
            "  Batch 170/3208 | Loss: 1.8679\n",
            "  Batch 180/3208 | Loss: 2.1807\n",
            "  Batch 190/3208 | Loss: 1.7974\n",
            "  Batch 200/3208 | Loss: 1.5128\n",
            "  Batch 210/3208 | Loss: 2.3714\n",
            "  Batch 220/3208 | Loss: 1.8855\n",
            "  Batch 230/3208 | Loss: 1.6721\n",
            "  Batch 240/3208 | Loss: 1.4455\n",
            "  Batch 250/3208 | Loss: 1.7491\n",
            "  Batch 260/3208 | Loss: 1.8710\n",
            "  Batch 270/3208 | Loss: 2.0037\n",
            "  Batch 280/3208 | Loss: 2.1617\n",
            "  Batch 290/3208 | Loss: 1.8010\n",
            "  Batch 300/3208 | Loss: 1.6187\n",
            "  Batch 310/3208 | Loss: 1.9299\n",
            "  Batch 320/3208 | Loss: 1.5740\n",
            "  Batch 330/3208 | Loss: 1.8411\n",
            "  Batch 340/3208 | Loss: 1.6459\n",
            "  Batch 350/3208 | Loss: 2.6456\n",
            "  Batch 360/3208 | Loss: 1.7332\n",
            "  Batch 370/3208 | Loss: 1.8666\n",
            "  Batch 380/3208 | Loss: 2.0217\n",
            "  Batch 390/3208 | Loss: 1.5825\n",
            "  Batch 400/3208 | Loss: 1.8570\n",
            "  Batch 410/3208 | Loss: 1.4606\n",
            "  Batch 420/3208 | Loss: 1.8312\n",
            "  Batch 430/3208 | Loss: 2.2955\n",
            "  Batch 440/3208 | Loss: 1.9571\n",
            "  Batch 450/3208 | Loss: 2.0949\n",
            "  Batch 460/3208 | Loss: 1.6477\n",
            "  Batch 470/3208 | Loss: 1.9911\n",
            "  Batch 480/3208 | Loss: 2.0261\n",
            "  Batch 490/3208 | Loss: 2.0393\n",
            "  Batch 500/3208 | Loss: 1.6207\n",
            "  Batch 510/3208 | Loss: 2.1760\n",
            "  Batch 520/3208 | Loss: 1.9215\n",
            "  Batch 530/3208 | Loss: 1.5682\n",
            "  Batch 540/3208 | Loss: 2.0906\n",
            "  Batch 550/3208 | Loss: 1.4198\n",
            "  Batch 560/3208 | Loss: 1.5587\n",
            "  Batch 570/3208 | Loss: 1.4821\n",
            "  Batch 580/3208 | Loss: 1.6480\n",
            "  Batch 590/3208 | Loss: 1.4321\n",
            "  Batch 600/3208 | Loss: 1.5939\n",
            "  Batch 610/3208 | Loss: 1.9414\n",
            "  Batch 620/3208 | Loss: 2.3107\n",
            "  Batch 630/3208 | Loss: 2.0795\n",
            "  Batch 640/3208 | Loss: 1.8023\n",
            "  Batch 650/3208 | Loss: 1.5256\n",
            "  Batch 660/3208 | Loss: 1.9463\n",
            "  Batch 670/3208 | Loss: 2.1602\n",
            "  Batch 680/3208 | Loss: 2.1654\n",
            "  Batch 690/3208 | Loss: 2.0327\n",
            "  Batch 700/3208 | Loss: 1.4303\n",
            "  Batch 710/3208 | Loss: 2.2431\n",
            "  Batch 720/3208 | Loss: 1.6153\n",
            "  Batch 730/3208 | Loss: 2.0452\n",
            "  Batch 740/3208 | Loss: 2.1241\n",
            "  Batch 750/3208 | Loss: 1.7506\n",
            "  Batch 760/3208 | Loss: 2.0867\n",
            "  Batch 770/3208 | Loss: 1.9717\n",
            "  Batch 780/3208 | Loss: 1.8464\n",
            "  Batch 790/3208 | Loss: 2.0946\n",
            "  Batch 800/3208 | Loss: 2.2111\n",
            "  Batch 810/3208 | Loss: 1.9706\n",
            "  Batch 820/3208 | Loss: 1.8845\n",
            "  Batch 830/3208 | Loss: 2.0544\n",
            "  Batch 840/3208 | Loss: 1.4585\n",
            "  Batch 850/3208 | Loss: 1.7166\n",
            "  Batch 860/3208 | Loss: 1.3663\n",
            "  Batch 870/3208 | Loss: 1.7882\n",
            "  Batch 880/3208 | Loss: 1.7349\n",
            "  Batch 890/3208 | Loss: 1.7351\n",
            "  Batch 900/3208 | Loss: 1.8350\n",
            "  Batch 910/3208 | Loss: 2.1065\n",
            "  Batch 920/3208 | Loss: 1.5507\n",
            "  Batch 930/3208 | Loss: 2.1227\n",
            "  Batch 940/3208 | Loss: 1.9125\n",
            "  Batch 950/3208 | Loss: 1.4216\n",
            "  Batch 960/3208 | Loss: 2.0623\n",
            "  Batch 970/3208 | Loss: 1.6234\n",
            "  Batch 980/3208 | Loss: 1.3505\n",
            "  Batch 990/3208 | Loss: 2.1914\n",
            "  Batch 1000/3208 | Loss: 2.0997\n",
            "  Batch 1010/3208 | Loss: 1.9191\n",
            "  Batch 1020/3208 | Loss: 2.2418\n",
            "  Batch 1030/3208 | Loss: 1.6319\n",
            "  Batch 1040/3208 | Loss: 1.9286\n",
            "  Batch 1050/3208 | Loss: 1.5829\n",
            "  Batch 1060/3208 | Loss: 1.6178\n",
            "  Batch 1070/3208 | Loss: 2.1369\n",
            "  Batch 1080/3208 | Loss: 1.8935\n",
            "  Batch 1090/3208 | Loss: 1.7692\n",
            "  Batch 1100/3208 | Loss: 1.6463\n",
            "  Batch 1110/3208 | Loss: 2.0459\n",
            "  Batch 1120/3208 | Loss: 2.0137\n",
            "  Batch 1130/3208 | Loss: 1.8336\n",
            "  Batch 1140/3208 | Loss: 1.7693\n",
            "  Batch 1150/3208 | Loss: 1.6400\n",
            "  Batch 1160/3208 | Loss: 1.6479\n",
            "  Batch 1170/3208 | Loss: 1.7708\n",
            "  Batch 1180/3208 | Loss: 1.8209\n",
            "  Batch 1190/3208 | Loss: 1.7967\n",
            "  Batch 1200/3208 | Loss: 1.8266\n",
            "  Batch 1210/3208 | Loss: 1.7879\n",
            "  Batch 1220/3208 | Loss: 2.4145\n",
            "  Batch 1230/3208 | Loss: 1.8879\n",
            "  Batch 1240/3208 | Loss: 2.2774\n",
            "  Batch 1250/3208 | Loss: 1.8408\n",
            "  Batch 1260/3208 | Loss: 1.5012\n",
            "  Batch 1270/3208 | Loss: 1.7480\n",
            "  Batch 1280/3208 | Loss: 2.2240\n",
            "  Batch 1290/3208 | Loss: 2.2108\n",
            "  Batch 1300/3208 | Loss: 1.8408\n",
            "  Batch 1310/3208 | Loss: 1.2756\n",
            "  Batch 1320/3208 | Loss: 2.0319\n",
            "  Batch 1330/3208 | Loss: 1.4554\n",
            "  Batch 1340/3208 | Loss: 2.1686\n",
            "  Batch 1350/3208 | Loss: 2.1073\n",
            "  Batch 1360/3208 | Loss: 1.6218\n",
            "  Batch 1370/3208 | Loss: 2.0822\n",
            "  Batch 1380/3208 | Loss: 1.5493\n",
            "  Batch 1390/3208 | Loss: 2.1654\n",
            "  Batch 1400/3208 | Loss: 1.5942\n",
            "  Batch 1410/3208 | Loss: 2.1506\n",
            "  Batch 1420/3208 | Loss: 1.8158\n",
            "  Batch 1430/3208 | Loss: 1.6471\n",
            "  Batch 1440/3208 | Loss: 1.8194\n",
            "  Batch 1450/3208 | Loss: 1.6867\n",
            "  Batch 1460/3208 | Loss: 1.9328\n",
            "  Batch 1470/3208 | Loss: 1.7975\n",
            "  Batch 1480/3208 | Loss: 1.6904\n",
            "  Batch 1490/3208 | Loss: 1.6494\n",
            "  Batch 1500/3208 | Loss: 1.6195\n",
            "  Batch 1510/3208 | Loss: 1.7227\n",
            "  Batch 1520/3208 | Loss: 2.2153\n",
            "  Batch 1530/3208 | Loss: 1.8566\n",
            "  Batch 1540/3208 | Loss: 2.0450\n",
            "  Batch 1550/3208 | Loss: 2.1063\n",
            "  Batch 1560/3208 | Loss: 2.1232\n",
            "  Batch 1570/3208 | Loss: 1.8141\n",
            "  Batch 1580/3208 | Loss: 1.7912\n",
            "  Batch 1590/3208 | Loss: 1.6770\n",
            "  Batch 1600/3208 | Loss: 1.4849\n",
            "  Batch 1610/3208 | Loss: 2.0517\n",
            "  Batch 1620/3208 | Loss: 1.5112\n",
            "  Batch 1630/3208 | Loss: 1.8362\n",
            "  Batch 1640/3208 | Loss: 2.0719\n",
            "  Batch 1650/3208 | Loss: 2.0179\n",
            "  Batch 1660/3208 | Loss: 1.7509\n",
            "  Batch 1670/3208 | Loss: 2.2486\n",
            "  Batch 1680/3208 | Loss: 1.9141\n",
            "  Batch 1690/3208 | Loss: 1.9307\n",
            "  Batch 1700/3208 | Loss: 2.0403\n",
            "  Batch 1710/3208 | Loss: 1.9052\n",
            "  Batch 1720/3208 | Loss: 2.1281\n",
            "  Batch 1730/3208 | Loss: 1.6734\n",
            "  Batch 1740/3208 | Loss: 1.2054\n",
            "  Batch 1750/3208 | Loss: 1.6998\n",
            "  Batch 1760/3208 | Loss: 1.4379\n",
            "  Batch 1770/3208 | Loss: 1.7936\n",
            "  Batch 1780/3208 | Loss: 1.6890\n",
            "  Batch 1790/3208 | Loss: 1.7226\n",
            "  Batch 1800/3208 | Loss: 2.1860\n",
            "  Batch 1810/3208 | Loss: 1.8188\n",
            "  Batch 1820/3208 | Loss: 1.3643\n",
            "  Batch 1830/3208 | Loss: 2.0405\n",
            "  Batch 1840/3208 | Loss: 1.6600\n",
            "  Batch 1850/3208 | Loss: 1.5754\n",
            "  Batch 1860/3208 | Loss: 1.2153\n",
            "  Batch 1870/3208 | Loss: 1.6328\n",
            "  Batch 1880/3208 | Loss: 1.9923\n",
            "  Batch 1890/3208 | Loss: 2.0649\n",
            "  Batch 1900/3208 | Loss: 1.6882\n",
            "  Batch 1910/3208 | Loss: 1.4932\n",
            "  Batch 1920/3208 | Loss: 2.4934\n",
            "  Batch 1930/3208 | Loss: 2.1691\n",
            "  Batch 1940/3208 | Loss: 1.6468\n",
            "  Batch 1950/3208 | Loss: 1.7726\n",
            "  Batch 1960/3208 | Loss: 1.3927\n",
            "  Batch 1970/3208 | Loss: 2.5065\n",
            "  Batch 1980/3208 | Loss: 1.5957\n",
            "  Batch 1990/3208 | Loss: 1.5592\n",
            "  Batch 2000/3208 | Loss: 1.8193\n",
            "  Batch 2010/3208 | Loss: 2.2103\n",
            "  Batch 2020/3208 | Loss: 1.8238\n",
            "  Batch 2030/3208 | Loss: 1.6388\n",
            "  Batch 2040/3208 | Loss: 2.0177\n",
            "  Batch 2050/3208 | Loss: 2.0193\n",
            "  Batch 2060/3208 | Loss: 1.8130\n",
            "  Batch 2070/3208 | Loss: 1.6155\n",
            "  Batch 2080/3208 | Loss: 1.6703\n",
            "  Batch 2090/3208 | Loss: 1.2338\n",
            "  Batch 2100/3208 | Loss: 1.8476\n",
            "  Batch 2110/3208 | Loss: 1.7025\n",
            "  Batch 2120/3208 | Loss: 1.4142\n",
            "  Batch 2130/3208 | Loss: 1.8198\n",
            "  Batch 2140/3208 | Loss: 1.4110\n",
            "  Batch 2150/3208 | Loss: 1.4437\n",
            "  Batch 2160/3208 | Loss: 1.4770\n",
            "  Batch 2170/3208 | Loss: 2.0172\n",
            "  Batch 2180/3208 | Loss: 1.8777\n",
            "  Batch 2190/3208 | Loss: 1.5704\n",
            "  Batch 2200/3208 | Loss: 1.4181\n",
            "  Batch 2210/3208 | Loss: 1.6717\n",
            "  Batch 2220/3208 | Loss: 1.6201\n",
            "  Batch 2230/3208 | Loss: 1.6698\n",
            "  Batch 2240/3208 | Loss: 1.4032\n",
            "  Batch 2250/3208 | Loss: 1.7942\n",
            "  Batch 2260/3208 | Loss: 1.6667\n",
            "  Batch 2270/3208 | Loss: 1.6049\n",
            "  Batch 2280/3208 | Loss: 1.8442\n",
            "  Batch 2290/3208 | Loss: 1.6321\n",
            "  Batch 2300/3208 | Loss: 1.7036\n",
            "  Batch 2310/3208 | Loss: 2.2557\n",
            "  Batch 2320/3208 | Loss: 1.2157\n",
            "  Batch 2330/3208 | Loss: 1.5621\n",
            "  Batch 2340/3208 | Loss: 2.0110\n",
            "  Batch 2350/3208 | Loss: 1.3456\n",
            "  Batch 2360/3208 | Loss: 1.4170\n",
            "  Batch 2370/3208 | Loss: 1.6593\n",
            "  Batch 2380/3208 | Loss: 1.7330\n",
            "  Batch 2390/3208 | Loss: 1.8986\n",
            "  Batch 2400/3208 | Loss: 1.7342\n",
            "  Batch 2410/3208 | Loss: 1.6902\n",
            "  Batch 2420/3208 | Loss: 2.2524\n",
            "  Batch 2430/3208 | Loss: 1.5087\n",
            "  Batch 2440/3208 | Loss: 1.7458\n",
            "  Batch 2450/3208 | Loss: 1.3879\n",
            "  Batch 2460/3208 | Loss: 2.0524\n",
            "  Batch 2470/3208 | Loss: 1.8159\n",
            "  Batch 2480/3208 | Loss: 1.7840\n",
            "  Batch 2490/3208 | Loss: 2.3545\n",
            "  Batch 2500/3208 | Loss: 1.4950\n",
            "  Batch 2510/3208 | Loss: 1.7483\n",
            "  Batch 2520/3208 | Loss: 1.8515\n",
            "  Batch 2530/3208 | Loss: 1.4891\n",
            "  Batch 2540/3208 | Loss: 1.4280\n",
            "  Batch 2550/3208 | Loss: 1.5431\n",
            "  Batch 2560/3208 | Loss: 1.7649\n",
            "  Batch 2570/3208 | Loss: 1.7735\n",
            "  Batch 2580/3208 | Loss: 1.8026\n",
            "  Batch 2590/3208 | Loss: 1.4082\n",
            "  Batch 2600/3208 | Loss: 2.2014\n",
            "  Batch 2610/3208 | Loss: 1.7678\n",
            "  Batch 2620/3208 | Loss: 1.7097\n",
            "  Batch 2630/3208 | Loss: 1.8710\n",
            "  Batch 2640/3208 | Loss: 2.3438\n",
            "  Batch 2650/3208 | Loss: 1.9403\n",
            "  Batch 2660/3208 | Loss: 1.6465\n",
            "  Batch 2670/3208 | Loss: 1.9004\n",
            "  Batch 2680/3208 | Loss: 1.8510\n",
            "  Batch 2690/3208 | Loss: 1.9454\n",
            "  Batch 2700/3208 | Loss: 1.5361\n",
            "  Batch 2710/3208 | Loss: 1.8732\n",
            "  Batch 2720/3208 | Loss: 1.5063\n",
            "  Batch 2730/3208 | Loss: 2.2410\n",
            "  Batch 2740/3208 | Loss: 1.8904\n",
            "  Batch 2750/3208 | Loss: 1.9021\n",
            "  Batch 2760/3208 | Loss: 1.8795\n",
            "  Batch 2770/3208 | Loss: 1.3049\n",
            "  Batch 2780/3208 | Loss: 1.4581\n",
            "  Batch 2790/3208 | Loss: 2.2371\n",
            "  Batch 2800/3208 | Loss: 1.8357\n",
            "  Batch 2810/3208 | Loss: 1.2590\n",
            "  Batch 2820/3208 | Loss: 1.4574\n",
            "  Batch 2830/3208 | Loss: 1.9569\n",
            "  Batch 2840/3208 | Loss: 1.2852\n",
            "  Batch 2850/3208 | Loss: 1.5240\n",
            "  Batch 2860/3208 | Loss: 1.9821\n",
            "  Batch 2870/3208 | Loss: 1.5468\n",
            "  Batch 2880/3208 | Loss: 2.0298\n",
            "  Batch 2890/3208 | Loss: 2.3159\n",
            "  Batch 2900/3208 | Loss: 1.9967\n",
            "  Batch 2910/3208 | Loss: 1.3971\n",
            "  Batch 2920/3208 | Loss: 1.5974\n",
            "  Batch 2930/3208 | Loss: 1.6185\n",
            "  Batch 2940/3208 | Loss: 2.0018\n",
            "  Batch 2950/3208 | Loss: 1.6382\n",
            "  Batch 2960/3208 | Loss: 1.9225\n",
            "  Batch 2970/3208 | Loss: 1.4346\n",
            "  Batch 2980/3208 | Loss: 1.6234\n",
            "  Batch 2990/3208 | Loss: 2.3555\n",
            "  Batch 3000/3208 | Loss: 1.2928\n",
            "  Batch 3010/3208 | Loss: 1.6004\n",
            "  Batch 3020/3208 | Loss: 1.9042\n",
            "  Batch 3030/3208 | Loss: 1.7514\n",
            "  Batch 3040/3208 | Loss: 2.0355\n",
            "  Batch 3050/3208 | Loss: 1.5381\n",
            "  Batch 3060/3208 | Loss: 2.1132\n",
            "  Batch 3070/3208 | Loss: 2.0622\n",
            "  Batch 3080/3208 | Loss: 2.2405\n",
            "  Batch 3090/3208 | Loss: 1.5181\n",
            "  Batch 3100/3208 | Loss: 1.7701\n",
            "  Batch 3110/3208 | Loss: 1.7086\n",
            "  Batch 3120/3208 | Loss: 1.7042\n",
            "  Batch 3130/3208 | Loss: 1.6070\n",
            "  Batch 3140/3208 | Loss: 1.5209\n",
            "  Batch 3150/3208 | Loss: 1.8440\n",
            "  Batch 3160/3208 | Loss: 1.4531\n",
            "  Batch 3170/3208 | Loss: 1.6725\n",
            "  Batch 3180/3208 | Loss: 1.6412\n",
            "  Batch 3190/3208 | Loss: 1.4394\n",
            "  Batch 3200/3208 | Loss: 1.3080\n",
            " Epoch 7 Completed | Avg Loss: 1.7758 | Training Accuracy: 49.29%\n",
            " Validation Accuracy: 53.77%\n",
            " Best Model Saved (Epoch 7 | Accuracy: 53.77%)\n",
            "\n",
            " Epoch 8/20\n",
            "  Batch 10/3208 | Loss: 1.5243\n",
            "  Batch 20/3208 | Loss: 1.5342\n",
            "  Batch 30/3208 | Loss: 1.5837\n",
            "  Batch 40/3208 | Loss: 1.8945\n",
            "  Batch 50/3208 | Loss: 1.8430\n",
            "  Batch 60/3208 | Loss: 1.6523\n",
            "  Batch 70/3208 | Loss: 1.9891\n",
            "  Batch 80/3208 | Loss: 1.3882\n",
            "  Batch 90/3208 | Loss: 1.6016\n",
            "  Batch 100/3208 | Loss: 1.5977\n",
            "  Batch 110/3208 | Loss: 1.7670\n",
            "  Batch 120/3208 | Loss: 1.5384\n",
            "  Batch 130/3208 | Loss: 1.4864\n",
            "  Batch 140/3208 | Loss: 1.6063\n",
            "  Batch 150/3208 | Loss: 1.6878\n",
            "  Batch 160/3208 | Loss: 1.6213\n",
            "  Batch 170/3208 | Loss: 1.6436\n",
            "  Batch 180/3208 | Loss: 1.3409\n",
            "  Batch 190/3208 | Loss: 1.5459\n",
            "  Batch 200/3208 | Loss: 1.4288\n",
            "  Batch 210/3208 | Loss: 1.7938\n",
            "  Batch 220/3208 | Loss: 1.5977\n",
            "  Batch 230/3208 | Loss: 1.4982\n",
            "  Batch 240/3208 | Loss: 1.9384\n",
            "  Batch 250/3208 | Loss: 1.9120\n",
            "  Batch 260/3208 | Loss: 1.4409\n",
            "  Batch 270/3208 | Loss: 2.2687\n",
            "  Batch 280/3208 | Loss: 2.2549\n",
            "  Batch 290/3208 | Loss: 1.9250\n",
            "  Batch 300/3208 | Loss: 2.5426\n",
            "  Batch 310/3208 | Loss: 1.9274\n",
            "  Batch 320/3208 | Loss: 2.0660\n",
            "  Batch 330/3208 | Loss: 1.8894\n",
            "  Batch 340/3208 | Loss: 1.3465\n",
            "  Batch 350/3208 | Loss: 1.6608\n",
            "  Batch 360/3208 | Loss: 1.7497\n",
            "  Batch 370/3208 | Loss: 2.1170\n",
            "  Batch 380/3208 | Loss: 1.8157\n",
            "  Batch 390/3208 | Loss: 1.5092\n",
            "  Batch 400/3208 | Loss: 1.8447\n",
            "  Batch 410/3208 | Loss: 1.3180\n",
            "  Batch 420/3208 | Loss: 1.7961\n",
            "  Batch 430/3208 | Loss: 2.1082\n",
            "  Batch 440/3208 | Loss: 1.5058\n",
            "  Batch 450/3208 | Loss: 1.6590\n",
            "  Batch 460/3208 | Loss: 1.7504\n",
            "  Batch 470/3208 | Loss: 1.7643\n",
            "  Batch 480/3208 | Loss: 1.4296\n",
            "  Batch 490/3208 | Loss: 1.6997\n",
            "  Batch 500/3208 | Loss: 2.1619\n",
            "  Batch 510/3208 | Loss: 1.5417\n",
            "  Batch 520/3208 | Loss: 1.4329\n",
            "  Batch 530/3208 | Loss: 1.5553\n",
            "  Batch 540/3208 | Loss: 1.5150\n",
            "  Batch 550/3208 | Loss: 2.1068\n",
            "  Batch 560/3208 | Loss: 1.3640\n",
            "  Batch 570/3208 | Loss: 1.4216\n",
            "  Batch 580/3208 | Loss: 1.7764\n",
            "  Batch 590/3208 | Loss: 1.6724\n",
            "  Batch 600/3208 | Loss: 2.2485\n",
            "  Batch 610/3208 | Loss: 1.8437\n",
            "  Batch 620/3208 | Loss: 2.0676\n",
            "  Batch 630/3208 | Loss: 1.9434\n",
            "  Batch 640/3208 | Loss: 1.3871\n",
            "  Batch 650/3208 | Loss: 1.6232\n",
            "  Batch 660/3208 | Loss: 1.7514\n",
            "  Batch 670/3208 | Loss: 1.9534\n",
            "  Batch 680/3208 | Loss: 1.8380\n",
            "  Batch 690/3208 | Loss: 1.6632\n",
            "  Batch 700/3208 | Loss: 2.3484\n",
            "  Batch 710/3208 | Loss: 1.4360\n",
            "  Batch 720/3208 | Loss: 1.4662\n",
            "  Batch 730/3208 | Loss: 1.3507\n",
            "  Batch 740/3208 | Loss: 2.0361\n",
            "  Batch 750/3208 | Loss: 1.5937\n",
            "  Batch 760/3208 | Loss: 1.3672\n",
            "  Batch 770/3208 | Loss: 1.6078\n",
            "  Batch 780/3208 | Loss: 1.4985\n",
            "  Batch 790/3208 | Loss: 1.6045\n",
            "  Batch 800/3208 | Loss: 1.8963\n",
            "  Batch 810/3208 | Loss: 1.5870\n",
            "  Batch 820/3208 | Loss: 1.6399\n",
            "  Batch 830/3208 | Loss: 1.8339\n",
            "  Batch 840/3208 | Loss: 1.4539\n",
            "  Batch 850/3208 | Loss: 1.7143\n",
            "  Batch 860/3208 | Loss: 1.4188\n",
            "  Batch 870/3208 | Loss: 1.8896\n",
            "  Batch 880/3208 | Loss: 1.4297\n",
            "  Batch 890/3208 | Loss: 1.6644\n",
            "  Batch 900/3208 | Loss: 1.6766\n",
            "  Batch 910/3208 | Loss: 1.7364\n",
            "  Batch 920/3208 | Loss: 2.2977\n",
            "  Batch 930/3208 | Loss: 1.5017\n",
            "  Batch 940/3208 | Loss: 2.0174\n",
            "  Batch 950/3208 | Loss: 1.8743\n",
            "  Batch 960/3208 | Loss: 1.9780\n",
            "  Batch 970/3208 | Loss: 1.9100\n",
            "  Batch 980/3208 | Loss: 2.1773\n",
            "  Batch 990/3208 | Loss: 1.8274\n",
            "  Batch 1000/3208 | Loss: 1.8254\n",
            "  Batch 1010/3208 | Loss: 1.4707\n",
            "  Batch 1020/3208 | Loss: 1.7310\n",
            "  Batch 1030/3208 | Loss: 1.4572\n",
            "  Batch 1040/3208 | Loss: 2.1872\n",
            "  Batch 1050/3208 | Loss: 1.6968\n",
            "  Batch 1060/3208 | Loss: 1.7165\n",
            "  Batch 1070/3208 | Loss: 2.0606\n",
            "  Batch 1080/3208 | Loss: 1.8625\n",
            "  Batch 1090/3208 | Loss: 1.9310\n",
            "  Batch 1100/3208 | Loss: 1.9127\n",
            "  Batch 1110/3208 | Loss: 1.4095\n",
            "  Batch 1120/3208 | Loss: 1.7938\n",
            "  Batch 1130/3208 | Loss: 1.4520\n",
            "  Batch 1140/3208 | Loss: 2.0949\n",
            "  Batch 1150/3208 | Loss: 1.4281\n",
            "  Batch 1160/3208 | Loss: 1.7220\n",
            "  Batch 1170/3208 | Loss: 1.5767\n",
            "  Batch 1180/3208 | Loss: 1.7254\n",
            "  Batch 1190/3208 | Loss: 1.2524\n",
            "  Batch 1200/3208 | Loss: 1.7498\n",
            "  Batch 1210/3208 | Loss: 1.4435\n",
            "  Batch 1220/3208 | Loss: 1.3381\n",
            "  Batch 1230/3208 | Loss: 1.5421\n",
            "  Batch 1240/3208 | Loss: 1.5101\n",
            "  Batch 1250/3208 | Loss: 1.7123\n",
            "  Batch 1260/3208 | Loss: 1.7638\n",
            "  Batch 1270/3208 | Loss: 1.3613\n",
            "  Batch 1280/3208 | Loss: 1.8289\n",
            "  Batch 1290/3208 | Loss: 1.1879\n",
            "  Batch 1300/3208 | Loss: 2.1221\n",
            "  Batch 1310/3208 | Loss: 1.3989\n",
            "  Batch 1320/3208 | Loss: 2.5117\n",
            "  Batch 1330/3208 | Loss: 1.2613\n",
            "  Batch 1340/3208 | Loss: 1.2645\n",
            "  Batch 1350/3208 | Loss: 1.1764\n",
            "  Batch 1360/3208 | Loss: 1.1175\n",
            "  Batch 1370/3208 | Loss: 1.8782\n",
            "  Batch 1380/3208 | Loss: 1.5555\n",
            "  Batch 1390/3208 | Loss: 1.4956\n",
            "  Batch 1400/3208 | Loss: 1.7845\n",
            "  Batch 1410/3208 | Loss: 1.4167\n",
            "  Batch 1420/3208 | Loss: 1.8276\n",
            "  Batch 1430/3208 | Loss: 1.4971\n",
            "  Batch 1440/3208 | Loss: 1.2094\n",
            "  Batch 1450/3208 | Loss: 1.8058\n",
            "  Batch 1460/3208 | Loss: 2.0907\n",
            "  Batch 1470/3208 | Loss: 1.9697\n",
            "  Batch 1480/3208 | Loss: 1.3775\n",
            "  Batch 1490/3208 | Loss: 1.6212\n",
            "  Batch 1500/3208 | Loss: 2.5472\n",
            "  Batch 1510/3208 | Loss: 1.7218\n",
            "  Batch 1520/3208 | Loss: 2.2977\n",
            "  Batch 1530/3208 | Loss: 2.1287\n",
            "  Batch 1540/3208 | Loss: 1.8052\n",
            "  Batch 1550/3208 | Loss: 1.6408\n",
            "  Batch 1560/3208 | Loss: 1.6101\n",
            "  Batch 1570/3208 | Loss: 1.5004\n",
            "  Batch 1580/3208 | Loss: 2.0936\n",
            "  Batch 1590/3208 | Loss: 1.3230\n",
            "  Batch 1600/3208 | Loss: 1.4665\n",
            "  Batch 1610/3208 | Loss: 1.1147\n",
            "  Batch 1620/3208 | Loss: 1.3977\n",
            "  Batch 1630/3208 | Loss: 1.2881\n",
            "  Batch 1640/3208 | Loss: 1.6240\n",
            "  Batch 1650/3208 | Loss: 1.7136\n",
            "  Batch 1660/3208 | Loss: 1.5633\n",
            "  Batch 1670/3208 | Loss: 1.4729\n",
            "  Batch 1680/3208 | Loss: 1.6844\n",
            "  Batch 1690/3208 | Loss: 1.6795\n",
            "  Batch 1700/3208 | Loss: 1.5383\n",
            "  Batch 1710/3208 | Loss: 2.2316\n",
            "  Batch 1720/3208 | Loss: 1.7315\n",
            "  Batch 1730/3208 | Loss: 1.2048\n",
            "  Batch 1740/3208 | Loss: 1.1059\n",
            "  Batch 1750/3208 | Loss: 1.5964\n",
            "  Batch 1760/3208 | Loss: 1.5902\n",
            "  Batch 1770/3208 | Loss: 1.8141\n",
            "  Batch 1780/3208 | Loss: 1.6060\n",
            "  Batch 1790/3208 | Loss: 1.7508\n",
            "  Batch 1800/3208 | Loss: 1.5575\n",
            "  Batch 1810/3208 | Loss: 1.5929\n",
            "  Batch 1820/3208 | Loss: 1.7963\n",
            "  Batch 1830/3208 | Loss: 2.0140\n",
            "  Batch 1840/3208 | Loss: 1.5124\n",
            "  Batch 1850/3208 | Loss: 1.5626\n",
            "  Batch 1860/3208 | Loss: 1.3396\n",
            "  Batch 1870/3208 | Loss: 2.0459\n",
            "  Batch 1880/3208 | Loss: 2.1845\n",
            "  Batch 1890/3208 | Loss: 1.4849\n",
            "  Batch 1900/3208 | Loss: 1.4009\n",
            "  Batch 1910/3208 | Loss: 1.6913\n",
            "  Batch 1920/3208 | Loss: 1.5986\n",
            "  Batch 1930/3208 | Loss: 1.1839\n",
            "  Batch 1940/3208 | Loss: 1.5926\n",
            "  Batch 1950/3208 | Loss: 1.5530\n",
            "  Batch 1960/3208 | Loss: 1.7051\n",
            "  Batch 1970/3208 | Loss: 1.3566\n",
            "  Batch 1980/3208 | Loss: 1.5299\n",
            "  Batch 1990/3208 | Loss: 1.3552\n",
            "  Batch 2000/3208 | Loss: 1.2383\n",
            "  Batch 2010/3208 | Loss: 1.4425\n",
            "  Batch 2020/3208 | Loss: 1.2309\n",
            "  Batch 2030/3208 | Loss: 1.9473\n",
            "  Batch 2040/3208 | Loss: 1.9120\n",
            "  Batch 2050/3208 | Loss: 1.7920\n",
            "  Batch 2060/3208 | Loss: 2.0979\n",
            "  Batch 2070/3208 | Loss: 2.2452\n",
            "  Batch 2080/3208 | Loss: 2.0758\n",
            "  Batch 2090/3208 | Loss: 1.3984\n",
            "  Batch 2100/3208 | Loss: 1.7882\n",
            "  Batch 2110/3208 | Loss: 2.2495\n",
            "  Batch 2120/3208 | Loss: 1.3599\n",
            "  Batch 2130/3208 | Loss: 2.1476\n",
            "  Batch 2140/3208 | Loss: 1.2915\n",
            "  Batch 2150/3208 | Loss: 1.7829\n",
            "  Batch 2160/3208 | Loss: 1.2928\n",
            "  Batch 2170/3208 | Loss: 1.4295\n",
            "  Batch 2180/3208 | Loss: 1.6412\n",
            "  Batch 2190/3208 | Loss: 1.6213\n",
            "  Batch 2200/3208 | Loss: 1.3848\n",
            "  Batch 2210/3208 | Loss: 1.7065\n",
            "  Batch 2220/3208 | Loss: 1.8676\n",
            "  Batch 2230/3208 | Loss: 1.2972\n",
            "  Batch 2240/3208 | Loss: 1.2487\n",
            "  Batch 2250/3208 | Loss: 1.3600\n",
            "  Batch 2260/3208 | Loss: 1.3850\n",
            "  Batch 2270/3208 | Loss: 1.4864\n",
            "  Batch 2280/3208 | Loss: 1.6019\n",
            "  Batch 2290/3208 | Loss: 1.7352\n",
            "  Batch 2300/3208 | Loss: 1.4305\n",
            "  Batch 2310/3208 | Loss: 1.2054\n",
            "  Batch 2320/3208 | Loss: 2.0889\n",
            "  Batch 2330/3208 | Loss: 1.7835\n",
            "  Batch 2340/3208 | Loss: 1.2858\n",
            "  Batch 2350/3208 | Loss: 1.6870\n",
            "  Batch 2360/3208 | Loss: 1.1994\n",
            "  Batch 2370/3208 | Loss: 1.2529\n",
            "  Batch 2380/3208 | Loss: 1.8056\n",
            "  Batch 2390/3208 | Loss: 1.2022\n",
            "  Batch 2400/3208 | Loss: 1.3888\n",
            "  Batch 2410/3208 | Loss: 1.4027\n",
            "  Batch 2420/3208 | Loss: 1.7253\n",
            "  Batch 2430/3208 | Loss: 1.9187\n",
            "  Batch 2440/3208 | Loss: 2.1962\n",
            "  Batch 2450/3208 | Loss: 1.5772\n",
            "  Batch 2460/3208 | Loss: 1.8501\n",
            "  Batch 2470/3208 | Loss: 1.7910\n",
            "  Batch 2480/3208 | Loss: 1.9518\n",
            "  Batch 2490/3208 | Loss: 1.6691\n",
            "  Batch 2500/3208 | Loss: 1.6131\n",
            "  Batch 2510/3208 | Loss: 1.3159\n",
            "  Batch 2520/3208 | Loss: 1.6858\n",
            "  Batch 2530/3208 | Loss: 1.5476\n",
            "  Batch 2540/3208 | Loss: 1.3163\n",
            "  Batch 2550/3208 | Loss: 1.9724\n",
            "  Batch 2560/3208 | Loss: 1.6110\n",
            "  Batch 2570/3208 | Loss: 1.9630\n",
            "  Batch 2580/3208 | Loss: 1.6419\n",
            "  Batch 2590/3208 | Loss: 1.1138\n",
            "  Batch 2600/3208 | Loss: 1.4558\n",
            "  Batch 2610/3208 | Loss: 1.4065\n",
            "  Batch 2620/3208 | Loss: 1.8466\n",
            "  Batch 2630/3208 | Loss: 1.1608\n",
            "  Batch 2640/3208 | Loss: 1.2830\n",
            "  Batch 2650/3208 | Loss: 1.1489\n",
            "  Batch 2660/3208 | Loss: 1.6213\n",
            "  Batch 2670/3208 | Loss: 1.4103\n",
            "  Batch 2680/3208 | Loss: 1.9348\n",
            "  Batch 2690/3208 | Loss: 1.7639\n",
            "  Batch 2700/3208 | Loss: 2.5480\n",
            "  Batch 2710/3208 | Loss: 1.7346\n",
            "  Batch 2720/3208 | Loss: 1.2479\n",
            "  Batch 2730/3208 | Loss: 1.5917\n",
            "  Batch 2740/3208 | Loss: 1.4323\n",
            "  Batch 2750/3208 | Loss: 1.5110\n",
            "  Batch 2760/3208 | Loss: 1.5336\n",
            "  Batch 2770/3208 | Loss: 1.6794\n",
            "  Batch 2780/3208 | Loss: 1.4333\n",
            "  Batch 2790/3208 | Loss: 1.5852\n",
            "  Batch 2800/3208 | Loss: 1.5151\n",
            "  Batch 2810/3208 | Loss: 1.8852\n",
            "  Batch 2820/3208 | Loss: 1.6180\n",
            "  Batch 2830/3208 | Loss: 1.2281\n",
            "  Batch 2840/3208 | Loss: 1.4779\n",
            "  Batch 2850/3208 | Loss: 1.9548\n",
            "  Batch 2860/3208 | Loss: 1.6767\n",
            "  Batch 2870/3208 | Loss: 2.0770\n",
            "  Batch 2880/3208 | Loss: 1.2273\n",
            "  Batch 2890/3208 | Loss: 1.2745\n",
            "  Batch 2900/3208 | Loss: 1.1896\n",
            "  Batch 2910/3208 | Loss: 1.6470\n",
            "  Batch 2920/3208 | Loss: 1.6061\n",
            "  Batch 2930/3208 | Loss: 1.7644\n",
            "  Batch 2940/3208 | Loss: 1.2780\n",
            "  Batch 2950/3208 | Loss: 1.8921\n",
            "  Batch 2960/3208 | Loss: 1.3455\n",
            "  Batch 2970/3208 | Loss: 1.3824\n",
            "  Batch 2980/3208 | Loss: 1.8146\n",
            "  Batch 2990/3208 | Loss: 1.8912\n",
            "  Batch 3000/3208 | Loss: 1.9718\n",
            "  Batch 3010/3208 | Loss: 1.4747\n",
            "  Batch 3020/3208 | Loss: 1.4473\n",
            "  Batch 3030/3208 | Loss: 1.5770\n",
            "  Batch 3040/3208 | Loss: 1.2637\n",
            "  Batch 3050/3208 | Loss: 1.3287\n",
            "  Batch 3060/3208 | Loss: 1.5106\n",
            "  Batch 3070/3208 | Loss: 1.8463\n",
            "  Batch 3080/3208 | Loss: 1.4821\n",
            "  Batch 3090/3208 | Loss: 1.5540\n",
            "  Batch 3100/3208 | Loss: 1.6791\n",
            "  Batch 3110/3208 | Loss: 1.3452\n",
            "  Batch 3120/3208 | Loss: 1.7089\n",
            "  Batch 3130/3208 | Loss: 1.5150\n",
            "  Batch 3140/3208 | Loss: 1.6873\n",
            "  Batch 3150/3208 | Loss: 1.8626\n",
            "  Batch 3160/3208 | Loss: 1.6882\n",
            "  Batch 3170/3208 | Loss: 1.8480\n",
            "  Batch 3180/3208 | Loss: 1.4314\n",
            "  Batch 3190/3208 | Loss: 1.8763\n",
            "  Batch 3200/3208 | Loss: 1.5983\n",
            " Epoch 8 Completed | Avg Loss: 1.6489 | Training Accuracy: 52.86%\n",
            " Validation Accuracy: 58.15%\n",
            " Best Model Saved (Epoch 8 | Accuracy: 58.15%)\n",
            "\n",
            " Epoch 9/20\n",
            "  Batch 10/3208 | Loss: 1.4551\n",
            "  Batch 20/3208 | Loss: 1.9577\n",
            "  Batch 30/3208 | Loss: 1.6013\n",
            "  Batch 40/3208 | Loss: 1.3651\n",
            "  Batch 50/3208 | Loss: 2.2670\n",
            "  Batch 60/3208 | Loss: 1.4818\n",
            "  Batch 70/3208 | Loss: 1.5141\n",
            "  Batch 80/3208 | Loss: 1.6707\n",
            "  Batch 90/3208 | Loss: 1.7753\n",
            "  Batch 100/3208 | Loss: 1.4601\n",
            "  Batch 110/3208 | Loss: 1.5695\n",
            "  Batch 120/3208 | Loss: 1.6561\n",
            "  Batch 130/3208 | Loss: 1.6755\n",
            "  Batch 140/3208 | Loss: 1.4290\n",
            "  Batch 150/3208 | Loss: 1.2801\n",
            "  Batch 160/3208 | Loss: 1.3661\n",
            "  Batch 170/3208 | Loss: 1.9029\n",
            "  Batch 180/3208 | Loss: 1.8860\n",
            "  Batch 190/3208 | Loss: 1.5062\n",
            "  Batch 200/3208 | Loss: 1.3296\n",
            "  Batch 210/3208 | Loss: 1.4667\n",
            "  Batch 220/3208 | Loss: 1.6542\n",
            "  Batch 230/3208 | Loss: 1.2851\n",
            "  Batch 240/3208 | Loss: 1.5876\n",
            "  Batch 250/3208 | Loss: 1.2789\n",
            "  Batch 260/3208 | Loss: 1.5144\n",
            "  Batch 270/3208 | Loss: 1.5346\n",
            "  Batch 280/3208 | Loss: 1.8231\n",
            "  Batch 290/3208 | Loss: 1.3048\n",
            "  Batch 300/3208 | Loss: 1.2661\n",
            "  Batch 310/3208 | Loss: 1.2140\n",
            "  Batch 320/3208 | Loss: 1.4267\n",
            "  Batch 330/3208 | Loss: 1.2673\n",
            "  Batch 340/3208 | Loss: 1.6309\n",
            "  Batch 350/3208 | Loss: 1.9745\n",
            "  Batch 360/3208 | Loss: 1.4799\n",
            "  Batch 370/3208 | Loss: 1.6179\n",
            "  Batch 380/3208 | Loss: 1.3483\n",
            "  Batch 390/3208 | Loss: 1.6312\n",
            "  Batch 400/3208 | Loss: 1.2379\n",
            "  Batch 410/3208 | Loss: 1.8445\n",
            "  Batch 420/3208 | Loss: 1.2461\n",
            "  Batch 430/3208 | Loss: 1.6492\n",
            "  Batch 440/3208 | Loss: 1.3789\n",
            "  Batch 450/3208 | Loss: 1.7281\n",
            "  Batch 460/3208 | Loss: 1.8187\n",
            "  Batch 470/3208 | Loss: 1.7282\n",
            "  Batch 480/3208 | Loss: 1.8590\n",
            "  Batch 490/3208 | Loss: 1.7579\n",
            "  Batch 500/3208 | Loss: 1.6516\n",
            "  Batch 510/3208 | Loss: 1.3294\n",
            "  Batch 520/3208 | Loss: 1.7024\n",
            "  Batch 530/3208 | Loss: 1.3805\n",
            "  Batch 540/3208 | Loss: 1.9161\n",
            "  Batch 550/3208 | Loss: 1.3500\n",
            "  Batch 560/3208 | Loss: 1.7334\n",
            "  Batch 570/3208 | Loss: 1.3476\n",
            "  Batch 580/3208 | Loss: 1.3045\n",
            "  Batch 590/3208 | Loss: 1.5462\n",
            "  Batch 600/3208 | Loss: 2.2539\n",
            "  Batch 610/3208 | Loss: 1.4665\n",
            "  Batch 620/3208 | Loss: 1.3491\n",
            "  Batch 630/3208 | Loss: 1.5739\n",
            "  Batch 640/3208 | Loss: 1.6340\n",
            "  Batch 650/3208 | Loss: 1.8428\n",
            "  Batch 660/3208 | Loss: 1.6480\n",
            "  Batch 670/3208 | Loss: 2.0779\n",
            "  Batch 680/3208 | Loss: 1.6214\n",
            "  Batch 690/3208 | Loss: 1.7371\n",
            "  Batch 700/3208 | Loss: 2.2404\n",
            "  Batch 710/3208 | Loss: 1.1552\n",
            "  Batch 720/3208 | Loss: 1.3245\n",
            "  Batch 730/3208 | Loss: 1.5116\n",
            "  Batch 740/3208 | Loss: 2.2908\n",
            "  Batch 750/3208 | Loss: 1.5252\n",
            "  Batch 760/3208 | Loss: 1.4953\n",
            "  Batch 770/3208 | Loss: 1.3789\n",
            "  Batch 780/3208 | Loss: 1.2837\n",
            "  Batch 790/3208 | Loss: 1.7235\n",
            "  Batch 800/3208 | Loss: 1.2656\n",
            "  Batch 810/3208 | Loss: 1.6620\n",
            "  Batch 820/3208 | Loss: 1.3722\n",
            "  Batch 830/3208 | Loss: 1.6391\n",
            "  Batch 840/3208 | Loss: 1.2664\n",
            "  Batch 850/3208 | Loss: 1.9446\n",
            "  Batch 860/3208 | Loss: 1.7325\n",
            "  Batch 870/3208 | Loss: 1.4905\n",
            "  Batch 880/3208 | Loss: 1.8314\n",
            "  Batch 890/3208 | Loss: 1.8288\n",
            "  Batch 900/3208 | Loss: 1.6741\n",
            "  Batch 910/3208 | Loss: 1.1746\n",
            "  Batch 920/3208 | Loss: 1.8598\n",
            "  Batch 930/3208 | Loss: 2.0194\n",
            "  Batch 940/3208 | Loss: 1.5525\n",
            "  Batch 950/3208 | Loss: 1.8585\n",
            "  Batch 960/3208 | Loss: 1.3976\n",
            "  Batch 970/3208 | Loss: 1.2391\n",
            "  Batch 980/3208 | Loss: 1.3565\n",
            "  Batch 990/3208 | Loss: 1.5433\n",
            "  Batch 1000/3208 | Loss: 1.4345\n",
            "  Batch 1010/3208 | Loss: 1.7235\n",
            "  Batch 1020/3208 | Loss: 1.3440\n",
            "  Batch 1030/3208 | Loss: 1.3975\n",
            "  Batch 1040/3208 | Loss: 1.3272\n",
            "  Batch 1050/3208 | Loss: 1.8767\n",
            "  Batch 1060/3208 | Loss: 1.6647\n",
            "  Batch 1070/3208 | Loss: 1.4305\n",
            "  Batch 1080/3208 | Loss: 2.0942\n",
            "  Batch 1090/3208 | Loss: 1.6106\n",
            "  Batch 1100/3208 | Loss: 1.0952\n",
            "  Batch 1110/3208 | Loss: 1.8200\n",
            "  Batch 1120/3208 | Loss: 1.5154\n",
            "  Batch 1130/3208 | Loss: 1.2917\n",
            "  Batch 1140/3208 | Loss: 2.1493\n",
            "  Batch 1150/3208 | Loss: 1.7444\n",
            "  Batch 1160/3208 | Loss: 1.5025\n",
            "  Batch 1170/3208 | Loss: 1.5798\n",
            "  Batch 1180/3208 | Loss: 1.6953\n",
            "  Batch 1190/3208 | Loss: 1.2687\n",
            "  Batch 1200/3208 | Loss: 1.5463\n",
            "  Batch 1210/3208 | Loss: 0.9738\n",
            "  Batch 1220/3208 | Loss: 1.2313\n",
            "  Batch 1230/3208 | Loss: 1.1446\n",
            "  Batch 1240/3208 | Loss: 1.6185\n",
            "  Batch 1250/3208 | Loss: 1.2529\n",
            "  Batch 1260/3208 | Loss: 1.7238\n",
            "  Batch 1270/3208 | Loss: 1.4417\n",
            "  Batch 1280/3208 | Loss: 1.5128\n",
            "  Batch 1290/3208 | Loss: 1.4967\n",
            "  Batch 1300/3208 | Loss: 1.5628\n",
            "  Batch 1310/3208 | Loss: 2.0091\n",
            "  Batch 1320/3208 | Loss: 1.7613\n",
            "  Batch 1330/3208 | Loss: 1.5588\n",
            "  Batch 1340/3208 | Loss: 1.7277\n",
            "  Batch 1350/3208 | Loss: 1.7036\n",
            "  Batch 1360/3208 | Loss: 1.2380\n",
            "  Batch 1370/3208 | Loss: 1.5191\n",
            "  Batch 1380/3208 | Loss: 1.4929\n",
            "  Batch 1390/3208 | Loss: 2.0655\n",
            "  Batch 1400/3208 | Loss: 1.7709\n",
            "  Batch 1410/3208 | Loss: 1.6316\n",
            "  Batch 1420/3208 | Loss: 1.5195\n",
            "  Batch 1430/3208 | Loss: 1.2260\n",
            "  Batch 1440/3208 | Loss: 1.2855\n",
            "  Batch 1450/3208 | Loss: 1.3727\n",
            "  Batch 1460/3208 | Loss: 1.6672\n",
            "  Batch 1470/3208 | Loss: 1.1415\n",
            "  Batch 1480/3208 | Loss: 1.2444\n",
            "  Batch 1490/3208 | Loss: 1.1123\n",
            "  Batch 1500/3208 | Loss: 1.5975\n",
            "  Batch 1510/3208 | Loss: 1.2154\n",
            "  Batch 1520/3208 | Loss: 1.8339\n",
            "  Batch 1530/3208 | Loss: 1.2755\n",
            "  Batch 1540/3208 | Loss: 1.2564\n",
            "  Batch 1550/3208 | Loss: 2.2077\n",
            "  Batch 1560/3208 | Loss: 2.1946\n",
            "  Batch 1570/3208 | Loss: 1.4353\n",
            "  Batch 1580/3208 | Loss: 1.4858\n",
            "  Batch 1590/3208 | Loss: 1.6276\n",
            "  Batch 1600/3208 | Loss: 1.7278\n",
            "  Batch 1610/3208 | Loss: 1.3840\n",
            "  Batch 1620/3208 | Loss: 1.7341\n",
            "  Batch 1630/3208 | Loss: 1.3562\n",
            "  Batch 1640/3208 | Loss: 1.4403\n",
            "  Batch 1650/3208 | Loss: 2.0319\n",
            "  Batch 1660/3208 | Loss: 1.7346\n",
            "  Batch 1670/3208 | Loss: 1.9378\n",
            "  Batch 1680/3208 | Loss: 2.2430\n",
            "  Batch 1690/3208 | Loss: 1.2020\n",
            "  Batch 1700/3208 | Loss: 1.5484\n",
            "  Batch 1710/3208 | Loss: 2.1354\n",
            "  Batch 1720/3208 | Loss: 1.2336\n",
            "  Batch 1730/3208 | Loss: 0.8431\n",
            "  Batch 1740/3208 | Loss: 1.9358\n",
            "  Batch 1750/3208 | Loss: 1.3044\n",
            "  Batch 1760/3208 | Loss: 1.3959\n",
            "  Batch 1770/3208 | Loss: 1.0176\n",
            "  Batch 1780/3208 | Loss: 1.4752\n",
            "  Batch 1790/3208 | Loss: 1.5443\n",
            "  Batch 1800/3208 | Loss: 1.8987\n",
            "  Batch 1810/3208 | Loss: 1.4297\n",
            "  Batch 1820/3208 | Loss: 1.6399\n",
            "  Batch 1830/3208 | Loss: 1.6483\n",
            "  Batch 1840/3208 | Loss: 1.9383\n",
            "  Batch 1850/3208 | Loss: 1.4732\n",
            "  Batch 1860/3208 | Loss: 1.6215\n",
            "  Batch 1870/3208 | Loss: 1.3403\n",
            "  Batch 1880/3208 | Loss: 1.7863\n",
            "  Batch 1890/3208 | Loss: 1.9872\n",
            "  Batch 1900/3208 | Loss: 1.5955\n",
            "  Batch 1910/3208 | Loss: 1.4234\n",
            "  Batch 1920/3208 | Loss: 1.5434\n",
            "  Batch 1930/3208 | Loss: 1.7076\n",
            "  Batch 1940/3208 | Loss: 1.7733\n",
            "  Batch 1950/3208 | Loss: 1.5141\n",
            "  Batch 1960/3208 | Loss: 1.7195\n",
            "  Batch 1970/3208 | Loss: 1.6790\n",
            "  Batch 1980/3208 | Loss: 1.6591\n",
            "  Batch 1990/3208 | Loss: 1.2145\n",
            "  Batch 2000/3208 | Loss: 1.2463\n",
            "  Batch 2010/3208 | Loss: 1.5342\n",
            "  Batch 2020/3208 | Loss: 1.5169\n",
            "  Batch 2030/3208 | Loss: 1.2505\n",
            "  Batch 2040/3208 | Loss: 1.3143\n",
            "  Batch 2050/3208 | Loss: 1.2913\n",
            "  Batch 2060/3208 | Loss: 1.7282\n",
            "  Batch 2070/3208 | Loss: 1.7508\n",
            "  Batch 2080/3208 | Loss: 1.2671\n",
            "  Batch 2090/3208 | Loss: 1.4301\n",
            "  Batch 2100/3208 | Loss: 2.2009\n",
            "  Batch 2110/3208 | Loss: 1.3774\n",
            "  Batch 2120/3208 | Loss: 1.6848\n",
            "  Batch 2130/3208 | Loss: 1.3900\n",
            "  Batch 2140/3208 | Loss: 1.7299\n",
            "  Batch 2150/3208 | Loss: 1.5672\n",
            "  Batch 2160/3208 | Loss: 1.2757\n",
            "  Batch 2170/3208 | Loss: 1.1697\n",
            "  Batch 2180/3208 | Loss: 1.9326\n",
            "  Batch 2190/3208 | Loss: 1.7043\n",
            "  Batch 2200/3208 | Loss: 1.4946\n",
            "  Batch 2210/3208 | Loss: 1.3125\n",
            "  Batch 2220/3208 | Loss: 1.2538\n",
            "  Batch 2230/3208 | Loss: 1.4688\n",
            "  Batch 2240/3208 | Loss: 1.8624\n",
            "  Batch 2250/3208 | Loss: 1.4396\n",
            "  Batch 2260/3208 | Loss: 1.5056\n",
            "  Batch 2270/3208 | Loss: 1.3018\n",
            "  Batch 2280/3208 | Loss: 1.9947\n",
            "  Batch 2290/3208 | Loss: 2.3563\n",
            "  Batch 2300/3208 | Loss: 1.3670\n",
            "  Batch 2310/3208 | Loss: 1.3565\n",
            "  Batch 2320/3208 | Loss: 1.3309\n",
            "  Batch 2330/3208 | Loss: 1.1202\n",
            "  Batch 2340/3208 | Loss: 1.3649\n",
            "  Batch 2350/3208 | Loss: 1.3133\n",
            "  Batch 2360/3208 | Loss: 1.2279\n",
            "  Batch 2370/3208 | Loss: 1.1821\n",
            "  Batch 2380/3208 | Loss: 1.5871\n",
            "  Batch 2390/3208 | Loss: 1.6367\n",
            "  Batch 2400/3208 | Loss: 1.4705\n",
            "  Batch 2410/3208 | Loss: 1.7248\n",
            "  Batch 2420/3208 | Loss: 1.8122\n",
            "  Batch 2430/3208 | Loss: 1.5869\n",
            "  Batch 2440/3208 | Loss: 1.1412\n",
            "  Batch 2450/3208 | Loss: 1.5391\n",
            "  Batch 2460/3208 | Loss: 1.8421\n",
            "  Batch 2470/3208 | Loss: 1.2552\n",
            "  Batch 2480/3208 | Loss: 0.9750\n",
            "  Batch 2490/3208 | Loss: 1.3976\n",
            "  Batch 2500/3208 | Loss: 1.7059\n",
            "  Batch 2510/3208 | Loss: 1.3843\n",
            "  Batch 2520/3208 | Loss: 1.4747\n",
            "  Batch 2530/3208 | Loss: 2.3042\n",
            "  Batch 2540/3208 | Loss: 2.2369\n",
            "  Batch 2550/3208 | Loss: 1.4830\n",
            "  Batch 2560/3208 | Loss: 1.5329\n",
            "  Batch 2570/3208 | Loss: 1.4212\n",
            "  Batch 2580/3208 | Loss: 1.5620\n",
            "  Batch 2590/3208 | Loss: 1.4825\n",
            "  Batch 2600/3208 | Loss: 2.1510\n",
            "  Batch 2610/3208 | Loss: 1.4637\n",
            "  Batch 2620/3208 | Loss: 1.5188\n",
            "  Batch 2630/3208 | Loss: 1.2286\n",
            "  Batch 2640/3208 | Loss: 1.0709\n",
            "  Batch 2650/3208 | Loss: 1.3825\n",
            "  Batch 2660/3208 | Loss: 1.0389\n",
            "  Batch 2670/3208 | Loss: 1.5308\n",
            "  Batch 2680/3208 | Loss: 1.6142\n",
            "  Batch 2690/3208 | Loss: 0.9270\n",
            "  Batch 2700/3208 | Loss: 1.0868\n",
            "  Batch 2710/3208 | Loss: 0.9791\n",
            "  Batch 2720/3208 | Loss: 1.5095\n",
            "  Batch 2730/3208 | Loss: 1.6086\n",
            "  Batch 2740/3208 | Loss: 1.4603\n",
            "  Batch 2750/3208 | Loss: 1.4751\n",
            "  Batch 2760/3208 | Loss: 1.4044\n",
            "  Batch 2770/3208 | Loss: 1.5928\n",
            "  Batch 2780/3208 | Loss: 1.4641\n",
            "  Batch 2790/3208 | Loss: 1.8357\n",
            "  Batch 2800/3208 | Loss: 1.8951\n",
            "  Batch 2810/3208 | Loss: 1.3731\n",
            "  Batch 2820/3208 | Loss: 1.6638\n",
            "  Batch 2830/3208 | Loss: 1.3876\n",
            "  Batch 2840/3208 | Loss: 1.7419\n",
            "  Batch 2850/3208 | Loss: 1.2618\n",
            "  Batch 2860/3208 | Loss: 1.3235\n",
            "  Batch 2870/3208 | Loss: 1.5123\n",
            "  Batch 2880/3208 | Loss: 1.4007\n",
            "  Batch 2890/3208 | Loss: 1.3434\n",
            "  Batch 2900/3208 | Loss: 1.1072\n",
            "  Batch 2910/3208 | Loss: 1.2990\n",
            "  Batch 2920/3208 | Loss: 0.9898\n",
            "  Batch 2930/3208 | Loss: 1.1207\n",
            "  Batch 2940/3208 | Loss: 1.2181\n",
            "  Batch 2950/3208 | Loss: 2.1916\n",
            "  Batch 2960/3208 | Loss: 1.4932\n",
            "  Batch 2970/3208 | Loss: 1.2121\n",
            "  Batch 2980/3208 | Loss: 1.3753\n",
            "  Batch 2990/3208 | Loss: 2.1835\n",
            "  Batch 3000/3208 | Loss: 1.4752\n",
            "  Batch 3010/3208 | Loss: 1.4450\n",
            "  Batch 3020/3208 | Loss: 1.4629\n",
            "  Batch 3030/3208 | Loss: 1.7592\n",
            "  Batch 3040/3208 | Loss: 1.7492\n",
            "  Batch 3050/3208 | Loss: 1.1155\n",
            "  Batch 3060/3208 | Loss: 2.0305\n",
            "  Batch 3070/3208 | Loss: 1.9561\n",
            "  Batch 3080/3208 | Loss: 1.2823\n",
            "  Batch 3090/3208 | Loss: 1.6899\n",
            "  Batch 3100/3208 | Loss: 1.1423\n",
            "  Batch 3110/3208 | Loss: 1.5675\n",
            "  Batch 3120/3208 | Loss: 1.7497\n",
            "  Batch 3130/3208 | Loss: 1.6952\n",
            "  Batch 3140/3208 | Loss: 0.8298\n",
            "  Batch 3150/3208 | Loss: 1.4172\n",
            "  Batch 3160/3208 | Loss: 1.3517\n",
            "  Batch 3170/3208 | Loss: 1.4763\n",
            "  Batch 3180/3208 | Loss: 1.5855\n",
            "  Batch 3190/3208 | Loss: 1.3541\n",
            "  Batch 3200/3208 | Loss: 1.3459\n",
            " Epoch 9 Completed | Avg Loss: 1.5387 | Training Accuracy: 56.07%\n",
            " Validation Accuracy: 60.62%\n",
            " Best Model Saved (Epoch 9 | Accuracy: 60.62%)\n",
            "\n",
            " Epoch 10/20\n",
            "  Batch 10/3208 | Loss: 1.5512\n",
            "  Batch 20/3208 | Loss: 1.6377\n",
            "  Batch 30/3208 | Loss: 1.6937\n",
            "  Batch 40/3208 | Loss: 1.0896\n",
            "  Batch 50/3208 | Loss: 1.6716\n",
            "  Batch 60/3208 | Loss: 1.3389\n",
            "  Batch 70/3208 | Loss: 1.4153\n",
            "  Batch 80/3208 | Loss: 1.6034\n",
            "  Batch 90/3208 | Loss: 1.5010\n",
            "  Batch 100/3208 | Loss: 1.1092\n",
            "  Batch 110/3208 | Loss: 1.5011\n",
            "  Batch 120/3208 | Loss: 2.1382\n",
            "  Batch 130/3208 | Loss: 1.0958\n",
            "  Batch 140/3208 | Loss: 2.1888\n",
            "  Batch 150/3208 | Loss: 1.6956\n",
            "  Batch 160/3208 | Loss: 1.7231\n",
            "  Batch 170/3208 | Loss: 1.3242\n",
            "  Batch 180/3208 | Loss: 1.6086\n",
            "  Batch 190/3208 | Loss: 1.4836\n",
            "  Batch 200/3208 | Loss: 1.1873\n",
            "  Batch 210/3208 | Loss: 1.2012\n",
            "  Batch 220/3208 | Loss: 1.6869\n",
            "  Batch 230/3208 | Loss: 1.4965\n",
            "  Batch 240/3208 | Loss: 1.4378\n",
            "  Batch 250/3208 | Loss: 1.4773\n",
            "  Batch 260/3208 | Loss: 1.1501\n",
            "  Batch 270/3208 | Loss: 1.4866\n",
            "  Batch 280/3208 | Loss: 1.6804\n",
            "  Batch 290/3208 | Loss: 1.5659\n",
            "  Batch 300/3208 | Loss: 1.2640\n",
            "  Batch 310/3208 | Loss: 1.4543\n",
            "  Batch 320/3208 | Loss: 1.1726\n",
            "  Batch 330/3208 | Loss: 1.7628\n",
            "  Batch 340/3208 | Loss: 1.1636\n",
            "  Batch 350/3208 | Loss: 1.2599\n",
            "  Batch 360/3208 | Loss: 1.4792\n",
            "  Batch 370/3208 | Loss: 1.1695\n",
            "  Batch 380/3208 | Loss: 1.8013\n",
            "  Batch 390/3208 | Loss: 1.2213\n",
            "  Batch 400/3208 | Loss: 1.0522\n",
            "  Batch 410/3208 | Loss: 1.4914\n",
            "  Batch 420/3208 | Loss: 1.7612\n",
            "  Batch 430/3208 | Loss: 1.4755\n",
            "  Batch 440/3208 | Loss: 1.5270\n",
            "  Batch 450/3208 | Loss: 1.1032\n",
            "  Batch 460/3208 | Loss: 2.4835\n",
            "  Batch 470/3208 | Loss: 1.4562\n",
            "  Batch 480/3208 | Loss: 1.4694\n",
            "  Batch 490/3208 | Loss: 1.5612\n",
            "  Batch 500/3208 | Loss: 1.1237\n",
            "  Batch 510/3208 | Loss: 1.5404\n",
            "  Batch 520/3208 | Loss: 1.5030\n",
            "  Batch 530/3208 | Loss: 1.4228\n",
            "  Batch 540/3208 | Loss: 1.3970\n",
            "  Batch 550/3208 | Loss: 1.5356\n",
            "  Batch 560/3208 | Loss: 1.6422\n",
            "  Batch 570/3208 | Loss: 1.5142\n",
            "  Batch 580/3208 | Loss: 1.5609\n",
            "  Batch 590/3208 | Loss: 1.2286\n",
            "  Batch 600/3208 | Loss: 1.5052\n",
            "  Batch 610/3208 | Loss: 1.3252\n",
            "  Batch 620/3208 | Loss: 1.2163\n",
            "  Batch 630/3208 | Loss: 1.6290\n",
            "  Batch 640/3208 | Loss: 1.5350\n",
            "  Batch 650/3208 | Loss: 1.2479\n",
            "  Batch 660/3208 | Loss: 1.3303\n",
            "  Batch 670/3208 | Loss: 1.1854\n",
            "  Batch 680/3208 | Loss: 1.1059\n",
            "  Batch 690/3208 | Loss: 1.7723\n",
            "  Batch 700/3208 | Loss: 1.4243\n",
            "  Batch 710/3208 | Loss: 1.2024\n",
            "  Batch 720/3208 | Loss: 1.3100\n",
            "  Batch 730/3208 | Loss: 1.0564\n",
            "  Batch 740/3208 | Loss: 1.7183\n",
            "  Batch 750/3208 | Loss: 1.7254\n",
            "  Batch 760/3208 | Loss: 2.2745\n",
            "  Batch 770/3208 | Loss: 1.7650\n",
            "  Batch 780/3208 | Loss: 1.0496\n",
            "  Batch 790/3208 | Loss: 1.3210\n",
            "  Batch 800/3208 | Loss: 1.4456\n",
            "  Batch 810/3208 | Loss: 1.2346\n",
            "  Batch 820/3208 | Loss: 1.3505\n",
            "  Batch 830/3208 | Loss: 1.1273\n",
            "  Batch 840/3208 | Loss: 1.8720\n",
            "  Batch 850/3208 | Loss: 2.0699\n",
            "  Batch 860/3208 | Loss: 1.4128\n",
            "  Batch 870/3208 | Loss: 1.4041\n",
            "  Batch 880/3208 | Loss: 1.2725\n",
            "  Batch 890/3208 | Loss: 1.4848\n",
            "  Batch 900/3208 | Loss: 1.3345\n",
            "  Batch 910/3208 | Loss: 1.2040\n",
            "  Batch 920/3208 | Loss: 1.4511\n",
            "  Batch 930/3208 | Loss: 1.5344\n",
            "  Batch 940/3208 | Loss: 1.5080\n",
            "  Batch 950/3208 | Loss: 1.3151\n",
            "  Batch 960/3208 | Loss: 1.8088\n",
            "  Batch 970/3208 | Loss: 1.1101\n",
            "  Batch 980/3208 | Loss: 1.3951\n",
            "  Batch 990/3208 | Loss: 1.6104\n",
            "  Batch 1000/3208 | Loss: 1.6812\n",
            "  Batch 1010/3208 | Loss: 1.1769\n",
            "  Batch 1020/3208 | Loss: 1.5386\n",
            "  Batch 1030/3208 | Loss: 1.6357\n",
            "  Batch 1040/3208 | Loss: 1.5063\n",
            "  Batch 1050/3208 | Loss: 1.9795\n",
            "  Batch 1060/3208 | Loss: 1.1269\n",
            "  Batch 1070/3208 | Loss: 1.2184\n",
            "  Batch 1080/3208 | Loss: 1.7340\n",
            "  Batch 1090/3208 | Loss: 1.4963\n",
            "  Batch 1100/3208 | Loss: 1.4085\n",
            "  Batch 1110/3208 | Loss: 1.2979\n",
            "  Batch 1120/3208 | Loss: 1.3949\n",
            "  Batch 1130/3208 | Loss: 1.7549\n",
            "  Batch 1140/3208 | Loss: 1.4114\n",
            "  Batch 1150/3208 | Loss: 1.0985\n",
            "  Batch 1160/3208 | Loss: 1.4616\n",
            "  Batch 1170/3208 | Loss: 1.4500\n",
            "  Batch 1180/3208 | Loss: 1.2982\n",
            "  Batch 1190/3208 | Loss: 1.7229\n",
            "  Batch 1200/3208 | Loss: 1.8665\n",
            "  Batch 1210/3208 | Loss: 1.2121\n",
            "  Batch 1220/3208 | Loss: 1.5287\n",
            "  Batch 1230/3208 | Loss: 1.8702\n",
            "  Batch 1240/3208 | Loss: 1.3691\n",
            "  Batch 1250/3208 | Loss: 1.3207\n",
            "  Batch 1260/3208 | Loss: 1.4644\n",
            "  Batch 1270/3208 | Loss: 1.3812\n",
            "  Batch 1280/3208 | Loss: 1.7331\n",
            "  Batch 1290/3208 | Loss: 1.2268\n",
            "  Batch 1300/3208 | Loss: 1.2907\n",
            "  Batch 1310/3208 | Loss: 1.4342\n",
            "  Batch 1320/3208 | Loss: 1.8227\n",
            "  Batch 1330/3208 | Loss: 1.4270\n",
            "  Batch 1340/3208 | Loss: 1.4165\n",
            "  Batch 1350/3208 | Loss: 1.7187\n",
            "  Batch 1360/3208 | Loss: 0.9748\n",
            "  Batch 1370/3208 | Loss: 1.2260\n",
            "  Batch 1380/3208 | Loss: 1.3268\n",
            "  Batch 1390/3208 | Loss: 1.2316\n",
            "  Batch 1400/3208 | Loss: 1.0034\n",
            "  Batch 1410/3208 | Loss: 1.3940\n",
            "  Batch 1420/3208 | Loss: 1.4623\n",
            "  Batch 1430/3208 | Loss: 1.4092\n",
            "  Batch 1440/3208 | Loss: 1.8779\n",
            "  Batch 1450/3208 | Loss: 1.4673\n",
            "  Batch 1460/3208 | Loss: 1.2850\n",
            "  Batch 1470/3208 | Loss: 1.4475\n",
            "  Batch 1480/3208 | Loss: 1.7305\n",
            "  Batch 1490/3208 | Loss: 1.7273\n",
            "  Batch 1500/3208 | Loss: 1.7498\n",
            "  Batch 1510/3208 | Loss: 1.6730\n",
            "  Batch 1520/3208 | Loss: 1.3109\n",
            "  Batch 1530/3208 | Loss: 1.0767\n",
            "  Batch 1540/3208 | Loss: 1.0771\n",
            "  Batch 1550/3208 | Loss: 1.1274\n",
            "  Batch 1560/3208 | Loss: 1.2250\n",
            "  Batch 1570/3208 | Loss: 1.2821\n",
            "  Batch 1580/3208 | Loss: 1.2716\n",
            "  Batch 1590/3208 | Loss: 1.6114\n",
            "  Batch 1600/3208 | Loss: 0.9167\n",
            "  Batch 1610/3208 | Loss: 1.6415\n",
            "  Batch 1620/3208 | Loss: 1.1266\n",
            "  Batch 1630/3208 | Loss: 1.1293\n",
            "  Batch 1640/3208 | Loss: 1.9037\n",
            "  Batch 1650/3208 | Loss: 1.1338\n",
            "  Batch 1660/3208 | Loss: 1.0975\n",
            "  Batch 1670/3208 | Loss: 1.4606\n",
            "  Batch 1680/3208 | Loss: 1.4070\n",
            "  Batch 1690/3208 | Loss: 1.8255\n",
            "  Batch 1700/3208 | Loss: 1.7127\n",
            "  Batch 1710/3208 | Loss: 1.4550\n",
            "  Batch 1720/3208 | Loss: 1.2916\n",
            "  Batch 1730/3208 | Loss: 1.5639\n",
            "  Batch 1740/3208 | Loss: 0.9497\n",
            "  Batch 1750/3208 | Loss: 1.5373\n",
            "  Batch 1760/3208 | Loss: 1.2318\n",
            "  Batch 1770/3208 | Loss: 1.4358\n",
            "  Batch 1780/3208 | Loss: 2.0342\n",
            "  Batch 1790/3208 | Loss: 1.1753\n",
            "  Batch 1800/3208 | Loss: 1.6835\n",
            "  Batch 1810/3208 | Loss: 1.2282\n",
            "  Batch 1820/3208 | Loss: 1.0832\n",
            "  Batch 1830/3208 | Loss: 1.3490\n",
            "  Batch 1840/3208 | Loss: 2.4216\n",
            "  Batch 1850/3208 | Loss: 1.3398\n",
            "  Batch 1860/3208 | Loss: 1.3207\n",
            "  Batch 1870/3208 | Loss: 1.9278\n",
            "  Batch 1880/3208 | Loss: 1.8838\n",
            "  Batch 1890/3208 | Loss: 1.6514\n",
            "  Batch 1900/3208 | Loss: 1.8673\n",
            "  Batch 1910/3208 | Loss: 1.6777\n",
            "  Batch 1920/3208 | Loss: 1.6508\n",
            "  Batch 1930/3208 | Loss: 1.4550\n",
            "  Batch 1940/3208 | Loss: 1.1914\n",
            "  Batch 1950/3208 | Loss: 1.3787\n",
            "  Batch 1960/3208 | Loss: 1.3704\n",
            "  Batch 1970/3208 | Loss: 1.6068\n",
            "  Batch 1980/3208 | Loss: 1.0250\n",
            "  Batch 1990/3208 | Loss: 1.4656\n",
            "  Batch 2000/3208 | Loss: 1.0386\n",
            "  Batch 2010/3208 | Loss: 1.2791\n",
            "  Batch 2020/3208 | Loss: 1.7534\n",
            "  Batch 2030/3208 | Loss: 1.3555\n",
            "  Batch 2040/3208 | Loss: 1.3880\n",
            "  Batch 2050/3208 | Loss: 1.7550\n",
            "  Batch 2060/3208 | Loss: 1.9108\n",
            "  Batch 2070/3208 | Loss: 1.2716\n",
            "  Batch 2080/3208 | Loss: 1.3097\n",
            "  Batch 2090/3208 | Loss: 1.6659\n",
            "  Batch 2100/3208 | Loss: 1.7330\n",
            "  Batch 2110/3208 | Loss: 1.2697\n",
            "  Batch 2120/3208 | Loss: 1.6208\n",
            "  Batch 2130/3208 | Loss: 1.4526\n",
            "  Batch 2140/3208 | Loss: 1.1404\n",
            "  Batch 2150/3208 | Loss: 1.9903\n",
            "  Batch 2160/3208 | Loss: 1.0707\n",
            "  Batch 2170/3208 | Loss: 1.2490\n",
            "  Batch 2180/3208 | Loss: 1.4804\n",
            "  Batch 2190/3208 | Loss: 1.4349\n",
            "  Batch 2200/3208 | Loss: 1.3570\n",
            "  Batch 2210/3208 | Loss: 1.3410\n",
            "  Batch 2220/3208 | Loss: 0.9447\n",
            "  Batch 2230/3208 | Loss: 0.9496\n",
            "  Batch 2240/3208 | Loss: 0.9808\n",
            "  Batch 2250/3208 | Loss: 1.4491\n",
            "  Batch 2260/3208 | Loss: 1.8759\n",
            "  Batch 2270/3208 | Loss: 1.4763\n",
            "  Batch 2280/3208 | Loss: 1.4371\n",
            "  Batch 2290/3208 | Loss: 1.0616\n",
            "  Batch 2300/3208 | Loss: 1.3836\n",
            "  Batch 2310/3208 | Loss: 0.9818\n",
            "  Batch 2320/3208 | Loss: 1.5838\n",
            "  Batch 2330/3208 | Loss: 1.4170\n",
            "  Batch 2340/3208 | Loss: 1.1876\n",
            "  Batch 2350/3208 | Loss: 1.4195\n",
            "  Batch 2360/3208 | Loss: 1.1564\n",
            "  Batch 2370/3208 | Loss: 1.0487\n",
            "  Batch 2380/3208 | Loss: 1.3189\n",
            "  Batch 2390/3208 | Loss: 1.2944\n",
            "  Batch 2400/3208 | Loss: 1.1091\n",
            "  Batch 2410/3208 | Loss: 1.3008\n",
            "  Batch 2420/3208 | Loss: 1.6496\n",
            "  Batch 2430/3208 | Loss: 1.2152\n",
            "  Batch 2440/3208 | Loss: 1.4083\n",
            "  Batch 2450/3208 | Loss: 2.2277\n",
            "  Batch 2460/3208 | Loss: 1.2910\n",
            "  Batch 2470/3208 | Loss: 1.4620\n",
            "  Batch 2480/3208 | Loss: 1.0582\n",
            "  Batch 2490/3208 | Loss: 1.3371\n",
            "  Batch 2500/3208 | Loss: 1.5538\n",
            "  Batch 2510/3208 | Loss: 1.3608\n",
            "  Batch 2520/3208 | Loss: 1.2034\n",
            "  Batch 2530/3208 | Loss: 0.9453\n",
            "  Batch 2540/3208 | Loss: 1.5917\n",
            "  Batch 2550/3208 | Loss: 1.9997\n",
            "  Batch 2560/3208 | Loss: 1.3236\n",
            "  Batch 2570/3208 | Loss: 1.2970\n",
            "  Batch 2580/3208 | Loss: 1.7782\n",
            "  Batch 2590/3208 | Loss: 1.6233\n",
            "  Batch 2600/3208 | Loss: 1.3596\n",
            "  Batch 2610/3208 | Loss: 0.9749\n",
            "  Batch 2620/3208 | Loss: 1.3182\n",
            "  Batch 2630/3208 | Loss: 1.0076\n",
            "  Batch 2640/3208 | Loss: 1.4855\n",
            "  Batch 2650/3208 | Loss: 1.5540\n",
            "  Batch 2660/3208 | Loss: 1.6306\n",
            "  Batch 2670/3208 | Loss: 1.0144\n",
            "  Batch 2680/3208 | Loss: 1.0241\n",
            "  Batch 2690/3208 | Loss: 1.9258\n",
            "  Batch 2700/3208 | Loss: 1.3708\n",
            "  Batch 2710/3208 | Loss: 1.1174\n",
            "  Batch 2720/3208 | Loss: 0.8926\n",
            "  Batch 2730/3208 | Loss: 1.1512\n",
            "  Batch 2740/3208 | Loss: 2.2716\n",
            "  Batch 2750/3208 | Loss: 0.9473\n",
            "  Batch 2760/3208 | Loss: 1.1870\n",
            "  Batch 2770/3208 | Loss: 1.2225\n",
            "  Batch 2780/3208 | Loss: 1.2445\n",
            "  Batch 2790/3208 | Loss: 1.3421\n",
            "  Batch 2800/3208 | Loss: 0.9850\n",
            "  Batch 2810/3208 | Loss: 1.2399\n",
            "  Batch 2820/3208 | Loss: 1.5523\n",
            "  Batch 2830/3208 | Loss: 1.7824\n",
            "  Batch 2840/3208 | Loss: 1.2435\n",
            "  Batch 2850/3208 | Loss: 1.4598\n",
            "  Batch 2860/3208 | Loss: 1.0923\n",
            "  Batch 2870/3208 | Loss: 1.6016\n",
            "  Batch 2880/3208 | Loss: 1.0781\n",
            "  Batch 2890/3208 | Loss: 1.1617\n",
            "  Batch 2900/3208 | Loss: 1.5250\n",
            "  Batch 2910/3208 | Loss: 1.5030\n",
            "  Batch 2920/3208 | Loss: 1.6113\n",
            "  Batch 2930/3208 | Loss: 1.3965\n",
            "  Batch 2940/3208 | Loss: 1.2849\n",
            "  Batch 2950/3208 | Loss: 1.2977\n",
            "  Batch 2960/3208 | Loss: 1.4700\n",
            "  Batch 2970/3208 | Loss: 1.4258\n",
            "  Batch 2980/3208 | Loss: 1.6146\n",
            "  Batch 2990/3208 | Loss: 1.6878\n",
            "  Batch 3000/3208 | Loss: 1.6078\n",
            "  Batch 3010/3208 | Loss: 1.3779\n",
            "  Batch 3020/3208 | Loss: 1.0041\n",
            "  Batch 3030/3208 | Loss: 1.2179\n",
            "  Batch 3040/3208 | Loss: 1.1817\n",
            "  Batch 3050/3208 | Loss: 1.3529\n",
            "  Batch 3060/3208 | Loss: 1.4054\n",
            "  Batch 3070/3208 | Loss: 1.2031\n",
            "  Batch 3080/3208 | Loss: 1.3057\n",
            "  Batch 3090/3208 | Loss: 1.8568\n",
            "  Batch 3100/3208 | Loss: 1.9924\n",
            "  Batch 3110/3208 | Loss: 1.1456\n",
            "  Batch 3120/3208 | Loss: 0.8564\n",
            "  Batch 3130/3208 | Loss: 1.2105\n",
            "  Batch 3140/3208 | Loss: 1.3758\n",
            "  Batch 3150/3208 | Loss: 1.3824\n",
            "  Batch 3160/3208 | Loss: 1.6267\n",
            "  Batch 3170/3208 | Loss: 1.4453\n",
            "  Batch 3180/3208 | Loss: 1.4826\n",
            "  Batch 3190/3208 | Loss: 1.2612\n",
            "  Batch 3200/3208 | Loss: 1.1004\n",
            " Epoch 10 Completed | Avg Loss: 1.4470 | Training Accuracy: 58.15%\n",
            " Validation Accuracy: 63.40%\n",
            " Best Model Saved (Epoch 10 | Accuracy: 63.40%)\n",
            "\n",
            " Epoch 11/20\n",
            "  Batch 10/3208 | Loss: 1.5717\n",
            "  Batch 20/3208 | Loss: 1.1040\n",
            "  Batch 30/3208 | Loss: 1.6105\n",
            "  Batch 40/3208 | Loss: 2.0969\n",
            "  Batch 50/3208 | Loss: 1.7631\n",
            "  Batch 60/3208 | Loss: 1.9380\n",
            "  Batch 70/3208 | Loss: 1.4499\n",
            "  Batch 80/3208 | Loss: 1.7262\n",
            "  Batch 90/3208 | Loss: 1.4906\n",
            "  Batch 100/3208 | Loss: 1.2546\n",
            "  Batch 110/3208 | Loss: 1.8659\n",
            "  Batch 120/3208 | Loss: 1.2989\n",
            "  Batch 130/3208 | Loss: 1.2397\n",
            "  Batch 140/3208 | Loss: 1.5313\n",
            "  Batch 150/3208 | Loss: 1.5216\n",
            "  Batch 160/3208 | Loss: 1.1932\n",
            "  Batch 170/3208 | Loss: 1.6293\n",
            "  Batch 180/3208 | Loss: 1.6110\n",
            "  Batch 190/3208 | Loss: 1.4002\n",
            "  Batch 200/3208 | Loss: 1.2892\n",
            "  Batch 210/3208 | Loss: 1.2684\n",
            "  Batch 220/3208 | Loss: 1.0482\n",
            "  Batch 230/3208 | Loss: 1.5735\n",
            "  Batch 240/3208 | Loss: 1.1871\n",
            "  Batch 250/3208 | Loss: 1.3545\n",
            "  Batch 260/3208 | Loss: 1.3313\n",
            "  Batch 270/3208 | Loss: 1.3665\n",
            "  Batch 280/3208 | Loss: 1.8980\n",
            "  Batch 290/3208 | Loss: 1.0016\n",
            "  Batch 300/3208 | Loss: 1.1900\n",
            "  Batch 310/3208 | Loss: 1.4705\n",
            "  Batch 320/3208 | Loss: 1.2827\n",
            "  Batch 330/3208 | Loss: 1.4486\n",
            "  Batch 340/3208 | Loss: 1.3209\n",
            "  Batch 350/3208 | Loss: 1.8104\n",
            "  Batch 360/3208 | Loss: 1.5030\n",
            "  Batch 370/3208 | Loss: 1.8667\n",
            "  Batch 380/3208 | Loss: 1.3540\n",
            "  Batch 390/3208 | Loss: 1.5687\n",
            "  Batch 400/3208 | Loss: 1.4907\n",
            "  Batch 410/3208 | Loss: 1.1636\n",
            "  Batch 420/3208 | Loss: 1.5250\n",
            "  Batch 430/3208 | Loss: 1.2472\n",
            "  Batch 440/3208 | Loss: 1.4863\n",
            "  Batch 450/3208 | Loss: 1.5445\n",
            "  Batch 460/3208 | Loss: 1.4927\n",
            "  Batch 470/3208 | Loss: 1.0686\n",
            "  Batch 480/3208 | Loss: 0.8352\n",
            "  Batch 490/3208 | Loss: 1.2597\n",
            "  Batch 500/3208 | Loss: 1.3566\n",
            "  Batch 510/3208 | Loss: 1.3675\n",
            "  Batch 520/3208 | Loss: 1.3836\n",
            "  Batch 530/3208 | Loss: 0.9789\n",
            "  Batch 540/3208 | Loss: 1.9619\n",
            "  Batch 550/3208 | Loss: 1.6138\n",
            "  Batch 560/3208 | Loss: 1.7358\n",
            "  Batch 570/3208 | Loss: 1.7527\n",
            "  Batch 580/3208 | Loss: 1.5303\n",
            "  Batch 590/3208 | Loss: 0.9759\n",
            "  Batch 600/3208 | Loss: 0.6287\n",
            "  Batch 610/3208 | Loss: 1.1351\n",
            "  Batch 620/3208 | Loss: 0.9547\n",
            "  Batch 630/3208 | Loss: 1.0001\n",
            "  Batch 640/3208 | Loss: 1.4573\n",
            "  Batch 650/3208 | Loss: 1.5614\n",
            "  Batch 660/3208 | Loss: 1.3031\n",
            "  Batch 670/3208 | Loss: 1.0864\n",
            "  Batch 680/3208 | Loss: 1.0703\n",
            "  Batch 690/3208 | Loss: 1.2617\n",
            "  Batch 700/3208 | Loss: 1.1857\n",
            "  Batch 710/3208 | Loss: 1.5947\n",
            "  Batch 720/3208 | Loss: 1.4027\n",
            "  Batch 730/3208 | Loss: 1.1784\n",
            "  Batch 740/3208 | Loss: 1.0810\n",
            "  Batch 750/3208 | Loss: 1.5078\n",
            "  Batch 760/3208 | Loss: 1.6138\n",
            "  Batch 770/3208 | Loss: 1.9465\n",
            "  Batch 780/3208 | Loss: 1.5152\n",
            "  Batch 790/3208 | Loss: 1.0688\n",
            "  Batch 800/3208 | Loss: 1.2141\n",
            "  Batch 810/3208 | Loss: 1.7741\n",
            "  Batch 820/3208 | Loss: 0.9284\n",
            "  Batch 830/3208 | Loss: 1.2124\n",
            "  Batch 840/3208 | Loss: 1.4685\n",
            "  Batch 850/3208 | Loss: 1.5790\n",
            "  Batch 860/3208 | Loss: 1.0103\n",
            "  Batch 870/3208 | Loss: 1.1230\n",
            "  Batch 880/3208 | Loss: 1.0662\n",
            "  Batch 890/3208 | Loss: 1.7895\n",
            "  Batch 900/3208 | Loss: 1.6178\n",
            "  Batch 910/3208 | Loss: 1.4582\n",
            "  Batch 920/3208 | Loss: 1.3200\n",
            "  Batch 930/3208 | Loss: 1.9050\n",
            "  Batch 940/3208 | Loss: 1.1845\n",
            "  Batch 950/3208 | Loss: 1.7154\n",
            "  Batch 960/3208 | Loss: 1.0826\n",
            "  Batch 970/3208 | Loss: 1.7875\n",
            "  Batch 980/3208 | Loss: 1.3085\n",
            "  Batch 990/3208 | Loss: 1.3163\n",
            "  Batch 1000/3208 | Loss: 1.1064\n",
            "  Batch 1010/3208 | Loss: 0.9995\n",
            "  Batch 1020/3208 | Loss: 1.1838\n",
            "  Batch 1030/3208 | Loss: 1.8165\n",
            "  Batch 1040/3208 | Loss: 1.3385\n",
            "  Batch 1050/3208 | Loss: 1.2636\n",
            "  Batch 1060/3208 | Loss: 1.1655\n",
            "  Batch 1070/3208 | Loss: 1.6188\n",
            "  Batch 1080/3208 | Loss: 1.6544\n",
            "  Batch 1090/3208 | Loss: 1.3751\n",
            "  Batch 1100/3208 | Loss: 1.2348\n",
            "  Batch 1110/3208 | Loss: 1.8597\n",
            "  Batch 1120/3208 | Loss: 1.3360\n",
            "  Batch 1130/3208 | Loss: 1.3212\n",
            "  Batch 1140/3208 | Loss: 0.8945\n",
            "  Batch 1150/3208 | Loss: 1.4877\n",
            "  Batch 1160/3208 | Loss: 1.1924\n",
            "  Batch 1170/3208 | Loss: 1.4243\n",
            "  Batch 1180/3208 | Loss: 0.9809\n",
            "  Batch 1190/3208 | Loss: 1.6847\n",
            "  Batch 1200/3208 | Loss: 1.4589\n",
            "  Batch 1210/3208 | Loss: 1.6459\n",
            "  Batch 1220/3208 | Loss: 1.6600\n",
            "  Batch 1230/3208 | Loss: 1.4734\n",
            "  Batch 1240/3208 | Loss: 1.9076\n",
            "  Batch 1250/3208 | Loss: 1.2920\n",
            "  Batch 1260/3208 | Loss: 1.2184\n",
            "  Batch 1270/3208 | Loss: 1.5474\n",
            "  Batch 1280/3208 | Loss: 1.8951\n",
            "  Batch 1290/3208 | Loss: 1.6202\n",
            "  Batch 1300/3208 | Loss: 1.6737\n",
            "  Batch 1310/3208 | Loss: 1.4764\n",
            "  Batch 1320/3208 | Loss: 1.3259\n",
            "  Batch 1330/3208 | Loss: 1.2993\n",
            "  Batch 1340/3208 | Loss: 1.4927\n",
            "  Batch 1350/3208 | Loss: 1.6316\n",
            "  Batch 1360/3208 | Loss: 0.9295\n",
            "  Batch 1370/3208 | Loss: 1.2406\n",
            "  Batch 1380/3208 | Loss: 1.6826\n",
            "  Batch 1390/3208 | Loss: 1.4790\n",
            "  Batch 1400/3208 | Loss: 1.1480\n",
            "  Batch 1410/3208 | Loss: 1.3409\n",
            "  Batch 1420/3208 | Loss: 1.3788\n",
            "  Batch 1430/3208 | Loss: 1.4959\n",
            "  Batch 1440/3208 | Loss: 0.8217\n",
            "  Batch 1450/3208 | Loss: 1.2216\n",
            "  Batch 1460/3208 | Loss: 1.0456\n",
            "  Batch 1470/3208 | Loss: 1.4892\n",
            "  Batch 1480/3208 | Loss: 1.8388\n",
            "  Batch 1490/3208 | Loss: 1.2168\n",
            "  Batch 1500/3208 | Loss: 1.0122\n",
            "  Batch 1510/3208 | Loss: 1.2944\n",
            "  Batch 1520/3208 | Loss: 1.6055\n",
            "  Batch 1530/3208 | Loss: 1.3288\n",
            "  Batch 1540/3208 | Loss: 1.3913\n",
            "  Batch 1550/3208 | Loss: 1.1704\n",
            "  Batch 1560/3208 | Loss: 1.3754\n",
            "  Batch 1570/3208 | Loss: 1.1559\n",
            "  Batch 1580/3208 | Loss: 1.7297\n",
            "  Batch 1590/3208 | Loss: 1.7492\n",
            "  Batch 1600/3208 | Loss: 1.3569\n",
            "  Batch 1610/3208 | Loss: 1.7704\n",
            "  Batch 1620/3208 | Loss: 1.0019\n",
            "  Batch 1630/3208 | Loss: 1.4139\n",
            "  Batch 1640/3208 | Loss: 1.9491\n",
            "  Batch 1650/3208 | Loss: 2.0401\n",
            "  Batch 1660/3208 | Loss: 1.2249\n",
            "  Batch 1670/3208 | Loss: 1.3166\n",
            "  Batch 1680/3208 | Loss: 1.6390\n",
            "  Batch 1690/3208 | Loss: 1.7432\n",
            "  Batch 1700/3208 | Loss: 1.1262\n",
            "  Batch 1710/3208 | Loss: 1.6015\n",
            "  Batch 1720/3208 | Loss: 0.6631\n",
            "  Batch 1730/3208 | Loss: 0.9998\n",
            "  Batch 1740/3208 | Loss: 1.5506\n",
            "  Batch 1750/3208 | Loss: 1.5674\n",
            "  Batch 1760/3208 | Loss: 0.6657\n",
            "  Batch 1770/3208 | Loss: 2.4430\n",
            "  Batch 1780/3208 | Loss: 1.2487\n",
            "  Batch 1790/3208 | Loss: 1.4409\n",
            "  Batch 1800/3208 | Loss: 1.0245\n",
            "  Batch 1810/3208 | Loss: 1.3027\n",
            "  Batch 1820/3208 | Loss: 1.4154\n",
            "  Batch 1830/3208 | Loss: 1.0835\n",
            "  Batch 1840/3208 | Loss: 1.4755\n",
            "  Batch 1850/3208 | Loss: 1.3240\n",
            "  Batch 1860/3208 | Loss: 1.2925\n",
            "  Batch 1870/3208 | Loss: 0.9793\n",
            "  Batch 1880/3208 | Loss: 1.6082\n",
            "  Batch 1890/3208 | Loss: 2.0065\n",
            "  Batch 1900/3208 | Loss: 1.8051\n",
            "  Batch 1910/3208 | Loss: 1.6211\n",
            "  Batch 1920/3208 | Loss: 1.2984\n",
            "  Batch 1930/3208 | Loss: 1.1031\n",
            "  Batch 1940/3208 | Loss: 1.7080\n",
            "  Batch 1950/3208 | Loss: 1.1312\n",
            "  Batch 1960/3208 | Loss: 1.1037\n",
            "  Batch 1970/3208 | Loss: 1.2034\n",
            "  Batch 1980/3208 | Loss: 1.2492\n",
            "  Batch 1990/3208 | Loss: 1.2813\n",
            "  Batch 2000/3208 | Loss: 1.2476\n",
            "  Batch 2010/3208 | Loss: 1.5643\n",
            "  Batch 2020/3208 | Loss: 1.3097\n",
            "  Batch 2030/3208 | Loss: 1.0512\n",
            "  Batch 2040/3208 | Loss: 1.3969\n",
            "  Batch 2050/3208 | Loss: 1.4348\n",
            "  Batch 2060/3208 | Loss: 1.4223\n",
            "  Batch 2070/3208 | Loss: 1.3452\n",
            "  Batch 2080/3208 | Loss: 0.9615\n",
            "  Batch 2090/3208 | Loss: 1.3642\n",
            "  Batch 2100/3208 | Loss: 1.8254\n",
            "  Batch 2110/3208 | Loss: 1.1951\n",
            "  Batch 2120/3208 | Loss: 1.3658\n",
            "  Batch 2130/3208 | Loss: 1.0969\n",
            "  Batch 2140/3208 | Loss: 1.8556\n",
            "  Batch 2150/3208 | Loss: 2.2287\n",
            "  Batch 2160/3208 | Loss: 1.5646\n",
            "  Batch 2170/3208 | Loss: 1.4557\n",
            "  Batch 2180/3208 | Loss: 1.6895\n",
            "  Batch 2190/3208 | Loss: 0.8241\n",
            "  Batch 2200/3208 | Loss: 1.2659\n",
            "  Batch 2210/3208 | Loss: 1.1430\n",
            "  Batch 2220/3208 | Loss: 1.6639\n",
            "  Batch 2230/3208 | Loss: 1.1347\n",
            "  Batch 2240/3208 | Loss: 1.3265\n",
            "  Batch 2250/3208 | Loss: 1.2903\n",
            "  Batch 2260/3208 | Loss: 1.5896\n",
            "  Batch 2270/3208 | Loss: 1.8087\n",
            "  Batch 2280/3208 | Loss: 1.3752\n",
            "  Batch 2290/3208 | Loss: 1.4933\n",
            "  Batch 2300/3208 | Loss: 1.3178\n",
            "  Batch 2310/3208 | Loss: 1.7070\n",
            "  Batch 2320/3208 | Loss: 1.4766\n",
            "  Batch 2330/3208 | Loss: 1.6735\n",
            "  Batch 2340/3208 | Loss: 1.0929\n",
            "  Batch 2350/3208 | Loss: 1.5963\n",
            "  Batch 2360/3208 | Loss: 1.6256\n",
            "  Batch 2370/3208 | Loss: 1.2179\n",
            "  Batch 2380/3208 | Loss: 1.2232\n",
            "  Batch 2390/3208 | Loss: 1.4997\n",
            "  Batch 2400/3208 | Loss: 1.0168\n",
            "  Batch 2410/3208 | Loss: 1.1666\n",
            "  Batch 2420/3208 | Loss: 1.3331\n",
            "  Batch 2430/3208 | Loss: 1.8703\n",
            "  Batch 2440/3208 | Loss: 2.4227\n",
            "  Batch 2450/3208 | Loss: 1.2362\n",
            "  Batch 2460/3208 | Loss: 1.2278\n",
            "  Batch 2470/3208 | Loss: 0.8993\n",
            "  Batch 2480/3208 | Loss: 1.6610\n",
            "  Batch 2490/3208 | Loss: 1.3574\n",
            "  Batch 2500/3208 | Loss: 1.5792\n",
            "  Batch 2510/3208 | Loss: 1.2428\n",
            "  Batch 2520/3208 | Loss: 1.8576\n",
            "  Batch 2530/3208 | Loss: 1.0781\n",
            "  Batch 2540/3208 | Loss: 1.5510\n",
            "  Batch 2550/3208 | Loss: 1.3997\n",
            "  Batch 2560/3208 | Loss: 1.2948\n",
            "  Batch 2570/3208 | Loss: 1.4333\n",
            "  Batch 2580/3208 | Loss: 0.8913\n",
            "  Batch 2590/3208 | Loss: 1.5498\n",
            "  Batch 2600/3208 | Loss: 1.2574\n",
            "  Batch 2610/3208 | Loss: 1.4107\n",
            "  Batch 2620/3208 | Loss: 1.0124\n",
            "  Batch 2630/3208 | Loss: 0.9264\n",
            "  Batch 2640/3208 | Loss: 1.4424\n",
            "  Batch 2650/3208 | Loss: 1.1421\n",
            "  Batch 2660/3208 | Loss: 1.9929\n",
            "  Batch 2670/3208 | Loss: 1.2994\n",
            "  Batch 2680/3208 | Loss: 0.9796\n",
            "  Batch 2690/3208 | Loss: 1.3592\n",
            "  Batch 2700/3208 | Loss: 1.5929\n",
            "  Batch 2710/3208 | Loss: 1.3368\n",
            "  Batch 2720/3208 | Loss: 1.0835\n",
            "  Batch 2730/3208 | Loss: 1.3161\n",
            "  Batch 2740/3208 | Loss: 1.3712\n",
            "  Batch 2750/3208 | Loss: 0.9695\n",
            "  Batch 2760/3208 | Loss: 1.8674\n",
            "  Batch 2770/3208 | Loss: 1.5392\n",
            "  Batch 2780/3208 | Loss: 1.3840\n",
            "  Batch 2790/3208 | Loss: 1.5100\n",
            "  Batch 2800/3208 | Loss: 1.5800\n",
            "  Batch 2810/3208 | Loss: 1.1007\n",
            "  Batch 2820/3208 | Loss: 0.8991\n",
            "  Batch 2830/3208 | Loss: 1.3308\n",
            "  Batch 2840/3208 | Loss: 0.7987\n",
            "  Batch 2850/3208 | Loss: 0.9496\n",
            "  Batch 2860/3208 | Loss: 1.0515\n",
            "  Batch 2870/3208 | Loss: 0.9467\n",
            "  Batch 2880/3208 | Loss: 1.2710\n",
            "  Batch 2890/3208 | Loss: 1.2324\n",
            "  Batch 2900/3208 | Loss: 0.8942\n",
            "  Batch 2910/3208 | Loss: 1.0878\n",
            "  Batch 2920/3208 | Loss: 1.1339\n",
            "  Batch 2930/3208 | Loss: 1.5468\n",
            "  Batch 2940/3208 | Loss: 1.5383\n",
            "  Batch 2950/3208 | Loss: 1.3962\n",
            "  Batch 2960/3208 | Loss: 1.1622\n",
            "  Batch 2970/3208 | Loss: 1.3244\n",
            "  Batch 2980/3208 | Loss: 1.4859\n",
            "  Batch 2990/3208 | Loss: 1.3529\n",
            "  Batch 3000/3208 | Loss: 1.2154\n",
            "  Batch 3010/3208 | Loss: 1.2707\n",
            "  Batch 3020/3208 | Loss: 1.5496\n",
            "  Batch 3030/3208 | Loss: 1.3210\n",
            "  Batch 3040/3208 | Loss: 1.3553\n",
            "  Batch 3050/3208 | Loss: 1.5506\n",
            "  Batch 3060/3208 | Loss: 1.7082\n",
            "  Batch 3070/3208 | Loss: 1.7325\n",
            "  Batch 3080/3208 | Loss: 1.2901\n",
            "  Batch 3090/3208 | Loss: 2.1453\n",
            "  Batch 3100/3208 | Loss: 1.3407\n",
            "  Batch 3110/3208 | Loss: 1.0505\n",
            "  Batch 3120/3208 | Loss: 1.1006\n",
            "  Batch 3130/3208 | Loss: 1.3395\n",
            "  Batch 3140/3208 | Loss: 0.9336\n",
            "  Batch 3150/3208 | Loss: 1.3368\n",
            "  Batch 3160/3208 | Loss: 1.1616\n",
            "  Batch 3170/3208 | Loss: 1.0933\n",
            "  Batch 3180/3208 | Loss: 1.2028\n",
            "  Batch 3190/3208 | Loss: 1.3630\n",
            "  Batch 3200/3208 | Loss: 1.1648\n",
            " Epoch 11 Completed | Avg Loss: 1.3696 | Training Accuracy: 60.48%\n",
            " Validation Accuracy: 65.20%\n",
            " Best Model Saved (Epoch 11 | Accuracy: 65.20%)\n",
            "\n",
            " Epoch 12/20\n",
            "  Batch 10/3208 | Loss: 1.5549\n",
            "  Batch 20/3208 | Loss: 1.3920\n",
            "  Batch 30/3208 | Loss: 1.3418\n",
            "  Batch 40/3208 | Loss: 1.3702\n",
            "  Batch 50/3208 | Loss: 1.4227\n",
            "  Batch 60/3208 | Loss: 1.3772\n",
            "  Batch 70/3208 | Loss: 2.0107\n",
            "  Batch 80/3208 | Loss: 1.6546\n",
            "  Batch 90/3208 | Loss: 1.4794\n",
            "  Batch 100/3208 | Loss: 1.4391\n",
            "  Batch 110/3208 | Loss: 1.5265\n",
            "  Batch 120/3208 | Loss: 1.6874\n",
            "  Batch 130/3208 | Loss: 1.6113\n",
            "  Batch 140/3208 | Loss: 1.9050\n",
            "  Batch 150/3208 | Loss: 1.4099\n",
            "  Batch 160/3208 | Loss: 1.4589\n",
            "  Batch 170/3208 | Loss: 1.2242\n",
            "  Batch 180/3208 | Loss: 0.7774\n",
            "  Batch 190/3208 | Loss: 1.3529\n",
            "  Batch 200/3208 | Loss: 1.1757\n",
            "  Batch 210/3208 | Loss: 1.1181\n",
            "  Batch 220/3208 | Loss: 1.1258\n",
            "  Batch 230/3208 | Loss: 1.0456\n",
            "  Batch 240/3208 | Loss: 1.6414\n",
            "  Batch 250/3208 | Loss: 1.2840\n",
            "  Batch 260/3208 | Loss: 1.5471\n",
            "  Batch 270/3208 | Loss: 1.1132\n",
            "  Batch 280/3208 | Loss: 1.4313\n",
            "  Batch 290/3208 | Loss: 1.1631\n",
            "  Batch 300/3208 | Loss: 0.9522\n",
            "  Batch 310/3208 | Loss: 2.1875\n",
            "  Batch 320/3208 | Loss: 1.0214\n",
            "  Batch 330/3208 | Loss: 1.3433\n",
            "  Batch 340/3208 | Loss: 1.2045\n",
            "  Batch 350/3208 | Loss: 1.2021\n",
            "  Batch 360/3208 | Loss: 1.9443\n",
            "  Batch 370/3208 | Loss: 0.9085\n",
            "  Batch 380/3208 | Loss: 1.3734\n",
            "  Batch 390/3208 | Loss: 1.3922\n",
            "  Batch 400/3208 | Loss: 1.2153\n",
            "  Batch 410/3208 | Loss: 1.4633\n",
            "  Batch 420/3208 | Loss: 1.5463\n",
            "  Batch 430/3208 | Loss: 1.6525\n",
            "  Batch 440/3208 | Loss: 1.2705\n",
            "  Batch 450/3208 | Loss: 1.1500\n",
            "  Batch 460/3208 | Loss: 1.5915\n",
            "  Batch 470/3208 | Loss: 1.3568\n",
            "  Batch 480/3208 | Loss: 0.9993\n",
            "  Batch 490/3208 | Loss: 1.2243\n",
            "  Batch 500/3208 | Loss: 0.9121\n",
            "  Batch 510/3208 | Loss: 1.5664\n",
            "  Batch 520/3208 | Loss: 2.5392\n",
            "  Batch 530/3208 | Loss: 1.2206\n",
            "  Batch 540/3208 | Loss: 1.8538\n",
            "  Batch 550/3208 | Loss: 1.0580\n",
            "  Batch 560/3208 | Loss: 1.0639\n",
            "  Batch 570/3208 | Loss: 1.0298\n",
            "  Batch 580/3208 | Loss: 1.2156\n",
            "  Batch 590/3208 | Loss: 2.0692\n",
            "  Batch 600/3208 | Loss: 0.8110\n",
            "  Batch 610/3208 | Loss: 0.7899\n",
            "  Batch 620/3208 | Loss: 1.1129\n",
            "  Batch 630/3208 | Loss: 0.7338\n",
            "  Batch 640/3208 | Loss: 0.7545\n",
            "  Batch 650/3208 | Loss: 1.0256\n",
            "  Batch 660/3208 | Loss: 1.2027\n",
            "  Batch 670/3208 | Loss: 1.1526\n",
            "  Batch 680/3208 | Loss: 1.3288\n",
            "  Batch 690/3208 | Loss: 1.5204\n",
            "  Batch 700/3208 | Loss: 1.4790\n",
            "  Batch 710/3208 | Loss: 2.1278\n",
            "  Batch 720/3208 | Loss: 1.2822\n",
            "  Batch 730/3208 | Loss: 1.3522\n",
            "  Batch 740/3208 | Loss: 1.3950\n",
            "  Batch 750/3208 | Loss: 0.8421\n",
            "  Batch 760/3208 | Loss: 1.1498\n",
            "  Batch 770/3208 | Loss: 1.0528\n",
            "  Batch 780/3208 | Loss: 1.5458\n",
            "  Batch 790/3208 | Loss: 1.0875\n",
            "  Batch 800/3208 | Loss: 1.6571\n",
            "  Batch 810/3208 | Loss: 2.0063\n",
            "  Batch 820/3208 | Loss: 1.3452\n",
            "  Batch 830/3208 | Loss: 2.1230\n",
            "  Batch 840/3208 | Loss: 1.1359\n",
            "  Batch 850/3208 | Loss: 1.4212\n",
            "  Batch 860/3208 | Loss: 1.3495\n",
            "  Batch 870/3208 | Loss: 1.1629\n",
            "  Batch 880/3208 | Loss: 1.9096\n",
            "  Batch 890/3208 | Loss: 1.1609\n",
            "  Batch 900/3208 | Loss: 1.1266\n",
            "  Batch 910/3208 | Loss: 0.8984\n",
            "  Batch 920/3208 | Loss: 1.1902\n",
            "  Batch 930/3208 | Loss: 1.0933\n",
            "  Batch 940/3208 | Loss: 1.6097\n",
            "  Batch 950/3208 | Loss: 1.3590\n",
            "  Batch 960/3208 | Loss: 0.9094\n",
            "  Batch 970/3208 | Loss: 1.2310\n",
            "  Batch 980/3208 | Loss: 1.1302\n",
            "  Batch 990/3208 | Loss: 0.9437\n",
            "  Batch 1000/3208 | Loss: 1.0333\n",
            "  Batch 1010/3208 | Loss: 1.4701\n",
            "  Batch 1020/3208 | Loss: 1.2396\n",
            "  Batch 1030/3208 | Loss: 1.5598\n",
            "  Batch 1040/3208 | Loss: 1.0214\n",
            "  Batch 1050/3208 | Loss: 1.7218\n",
            "  Batch 1060/3208 | Loss: 1.4335\n",
            "  Batch 1070/3208 | Loss: 2.0072\n",
            "  Batch 1080/3208 | Loss: 2.1148\n",
            "  Batch 1090/3208 | Loss: 1.5782\n",
            "  Batch 1100/3208 | Loss: 1.0537\n",
            "  Batch 1110/3208 | Loss: 1.4644\n",
            "  Batch 1120/3208 | Loss: 1.0002\n",
            "  Batch 1130/3208 | Loss: 1.4435\n",
            "  Batch 1140/3208 | Loss: 1.5395\n",
            "  Batch 1150/3208 | Loss: 1.3048\n",
            "  Batch 1160/3208 | Loss: 1.0290\n",
            "  Batch 1170/3208 | Loss: 1.1038\n",
            "  Batch 1180/3208 | Loss: 1.8949\n",
            "  Batch 1190/3208 | Loss: 0.9789\n",
            "  Batch 1200/3208 | Loss: 0.8375\n",
            "  Batch 1210/3208 | Loss: 1.2058\n",
            "  Batch 1220/3208 | Loss: 1.5458\n",
            "  Batch 1230/3208 | Loss: 1.5753\n",
            "  Batch 1240/3208 | Loss: 1.3879\n",
            "  Batch 1250/3208 | Loss: 1.2069\n",
            "  Batch 1260/3208 | Loss: 0.8569\n",
            "  Batch 1270/3208 | Loss: 1.3766\n",
            "  Batch 1280/3208 | Loss: 1.0247\n",
            "  Batch 1290/3208 | Loss: 0.6718\n",
            "  Batch 1300/3208 | Loss: 0.8631\n",
            "  Batch 1310/3208 | Loss: 1.4276\n",
            "  Batch 1320/3208 | Loss: 1.6324\n",
            "  Batch 1330/3208 | Loss: 1.3117\n",
            "  Batch 1340/3208 | Loss: 1.3907\n",
            "  Batch 1350/3208 | Loss: 0.6703\n",
            "  Batch 1360/3208 | Loss: 1.5729\n",
            "  Batch 1370/3208 | Loss: 1.2863\n",
            "  Batch 1380/3208 | Loss: 1.1039\n",
            "  Batch 1390/3208 | Loss: 1.0814\n",
            "  Batch 1400/3208 | Loss: 1.0422\n",
            "  Batch 1410/3208 | Loss: 1.2338\n",
            "  Batch 1420/3208 | Loss: 1.6833\n",
            "  Batch 1430/3208 | Loss: 1.5221\n",
            "  Batch 1440/3208 | Loss: 1.0018\n",
            "  Batch 1450/3208 | Loss: 1.1430\n",
            "  Batch 1460/3208 | Loss: 1.7974\n",
            "  Batch 1470/3208 | Loss: 1.4507\n",
            "  Batch 1480/3208 | Loss: 0.7304\n",
            "  Batch 1490/3208 | Loss: 2.1324\n",
            "  Batch 1500/3208 | Loss: 1.6460\n",
            "  Batch 1510/3208 | Loss: 0.7840\n",
            "  Batch 1520/3208 | Loss: 1.3480\n",
            "  Batch 1530/3208 | Loss: 1.3350\n",
            "  Batch 1540/3208 | Loss: 0.5948\n",
            "  Batch 1550/3208 | Loss: 1.5899\n",
            "  Batch 1560/3208 | Loss: 0.8625\n",
            "  Batch 1570/3208 | Loss: 0.6643\n",
            "  Batch 1580/3208 | Loss: 1.3317\n",
            "  Batch 1590/3208 | Loss: 1.6927\n",
            "  Batch 1600/3208 | Loss: 0.9561\n",
            "  Batch 1610/3208 | Loss: 1.1166\n",
            "  Batch 1620/3208 | Loss: 1.6230\n",
            "  Batch 1630/3208 | Loss: 1.0380\n",
            "  Batch 1640/3208 | Loss: 1.2794\n",
            "  Batch 1650/3208 | Loss: 0.9266\n",
            "  Batch 1660/3208 | Loss: 1.2735\n",
            "  Batch 1670/3208 | Loss: 1.8819\n",
            "  Batch 1680/3208 | Loss: 0.9509\n",
            "  Batch 1690/3208 | Loss: 0.9708\n",
            "  Batch 1700/3208 | Loss: 1.2915\n",
            "  Batch 1710/3208 | Loss: 1.6712\n",
            "  Batch 1720/3208 | Loss: 0.9881\n",
            "  Batch 1730/3208 | Loss: 1.2540\n",
            "  Batch 1740/3208 | Loss: 0.9665\n",
            "  Batch 1750/3208 | Loss: 1.4987\n",
            "  Batch 1760/3208 | Loss: 1.2439\n",
            "  Batch 1770/3208 | Loss: 1.3543\n",
            "  Batch 1780/3208 | Loss: 1.1332\n",
            "  Batch 1790/3208 | Loss: 0.9544\n",
            "  Batch 1800/3208 | Loss: 1.0609\n",
            "  Batch 1810/3208 | Loss: 1.6191\n",
            "  Batch 1820/3208 | Loss: 1.5905\n",
            "  Batch 1830/3208 | Loss: 1.8776\n",
            "  Batch 1840/3208 | Loss: 1.3780\n",
            "  Batch 1850/3208 | Loss: 0.9347\n",
            "  Batch 1860/3208 | Loss: 1.0627\n",
            "  Batch 1870/3208 | Loss: 1.8063\n",
            "  Batch 1880/3208 | Loss: 0.8168\n",
            "  Batch 1890/3208 | Loss: 1.0907\n",
            "  Batch 1900/3208 | Loss: 1.6082\n",
            "  Batch 1910/3208 | Loss: 1.2077\n",
            "  Batch 1920/3208 | Loss: 0.9308\n",
            "  Batch 1930/3208 | Loss: 1.2889\n",
            "  Batch 1940/3208 | Loss: 1.1251\n",
            "  Batch 1950/3208 | Loss: 1.3070\n",
            "  Batch 1960/3208 | Loss: 1.1918\n",
            "  Batch 1970/3208 | Loss: 1.3541\n",
            "  Batch 1980/3208 | Loss: 1.3473\n",
            "  Batch 1990/3208 | Loss: 1.0475\n",
            "  Batch 2000/3208 | Loss: 1.4542\n",
            "  Batch 2010/3208 | Loss: 0.9543\n",
            "  Batch 2020/3208 | Loss: 0.8765\n",
            "  Batch 2030/3208 | Loss: 1.0146\n",
            "  Batch 2040/3208 | Loss: 1.5003\n",
            "  Batch 2050/3208 | Loss: 0.8109\n",
            "  Batch 2060/3208 | Loss: 1.3130\n",
            "  Batch 2070/3208 | Loss: 1.4770\n",
            "  Batch 2080/3208 | Loss: 1.2168\n",
            "  Batch 2090/3208 | Loss: 0.8441\n",
            "  Batch 2100/3208 | Loss: 1.1986\n",
            "  Batch 2110/3208 | Loss: 1.1347\n",
            "  Batch 2120/3208 | Loss: 2.0062\n",
            "  Batch 2130/3208 | Loss: 1.3308\n",
            "  Batch 2140/3208 | Loss: 1.1516\n",
            "  Batch 2150/3208 | Loss: 1.6780\n",
            "  Batch 2160/3208 | Loss: 0.9697\n",
            "  Batch 2170/3208 | Loss: 1.2646\n",
            "  Batch 2180/3208 | Loss: 1.2753\n",
            "  Batch 2190/3208 | Loss: 1.2225\n",
            "  Batch 2200/3208 | Loss: 1.3990\n",
            "  Batch 2210/3208 | Loss: 1.1590\n",
            "  Batch 2220/3208 | Loss: 1.2266\n",
            "  Batch 2230/3208 | Loss: 1.3567\n",
            "  Batch 2240/3208 | Loss: 1.2665\n",
            "  Batch 2250/3208 | Loss: 1.3967\n",
            "  Batch 2260/3208 | Loss: 0.9763\n",
            "  Batch 2270/3208 | Loss: 1.3881\n",
            "  Batch 2280/3208 | Loss: 1.7429\n",
            "  Batch 2290/3208 | Loss: 0.8369\n",
            "  Batch 2300/3208 | Loss: 1.0799\n",
            "  Batch 2310/3208 | Loss: 1.4286\n",
            "  Batch 2320/3208 | Loss: 1.0840\n",
            "  Batch 2330/3208 | Loss: 1.0305\n",
            "  Batch 2340/3208 | Loss: 1.1834\n",
            "  Batch 2350/3208 | Loss: 0.9965\n",
            "  Batch 2360/3208 | Loss: 1.6312\n",
            "  Batch 2370/3208 | Loss: 1.0161\n",
            "  Batch 2380/3208 | Loss: 1.3086\n",
            "  Batch 2390/3208 | Loss: 1.4202\n",
            "  Batch 2400/3208 | Loss: 1.8857\n",
            "  Batch 2410/3208 | Loss: 1.1779\n",
            "  Batch 2420/3208 | Loss: 1.7827\n",
            "  Batch 2430/3208 | Loss: 1.8425\n",
            "  Batch 2440/3208 | Loss: 1.0426\n",
            "  Batch 2450/3208 | Loss: 0.9485\n",
            "  Batch 2460/3208 | Loss: 0.8344\n",
            "  Batch 2470/3208 | Loss: 1.3469\n",
            "  Batch 2480/3208 | Loss: 1.5433\n",
            "  Batch 2490/3208 | Loss: 1.7668\n",
            "  Batch 2500/3208 | Loss: 1.5912\n",
            "  Batch 2510/3208 | Loss: 1.7404\n",
            "  Batch 2520/3208 | Loss: 1.1775\n",
            "  Batch 2530/3208 | Loss: 1.2016\n",
            "  Batch 2540/3208 | Loss: 1.8583\n",
            "  Batch 2550/3208 | Loss: 1.4699\n",
            "  Batch 2560/3208 | Loss: 1.6812\n",
            "  Batch 2570/3208 | Loss: 1.6586\n",
            "  Batch 2580/3208 | Loss: 1.5313\n",
            "  Batch 2590/3208 | Loss: 1.3913\n",
            "  Batch 2600/3208 | Loss: 1.1595\n",
            "  Batch 2610/3208 | Loss: 1.1746\n",
            "  Batch 2620/3208 | Loss: 1.0265\n",
            "  Batch 2630/3208 | Loss: 2.1011\n",
            "  Batch 2640/3208 | Loss: 1.0542\n",
            "  Batch 2650/3208 | Loss: 1.6649\n",
            "  Batch 2660/3208 | Loss: 0.9704\n",
            "  Batch 2670/3208 | Loss: 1.5597\n",
            "  Batch 2680/3208 | Loss: 0.8451\n",
            "  Batch 2690/3208 | Loss: 1.1021\n",
            "  Batch 2700/3208 | Loss: 1.0366\n",
            "  Batch 2710/3208 | Loss: 0.7445\n",
            "  Batch 2720/3208 | Loss: 1.1778\n",
            "  Batch 2730/3208 | Loss: 1.3366\n",
            "  Batch 2740/3208 | Loss: 1.4102\n",
            "  Batch 2750/3208 | Loss: 1.4585\n",
            "  Batch 2760/3208 | Loss: 1.8274\n",
            "  Batch 2770/3208 | Loss: 1.1532\n",
            "  Batch 2780/3208 | Loss: 2.2115\n",
            "  Batch 2790/3208 | Loss: 1.2565\n",
            "  Batch 2800/3208 | Loss: 1.3612\n",
            "  Batch 2810/3208 | Loss: 1.1486\n",
            "  Batch 2820/3208 | Loss: 1.0637\n",
            "  Batch 2830/3208 | Loss: 1.1289\n",
            "  Batch 2840/3208 | Loss: 1.4976\n",
            "  Batch 2850/3208 | Loss: 1.1327\n",
            "  Batch 2860/3208 | Loss: 1.3209\n",
            "  Batch 2870/3208 | Loss: 1.3467\n",
            "  Batch 2880/3208 | Loss: 1.0410\n",
            "  Batch 2890/3208 | Loss: 1.5622\n",
            "  Batch 2900/3208 | Loss: 1.6012\n",
            "  Batch 2910/3208 | Loss: 1.5237\n",
            "  Batch 2920/3208 | Loss: 0.7374\n",
            "  Batch 2930/3208 | Loss: 1.0917\n",
            "  Batch 2940/3208 | Loss: 1.1148\n",
            "  Batch 2950/3208 | Loss: 1.3253\n",
            "  Batch 2960/3208 | Loss: 0.7383\n",
            "  Batch 2970/3208 | Loss: 1.6813\n",
            "  Batch 2980/3208 | Loss: 0.8519\n",
            "  Batch 2990/3208 | Loss: 0.9537\n",
            "  Batch 3000/3208 | Loss: 0.8453\n",
            "  Batch 3010/3208 | Loss: 1.1569\n",
            "  Batch 3020/3208 | Loss: 2.0094\n",
            "  Batch 3030/3208 | Loss: 1.3594\n",
            "  Batch 3040/3208 | Loss: 1.1133\n",
            "  Batch 3050/3208 | Loss: 0.8301\n",
            "  Batch 3060/3208 | Loss: 1.0844\n",
            "  Batch 3070/3208 | Loss: 1.6040\n",
            "  Batch 3080/3208 | Loss: 1.3322\n",
            "  Batch 3090/3208 | Loss: 1.1129\n",
            "  Batch 3100/3208 | Loss: 1.2067\n",
            "  Batch 3110/3208 | Loss: 1.1370\n",
            "  Batch 3120/3208 | Loss: 1.0950\n",
            "  Batch 3130/3208 | Loss: 1.3811\n",
            "  Batch 3140/3208 | Loss: 1.8016\n",
            "  Batch 3150/3208 | Loss: 0.8958\n",
            "  Batch 3160/3208 | Loss: 1.3679\n",
            "  Batch 3170/3208 | Loss: 1.4124\n",
            "  Batch 3180/3208 | Loss: 1.0652\n",
            "  Batch 3190/3208 | Loss: 1.4421\n",
            "  Batch 3200/3208 | Loss: 1.5823\n",
            " Epoch 12 Completed | Avg Loss: 1.3010 | Training Accuracy: 62.37%\n",
            " Validation Accuracy: 67.01%\n",
            " Best Model Saved (Epoch 12 | Accuracy: 67.01%)\n",
            "\n",
            " Epoch 13/20\n",
            "  Batch 10/3208 | Loss: 1.0275\n",
            "  Batch 20/3208 | Loss: 1.2765\n",
            "  Batch 30/3208 | Loss: 1.3170\n",
            "  Batch 40/3208 | Loss: 1.3446\n",
            "  Batch 50/3208 | Loss: 1.2931\n",
            "  Batch 60/3208 | Loss: 1.5041\n",
            "  Batch 70/3208 | Loss: 1.2386\n",
            "  Batch 80/3208 | Loss: 1.6567\n",
            "  Batch 90/3208 | Loss: 1.3229\n",
            "  Batch 100/3208 | Loss: 1.3856\n",
            "  Batch 110/3208 | Loss: 1.0340\n",
            "  Batch 120/3208 | Loss: 1.4433\n",
            "  Batch 130/3208 | Loss: 2.3377\n",
            "  Batch 140/3208 | Loss: 1.7434\n",
            "  Batch 150/3208 | Loss: 1.3808\n",
            "  Batch 160/3208 | Loss: 1.9302\n",
            "  Batch 170/3208 | Loss: 1.3396\n",
            "  Batch 180/3208 | Loss: 1.2833\n",
            "  Batch 190/3208 | Loss: 1.4550\n",
            "  Batch 200/3208 | Loss: 1.1778\n",
            "  Batch 210/3208 | Loss: 0.8651\n",
            "  Batch 220/3208 | Loss: 1.2140\n",
            "  Batch 230/3208 | Loss: 0.9813\n",
            "  Batch 240/3208 | Loss: 1.1087\n",
            "  Batch 250/3208 | Loss: 1.0138\n",
            "  Batch 260/3208 | Loss: 1.5056\n",
            "  Batch 270/3208 | Loss: 1.5256\n",
            "  Batch 280/3208 | Loss: 1.2510\n",
            "  Batch 290/3208 | Loss: 1.0163\n",
            "  Batch 300/3208 | Loss: 1.9147\n",
            "  Batch 310/3208 | Loss: 1.1031\n",
            "  Batch 320/3208 | Loss: 1.2885\n",
            "  Batch 330/3208 | Loss: 1.7484\n",
            "  Batch 340/3208 | Loss: 0.8877\n",
            "  Batch 350/3208 | Loss: 0.9238\n",
            "  Batch 360/3208 | Loss: 1.4134\n",
            "  Batch 370/3208 | Loss: 1.9145\n",
            "  Batch 380/3208 | Loss: 1.0976\n",
            "  Batch 390/3208 | Loss: 0.8245\n",
            "  Batch 400/3208 | Loss: 1.1064\n",
            "  Batch 410/3208 | Loss: 0.9743\n",
            "  Batch 420/3208 | Loss: 1.4555\n",
            "  Batch 430/3208 | Loss: 1.6338\n",
            "  Batch 440/3208 | Loss: 0.8590\n",
            "  Batch 450/3208 | Loss: 0.9021\n",
            "  Batch 460/3208 | Loss: 0.9826\n",
            "  Batch 470/3208 | Loss: 0.8064\n",
            "  Batch 480/3208 | Loss: 1.4297\n",
            "  Batch 490/3208 | Loss: 0.8321\n",
            "  Batch 500/3208 | Loss: 1.2689\n",
            "  Batch 510/3208 | Loss: 1.2045\n",
            "  Batch 520/3208 | Loss: 1.2978\n",
            "  Batch 530/3208 | Loss: 1.0246\n",
            "  Batch 540/3208 | Loss: 0.8701\n",
            "  Batch 550/3208 | Loss: 0.9709\n",
            "  Batch 560/3208 | Loss: 1.9458\n",
            "  Batch 570/3208 | Loss: 1.1841\n",
            "  Batch 580/3208 | Loss: 1.0029\n",
            "  Batch 590/3208 | Loss: 1.2764\n",
            "  Batch 600/3208 | Loss: 1.3572\n",
            "  Batch 610/3208 | Loss: 1.6873\n",
            "  Batch 620/3208 | Loss: 1.2470\n",
            "  Batch 630/3208 | Loss: 2.3567\n",
            "  Batch 640/3208 | Loss: 1.1050\n",
            "  Batch 650/3208 | Loss: 1.0556\n",
            "  Batch 660/3208 | Loss: 1.6639\n",
            "  Batch 670/3208 | Loss: 1.0382\n",
            "  Batch 680/3208 | Loss: 1.3179\n",
            "  Batch 690/3208 | Loss: 1.2371\n",
            "  Batch 700/3208 | Loss: 1.9414\n",
            "  Batch 710/3208 | Loss: 1.2382\n",
            "  Batch 720/3208 | Loss: 1.0922\n",
            "  Batch 730/3208 | Loss: 1.1371\n",
            "  Batch 740/3208 | Loss: 1.3131\n",
            "  Batch 750/3208 | Loss: 1.4846\n",
            "  Batch 760/3208 | Loss: 1.8337\n",
            "  Batch 770/3208 | Loss: 1.2914\n",
            "  Batch 780/3208 | Loss: 0.9555\n",
            "  Batch 790/3208 | Loss: 1.4582\n",
            "  Batch 800/3208 | Loss: 1.1782\n",
            "  Batch 810/3208 | Loss: 1.8612\n",
            "  Batch 820/3208 | Loss: 1.1153\n",
            "  Batch 830/3208 | Loss: 1.6297\n",
            "  Batch 840/3208 | Loss: 1.5961\n",
            "  Batch 850/3208 | Loss: 1.1155\n",
            "  Batch 860/3208 | Loss: 1.1813\n",
            "  Batch 870/3208 | Loss: 1.2150\n",
            "  Batch 880/3208 | Loss: 1.1421\n",
            "  Batch 890/3208 | Loss: 0.7277\n",
            "  Batch 900/3208 | Loss: 1.0681\n",
            "  Batch 910/3208 | Loss: 1.2372\n",
            "  Batch 920/3208 | Loss: 0.9941\n",
            "  Batch 930/3208 | Loss: 1.3096\n",
            "  Batch 940/3208 | Loss: 1.5646\n",
            "  Batch 950/3208 | Loss: 1.4123\n",
            "  Batch 960/3208 | Loss: 1.1141\n",
            "  Batch 970/3208 | Loss: 0.9556\n",
            "  Batch 980/3208 | Loss: 1.5988\n",
            "  Batch 990/3208 | Loss: 1.2862\n",
            "  Batch 1000/3208 | Loss: 1.4580\n",
            "  Batch 1010/3208 | Loss: 1.4217\n",
            "  Batch 1020/3208 | Loss: 1.2041\n",
            "  Batch 1030/3208 | Loss: 1.1454\n",
            "  Batch 1040/3208 | Loss: 1.1507\n",
            "  Batch 1050/3208 | Loss: 0.8265\n",
            "  Batch 1060/3208 | Loss: 0.9739\n",
            "  Batch 1070/3208 | Loss: 1.2034\n",
            "  Batch 1080/3208 | Loss: 1.0617\n",
            "  Batch 1090/3208 | Loss: 0.7911\n",
            "  Batch 1100/3208 | Loss: 0.8746\n",
            "  Batch 1110/3208 | Loss: 1.0884\n",
            "  Batch 1120/3208 | Loss: 0.8671\n",
            "  Batch 1130/3208 | Loss: 1.2446\n",
            "  Batch 1140/3208 | Loss: 1.4101\n",
            "  Batch 1150/3208 | Loss: 1.6343\n",
            "  Batch 1160/3208 | Loss: 1.1817\n",
            "  Batch 1170/3208 | Loss: 1.6426\n",
            "  Batch 1180/3208 | Loss: 0.7485\n",
            "  Batch 1190/3208 | Loss: 1.2860\n",
            "  Batch 1200/3208 | Loss: 1.0161\n",
            "  Batch 1210/3208 | Loss: 0.8134\n",
            "  Batch 1220/3208 | Loss: 1.7320\n",
            "  Batch 1230/3208 | Loss: 1.2059\n",
            "  Batch 1240/3208 | Loss: 0.7412\n",
            "  Batch 1250/3208 | Loss: 0.8852\n",
            "  Batch 1260/3208 | Loss: 1.3994\n",
            "  Batch 1270/3208 | Loss: 1.4263\n",
            "  Batch 1280/3208 | Loss: 1.4115\n",
            "  Batch 1290/3208 | Loss: 0.9161\n",
            "  Batch 1300/3208 | Loss: 1.3115\n",
            "  Batch 1310/3208 | Loss: 1.4440\n",
            "  Batch 1320/3208 | Loss: 1.2361\n",
            "  Batch 1330/3208 | Loss: 1.5062\n",
            "  Batch 1340/3208 | Loss: 1.9245\n",
            "  Batch 1350/3208 | Loss: 1.2150\n",
            "  Batch 1360/3208 | Loss: 1.3439\n",
            "  Batch 1370/3208 | Loss: 1.0607\n",
            "  Batch 1380/3208 | Loss: 1.4080\n",
            "  Batch 1390/3208 | Loss: 0.9400\n",
            "  Batch 1400/3208 | Loss: 0.7615\n",
            "  Batch 1410/3208 | Loss: 1.0021\n",
            "  Batch 1420/3208 | Loss: 1.3426\n",
            "  Batch 1430/3208 | Loss: 1.4027\n",
            "  Batch 1440/3208 | Loss: 1.5497\n",
            "  Batch 1450/3208 | Loss: 1.0190\n",
            "  Batch 1460/3208 | Loss: 1.2609\n",
            "  Batch 1470/3208 | Loss: 1.2167\n",
            "  Batch 1480/3208 | Loss: 1.0205\n",
            "  Batch 1490/3208 | Loss: 1.1910\n",
            "  Batch 1500/3208 | Loss: 1.1424\n",
            "  Batch 1510/3208 | Loss: 1.2301\n",
            "  Batch 1520/3208 | Loss: 1.5120\n",
            "  Batch 1530/3208 | Loss: 1.3732\n",
            "  Batch 1540/3208 | Loss: 1.3113\n",
            "  Batch 1550/3208 | Loss: 1.1575\n",
            "  Batch 1560/3208 | Loss: 0.9290\n",
            "  Batch 1570/3208 | Loss: 1.1083\n",
            "  Batch 1580/3208 | Loss: 1.5584\n",
            "  Batch 1590/3208 | Loss: 1.2061\n",
            "  Batch 1600/3208 | Loss: 1.1365\n",
            "  Batch 1610/3208 | Loss: 0.8635\n",
            "  Batch 1620/3208 | Loss: 1.3881\n",
            "  Batch 1630/3208 | Loss: 1.0465\n",
            "  Batch 1640/3208 | Loss: 1.1147\n",
            "  Batch 1650/3208 | Loss: 1.0514\n",
            "  Batch 1660/3208 | Loss: 1.4426\n",
            "  Batch 1670/3208 | Loss: 1.3094\n",
            "  Batch 1680/3208 | Loss: 1.1379\n",
            "  Batch 1690/3208 | Loss: 1.3934\n",
            "  Batch 1700/3208 | Loss: 0.5378\n",
            "  Batch 1710/3208 | Loss: 1.6289\n",
            "  Batch 1720/3208 | Loss: 1.1694\n",
            "  Batch 1730/3208 | Loss: 0.8027\n",
            "  Batch 1740/3208 | Loss: 1.3649\n",
            "  Batch 1750/3208 | Loss: 1.2335\n",
            "  Batch 1760/3208 | Loss: 0.7813\n",
            "  Batch 1770/3208 | Loss: 1.0233\n",
            "  Batch 1780/3208 | Loss: 1.1935\n",
            "  Batch 1790/3208 | Loss: 1.6805\n",
            "  Batch 1800/3208 | Loss: 1.4639\n",
            "  Batch 1810/3208 | Loss: 1.2935\n",
            "  Batch 1820/3208 | Loss: 1.3856\n",
            "  Batch 1830/3208 | Loss: 1.5013\n",
            "  Batch 1840/3208 | Loss: 1.0701\n",
            "  Batch 1850/3208 | Loss: 1.1670\n",
            "  Batch 1860/3208 | Loss: 1.3281\n",
            "  Batch 1870/3208 | Loss: 1.4874\n",
            "  Batch 1880/3208 | Loss: 0.8259\n",
            "  Batch 1890/3208 | Loss: 1.8490\n",
            "  Batch 1900/3208 | Loss: 1.1886\n",
            "  Batch 1910/3208 | Loss: 1.1128\n",
            "  Batch 1920/3208 | Loss: 1.5197\n",
            "  Batch 1930/3208 | Loss: 1.0129\n",
            "  Batch 1940/3208 | Loss: 1.0042\n",
            "  Batch 1950/3208 | Loss: 1.5060\n",
            "  Batch 1960/3208 | Loss: 1.4944\n",
            "  Batch 1970/3208 | Loss: 0.9290\n",
            "  Batch 1980/3208 | Loss: 1.2464\n",
            "  Batch 1990/3208 | Loss: 1.3049\n",
            "  Batch 2000/3208 | Loss: 1.0917\n",
            "  Batch 2010/3208 | Loss: 0.9660\n",
            "  Batch 2020/3208 | Loss: 0.8587\n",
            "  Batch 2030/3208 | Loss: 1.2782\n",
            "  Batch 2040/3208 | Loss: 1.0189\n",
            "  Batch 2050/3208 | Loss: 1.0288\n",
            "  Batch 2060/3208 | Loss: 1.5911\n",
            "  Batch 2070/3208 | Loss: 0.9026\n",
            "  Batch 2080/3208 | Loss: 1.5755\n",
            "  Batch 2090/3208 | Loss: 1.2128\n",
            "  Batch 2100/3208 | Loss: 1.8432\n",
            "  Batch 2110/3208 | Loss: 1.0714\n",
            "  Batch 2120/3208 | Loss: 1.3279\n",
            "  Batch 2130/3208 | Loss: 0.9048\n",
            "  Batch 2140/3208 | Loss: 1.0003\n",
            "  Batch 2150/3208 | Loss: 1.1520\n",
            "  Batch 2160/3208 | Loss: 1.4574\n",
            "  Batch 2170/3208 | Loss: 1.1496\n",
            "  Batch 2180/3208 | Loss: 1.1845\n",
            "  Batch 2190/3208 | Loss: 0.9943\n",
            "  Batch 2200/3208 | Loss: 1.1059\n",
            "  Batch 2210/3208 | Loss: 1.6681\n",
            "  Batch 2220/3208 | Loss: 1.5627\n",
            "  Batch 2230/3208 | Loss: 1.2331\n",
            "  Batch 2240/3208 | Loss: 1.7919\n",
            "  Batch 2250/3208 | Loss: 1.4275\n",
            "  Batch 2260/3208 | Loss: 1.8039\n",
            "  Batch 2270/3208 | Loss: 1.0620\n",
            "  Batch 2280/3208 | Loss: 1.7158\n",
            "  Batch 2290/3208 | Loss: 0.7379\n",
            "  Batch 2300/3208 | Loss: 1.2577\n",
            "  Batch 2310/3208 | Loss: 0.8797\n",
            "  Batch 2320/3208 | Loss: 0.9076\n",
            "  Batch 2330/3208 | Loss: 1.2986\n",
            "  Batch 2340/3208 | Loss: 0.9716\n",
            "  Batch 2350/3208 | Loss: 1.5486\n",
            "  Batch 2360/3208 | Loss: 0.7576\n",
            "  Batch 2370/3208 | Loss: 1.0778\n",
            "  Batch 2380/3208 | Loss: 0.9344\n",
            "  Batch 2390/3208 | Loss: 1.0827\n",
            "  Batch 2400/3208 | Loss: 1.3689\n",
            "  Batch 2410/3208 | Loss: 1.0928\n",
            "  Batch 2420/3208 | Loss: 1.1530\n",
            "  Batch 2430/3208 | Loss: 0.8573\n",
            "  Batch 2440/3208 | Loss: 1.0160\n",
            "  Batch 2450/3208 | Loss: 0.7900\n",
            "  Batch 2460/3208 | Loss: 1.1575\n",
            "  Batch 2470/3208 | Loss: 1.0805\n",
            "  Batch 2480/3208 | Loss: 1.5123\n",
            "  Batch 2490/3208 | Loss: 0.8971\n",
            "  Batch 2500/3208 | Loss: 1.4104\n",
            "  Batch 2510/3208 | Loss: 1.0093\n",
            "  Batch 2520/3208 | Loss: 1.0320\n",
            "  Batch 2530/3208 | Loss: 1.0935\n",
            "  Batch 2540/3208 | Loss: 1.2068\n",
            "  Batch 2550/3208 | Loss: 1.7586\n",
            "  Batch 2560/3208 | Loss: 1.1417\n",
            "  Batch 2570/3208 | Loss: 1.0530\n",
            "  Batch 2580/3208 | Loss: 1.1937\n",
            "  Batch 2590/3208 | Loss: 1.0300\n",
            "  Batch 2600/3208 | Loss: 0.9019\n",
            "  Batch 2610/3208 | Loss: 1.0018\n",
            "  Batch 2620/3208 | Loss: 1.2279\n",
            "  Batch 2630/3208 | Loss: 1.4410\n",
            "  Batch 2640/3208 | Loss: 1.2741\n",
            "  Batch 2650/3208 | Loss: 1.3552\n",
            "  Batch 2660/3208 | Loss: 0.7214\n",
            "  Batch 2670/3208 | Loss: 1.6955\n",
            "  Batch 2680/3208 | Loss: 1.7862\n",
            "  Batch 2690/3208 | Loss: 0.9271\n",
            "  Batch 2700/3208 | Loss: 0.8627\n",
            "  Batch 2710/3208 | Loss: 0.8234\n",
            "  Batch 2720/3208 | Loss: 1.2551\n",
            "  Batch 2730/3208 | Loss: 1.3102\n",
            "  Batch 2740/3208 | Loss: 1.4228\n",
            "  Batch 2750/3208 | Loss: 1.0524\n",
            "  Batch 2760/3208 | Loss: 1.2568\n",
            "  Batch 2770/3208 | Loss: 0.8349\n",
            "  Batch 2780/3208 | Loss: 1.1761\n",
            "  Batch 2790/3208 | Loss: 1.1120\n",
            "  Batch 2800/3208 | Loss: 1.2113\n",
            "  Batch 2810/3208 | Loss: 1.0247\n",
            "  Batch 2820/3208 | Loss: 1.1637\n",
            "  Batch 2830/3208 | Loss: 0.8171\n",
            "  Batch 2840/3208 | Loss: 1.0938\n",
            "  Batch 2850/3208 | Loss: 1.3784\n",
            "  Batch 2860/3208 | Loss: 1.3041\n",
            "  Batch 2870/3208 | Loss: 1.6396\n",
            "  Batch 2880/3208 | Loss: 2.2856\n",
            "  Batch 2890/3208 | Loss: 1.0949\n",
            "  Batch 2900/3208 | Loss: 1.2381\n",
            "  Batch 2910/3208 | Loss: 1.4485\n",
            "  Batch 2920/3208 | Loss: 1.1435\n",
            "  Batch 2930/3208 | Loss: 1.9678\n",
            "  Batch 2940/3208 | Loss: 1.2719\n",
            "  Batch 2950/3208 | Loss: 1.1557\n",
            "  Batch 2960/3208 | Loss: 0.9770\n",
            "  Batch 2970/3208 | Loss: 1.1534\n",
            "  Batch 2980/3208 | Loss: 0.8569\n",
            "  Batch 2990/3208 | Loss: 1.0892\n",
            "  Batch 3000/3208 | Loss: 0.9603\n",
            "  Batch 3010/3208 | Loss: 1.3178\n",
            "  Batch 3020/3208 | Loss: 1.5915\n",
            "  Batch 3030/3208 | Loss: 1.5960\n",
            "  Batch 3040/3208 | Loss: 1.0763\n",
            "  Batch 3050/3208 | Loss: 1.5165\n",
            "  Batch 3060/3208 | Loss: 0.8462\n",
            "  Batch 3070/3208 | Loss: 1.4327\n",
            "  Batch 3080/3208 | Loss: 1.4788\n",
            "  Batch 3090/3208 | Loss: 1.6209\n",
            "  Batch 3100/3208 | Loss: 1.0352\n",
            "  Batch 3110/3208 | Loss: 1.2123\n",
            "  Batch 3120/3208 | Loss: 1.1887\n",
            "  Batch 3130/3208 | Loss: 0.8988\n",
            "  Batch 3140/3208 | Loss: 0.8790\n",
            "  Batch 3150/3208 | Loss: 0.9898\n",
            "  Batch 3160/3208 | Loss: 0.7582\n",
            "  Batch 3170/3208 | Loss: 1.4720\n",
            "  Batch 3180/3208 | Loss: 1.2028\n",
            "  Batch 3190/3208 | Loss: 2.0426\n",
            "  Batch 3200/3208 | Loss: 0.8995\n",
            " Epoch 13 Completed | Avg Loss: 1.2431 | Training Accuracy: 63.61%\n",
            " Validation Accuracy: 69.08%\n",
            " Best Model Saved (Epoch 13 | Accuracy: 69.08%)\n",
            "\n",
            " Epoch 14/20\n",
            "  Batch 10/3208 | Loss: 0.7396\n",
            "  Batch 20/3208 | Loss: 1.2278\n",
            "  Batch 30/3208 | Loss: 1.2981\n",
            "  Batch 40/3208 | Loss: 1.1340\n",
            "  Batch 50/3208 | Loss: 1.1070\n",
            "  Batch 60/3208 | Loss: 1.0693\n",
            "  Batch 70/3208 | Loss: 1.3537\n",
            "  Batch 80/3208 | Loss: 1.6280\n",
            "  Batch 90/3208 | Loss: 1.2086\n",
            "  Batch 100/3208 | Loss: 1.2674\n",
            "  Batch 110/3208 | Loss: 1.3597\n",
            "  Batch 120/3208 | Loss: 0.7119\n",
            "  Batch 130/3208 | Loss: 1.0891\n",
            "  Batch 140/3208 | Loss: 1.3295\n",
            "  Batch 150/3208 | Loss: 0.8627\n",
            "  Batch 160/3208 | Loss: 1.1140\n",
            "  Batch 170/3208 | Loss: 1.2118\n",
            "  Batch 180/3208 | Loss: 1.1115\n",
            "  Batch 190/3208 | Loss: 1.4105\n",
            "  Batch 200/3208 | Loss: 1.4068\n",
            "  Batch 210/3208 | Loss: 1.2851\n",
            "  Batch 220/3208 | Loss: 0.8137\n",
            "  Batch 230/3208 | Loss: 1.1986\n",
            "  Batch 240/3208 | Loss: 0.8491\n",
            "  Batch 250/3208 | Loss: 1.1956\n",
            "  Batch 260/3208 | Loss: 1.3039\n",
            "  Batch 270/3208 | Loss: 1.5048\n",
            "  Batch 280/3208 | Loss: 1.3592\n",
            "  Batch 290/3208 | Loss: 0.8751\n",
            "  Batch 300/3208 | Loss: 1.8528\n",
            "  Batch 310/3208 | Loss: 1.0279\n",
            "  Batch 320/3208 | Loss: 1.4879\n",
            "  Batch 330/3208 | Loss: 1.1675\n",
            "  Batch 340/3208 | Loss: 1.2339\n",
            "  Batch 350/3208 | Loss: 1.1292\n",
            "  Batch 360/3208 | Loss: 1.1864\n",
            "  Batch 370/3208 | Loss: 1.0000\n",
            "  Batch 380/3208 | Loss: 1.1226\n",
            "  Batch 390/3208 | Loss: 0.9127\n",
            "  Batch 400/3208 | Loss: 0.9463\n",
            "  Batch 410/3208 | Loss: 1.5557\n",
            "  Batch 420/3208 | Loss: 1.3795\n",
            "  Batch 430/3208 | Loss: 1.7620\n",
            "  Batch 440/3208 | Loss: 0.8175\n",
            "  Batch 450/3208 | Loss: 1.5883\n",
            "  Batch 460/3208 | Loss: 1.1515\n",
            "  Batch 470/3208 | Loss: 1.5566\n",
            "  Batch 480/3208 | Loss: 0.7357\n",
            "  Batch 490/3208 | Loss: 1.2751\n",
            "  Batch 500/3208 | Loss: 1.1531\n",
            "  Batch 510/3208 | Loss: 0.8087\n",
            "  Batch 520/3208 | Loss: 1.0730\n",
            "  Batch 530/3208 | Loss: 1.2297\n",
            "  Batch 540/3208 | Loss: 1.7040\n",
            "  Batch 550/3208 | Loss: 0.8701\n",
            "  Batch 560/3208 | Loss: 1.2044\n",
            "  Batch 570/3208 | Loss: 1.6565\n",
            "  Batch 580/3208 | Loss: 1.2765\n",
            "  Batch 590/3208 | Loss: 1.1518\n",
            "  Batch 600/3208 | Loss: 0.9454\n",
            "  Batch 610/3208 | Loss: 1.6040\n",
            "  Batch 620/3208 | Loss: 1.0487\n",
            "  Batch 630/3208 | Loss: 1.1080\n",
            "  Batch 640/3208 | Loss: 1.6112\n",
            "  Batch 650/3208 | Loss: 1.5559\n",
            "  Batch 660/3208 | Loss: 0.9251\n",
            "  Batch 670/3208 | Loss: 1.4771\n",
            "  Batch 680/3208 | Loss: 0.9731\n",
            "  Batch 690/3208 | Loss: 1.1312\n",
            "  Batch 700/3208 | Loss: 1.4497\n",
            "  Batch 710/3208 | Loss: 1.0673\n",
            "  Batch 720/3208 | Loss: 1.3668\n",
            "  Batch 730/3208 | Loss: 1.6201\n",
            "  Batch 740/3208 | Loss: 1.2266\n",
            "  Batch 750/3208 | Loss: 1.4588\n",
            "  Batch 760/3208 | Loss: 0.9366\n",
            "  Batch 770/3208 | Loss: 1.7613\n",
            "  Batch 780/3208 | Loss: 1.2842\n",
            "  Batch 790/3208 | Loss: 1.4881\n",
            "  Batch 800/3208 | Loss: 1.4133\n",
            "  Batch 810/3208 | Loss: 1.7167\n",
            "  Batch 820/3208 | Loss: 1.2243\n",
            "  Batch 830/3208 | Loss: 0.7338\n",
            "  Batch 840/3208 | Loss: 1.2130\n",
            "  Batch 850/3208 | Loss: 0.9313\n",
            "  Batch 860/3208 | Loss: 2.0759\n",
            "  Batch 870/3208 | Loss: 1.3923\n",
            "  Batch 880/3208 | Loss: 1.2866\n",
            "  Batch 890/3208 | Loss: 1.3297\n",
            "  Batch 900/3208 | Loss: 1.0223\n",
            "  Batch 910/3208 | Loss: 1.4096\n",
            "  Batch 920/3208 | Loss: 0.8085\n",
            "  Batch 930/3208 | Loss: 1.5644\n",
            "  Batch 940/3208 | Loss: 1.2010\n",
            "  Batch 950/3208 | Loss: 1.2585\n",
            "  Batch 960/3208 | Loss: 1.0579\n",
            "  Batch 970/3208 | Loss: 1.2476\n",
            "  Batch 980/3208 | Loss: 0.7101\n",
            "  Batch 990/3208 | Loss: 1.8292\n",
            "  Batch 1000/3208 | Loss: 1.3578\n",
            "  Batch 1010/3208 | Loss: 1.6199\n",
            "  Batch 1020/3208 | Loss: 1.6617\n",
            "  Batch 1030/3208 | Loss: 0.8725\n",
            "  Batch 1040/3208 | Loss: 0.9668\n",
            "  Batch 1050/3208 | Loss: 1.0304\n",
            "  Batch 1060/3208 | Loss: 1.7572\n",
            "  Batch 1070/3208 | Loss: 1.4010\n",
            "  Batch 1080/3208 | Loss: 1.0648\n",
            "  Batch 1090/3208 | Loss: 1.3952\n",
            "  Batch 1100/3208 | Loss: 0.9739\n",
            "  Batch 1110/3208 | Loss: 0.9184\n",
            "  Batch 1120/3208 | Loss: 1.1475\n",
            "  Batch 1130/3208 | Loss: 1.0550\n",
            "  Batch 1140/3208 | Loss: 1.5301\n",
            "  Batch 1150/3208 | Loss: 1.2595\n",
            "  Batch 1160/3208 | Loss: 1.4045\n",
            "  Batch 1170/3208 | Loss: 0.8368\n",
            "  Batch 1180/3208 | Loss: 1.7742\n",
            "  Batch 1190/3208 | Loss: 1.0789\n",
            "  Batch 1200/3208 | Loss: 1.6198\n",
            "  Batch 1210/3208 | Loss: 1.3180\n",
            "  Batch 1220/3208 | Loss: 1.0568\n",
            "  Batch 1230/3208 | Loss: 1.4360\n",
            "  Batch 1240/3208 | Loss: 1.0556\n",
            "  Batch 1250/3208 | Loss: 1.0979\n",
            "  Batch 1260/3208 | Loss: 0.9531\n",
            "  Batch 1270/3208 | Loss: 1.3262\n",
            "  Batch 1280/3208 | Loss: 1.4781\n",
            "  Batch 1290/3208 | Loss: 1.5903\n",
            "  Batch 1300/3208 | Loss: 1.2029\n",
            "  Batch 1310/3208 | Loss: 1.7756\n",
            "  Batch 1320/3208 | Loss: 2.2482\n",
            "  Batch 1330/3208 | Loss: 1.4769\n",
            "  Batch 1340/3208 | Loss: 0.9521\n",
            "  Batch 1350/3208 | Loss: 1.5971\n",
            "  Batch 1360/3208 | Loss: 0.9180\n",
            "  Batch 1370/3208 | Loss: 0.7982\n",
            "  Batch 1380/3208 | Loss: 1.8016\n",
            "  Batch 1390/3208 | Loss: 0.8726\n",
            "  Batch 1400/3208 | Loss: 1.0022\n",
            "  Batch 1410/3208 | Loss: 0.9023\n",
            "  Batch 1420/3208 | Loss: 1.2578\n",
            "  Batch 1430/3208 | Loss: 0.7262\n",
            "  Batch 1440/3208 | Loss: 1.2084\n",
            "  Batch 1450/3208 | Loss: 1.4037\n",
            "  Batch 1460/3208 | Loss: 1.4547\n",
            "  Batch 1470/3208 | Loss: 0.8527\n",
            "  Batch 1480/3208 | Loss: 1.9456\n",
            "  Batch 1490/3208 | Loss: 1.5650\n",
            "  Batch 1500/3208 | Loss: 0.9182\n",
            "  Batch 1510/3208 | Loss: 0.9307\n",
            "  Batch 1520/3208 | Loss: 1.4210\n",
            "  Batch 1530/3208 | Loss: 1.8442\n",
            "  Batch 1540/3208 | Loss: 0.7553\n",
            "  Batch 1550/3208 | Loss: 0.8094\n",
            "  Batch 1560/3208 | Loss: 1.2609\n",
            "  Batch 1570/3208 | Loss: 1.1907\n",
            "  Batch 1580/3208 | Loss: 1.0604\n",
            "  Batch 1590/3208 | Loss: 1.4006\n",
            "  Batch 1600/3208 | Loss: 1.2019\n",
            "  Batch 1610/3208 | Loss: 0.8572\n",
            "  Batch 1620/3208 | Loss: 1.4387\n",
            "  Batch 1630/3208 | Loss: 1.1134\n",
            "  Batch 1640/3208 | Loss: 1.1956\n",
            "  Batch 1650/3208 | Loss: 1.1658\n",
            "  Batch 1660/3208 | Loss: 1.3307\n",
            "  Batch 1670/3208 | Loss: 1.4687\n",
            "  Batch 1680/3208 | Loss: 1.1563\n",
            "  Batch 1690/3208 | Loss: 1.0182\n",
            "  Batch 1700/3208 | Loss: 1.2784\n",
            "  Batch 1710/3208 | Loss: 0.9447\n",
            "  Batch 1720/3208 | Loss: 1.3495\n",
            "  Batch 1730/3208 | Loss: 1.2181\n",
            "  Batch 1740/3208 | Loss: 0.8999\n",
            "  Batch 1750/3208 | Loss: 1.5491\n",
            "  Batch 1760/3208 | Loss: 1.1697\n",
            "  Batch 1770/3208 | Loss: 1.1059\n",
            "  Batch 1780/3208 | Loss: 1.2319\n",
            "  Batch 1790/3208 | Loss: 1.7262\n",
            "  Batch 1800/3208 | Loss: 1.1311\n",
            "  Batch 1810/3208 | Loss: 1.0011\n",
            "  Batch 1820/3208 | Loss: 1.3779\n",
            "  Batch 1830/3208 | Loss: 1.2822\n",
            "  Batch 1840/3208 | Loss: 1.2039\n",
            "  Batch 1850/3208 | Loss: 1.1570\n",
            "  Batch 1860/3208 | Loss: 0.9556\n",
            "  Batch 1870/3208 | Loss: 1.1733\n",
            "  Batch 1880/3208 | Loss: 1.7038\n",
            "  Batch 1890/3208 | Loss: 1.1193\n",
            "  Batch 1900/3208 | Loss: 1.3527\n",
            "  Batch 1910/3208 | Loss: 1.3770\n",
            "  Batch 1920/3208 | Loss: 0.7894\n",
            "  Batch 1930/3208 | Loss: 1.4459\n",
            "  Batch 1940/3208 | Loss: 1.0807\n",
            "  Batch 1950/3208 | Loss: 1.0244\n",
            "  Batch 1960/3208 | Loss: 0.8517\n",
            "  Batch 1970/3208 | Loss: 0.9637\n",
            "  Batch 1980/3208 | Loss: 1.0918\n",
            "  Batch 1990/3208 | Loss: 1.5748\n",
            "  Batch 2000/3208 | Loss: 1.7461\n",
            "  Batch 2010/3208 | Loss: 1.0896\n",
            "  Batch 2020/3208 | Loss: 0.9136\n",
            "  Batch 2030/3208 | Loss: 0.9040\n",
            "  Batch 2040/3208 | Loss: 1.4564\n",
            "  Batch 2050/3208 | Loss: 1.2941\n",
            "  Batch 2060/3208 | Loss: 1.1873\n",
            "  Batch 2070/3208 | Loss: 1.2164\n",
            "  Batch 2080/3208 | Loss: 1.7341\n",
            "  Batch 2090/3208 | Loss: 1.2684\n",
            "  Batch 2100/3208 | Loss: 2.2010\n",
            "  Batch 2110/3208 | Loss: 1.2327\n",
            "  Batch 2120/3208 | Loss: 1.0700\n",
            "  Batch 2130/3208 | Loss: 1.0870\n",
            "  Batch 2140/3208 | Loss: 1.0483\n",
            "  Batch 2150/3208 | Loss: 0.9075\n",
            "  Batch 2160/3208 | Loss: 1.3242\n",
            "  Batch 2170/3208 | Loss: 0.9597\n",
            "  Batch 2180/3208 | Loss: 1.5603\n",
            "  Batch 2190/3208 | Loss: 1.3557\n",
            "  Batch 2200/3208 | Loss: 1.4972\n",
            "  Batch 2210/3208 | Loss: 1.0868\n",
            "  Batch 2220/3208 | Loss: 1.6034\n",
            "  Batch 2230/3208 | Loss: 1.0673\n",
            "  Batch 2240/3208 | Loss: 1.3010\n",
            "  Batch 2250/3208 | Loss: 1.8818\n",
            "  Batch 2260/3208 | Loss: 1.5635\n",
            "  Batch 2270/3208 | Loss: 1.0195\n",
            "  Batch 2280/3208 | Loss: 1.3489\n",
            "  Batch 2290/3208 | Loss: 1.2115\n",
            "  Batch 2300/3208 | Loss: 1.3983\n",
            "  Batch 2310/3208 | Loss: 1.5436\n",
            "  Batch 2320/3208 | Loss: 1.1634\n",
            "  Batch 2330/3208 | Loss: 1.3948\n",
            "  Batch 2340/3208 | Loss: 1.3753\n",
            "  Batch 2350/3208 | Loss: 0.8086\n",
            "  Batch 2360/3208 | Loss: 1.0404\n",
            "  Batch 2370/3208 | Loss: 1.0817\n",
            "  Batch 2380/3208 | Loss: 1.0726\n",
            "  Batch 2390/3208 | Loss: 0.9498\n",
            "  Batch 2400/3208 | Loss: 1.4555\n",
            "  Batch 2410/3208 | Loss: 1.2838\n",
            "  Batch 2420/3208 | Loss: 1.4624\n",
            "  Batch 2430/3208 | Loss: 1.4982\n",
            "  Batch 2440/3208 | Loss: 0.7876\n",
            "  Batch 2450/3208 | Loss: 1.2662\n",
            "  Batch 2460/3208 | Loss: 0.9702\n",
            "  Batch 2470/3208 | Loss: 1.6720\n",
            "  Batch 2480/3208 | Loss: 0.9001\n",
            "  Batch 2490/3208 | Loss: 1.3167\n",
            "  Batch 2500/3208 | Loss: 1.7313\n",
            "  Batch 2510/3208 | Loss: 1.2373\n",
            "  Batch 2520/3208 | Loss: 0.9227\n",
            "  Batch 2530/3208 | Loss: 1.3425\n",
            "  Batch 2540/3208 | Loss: 1.2957\n",
            "  Batch 2550/3208 | Loss: 1.0121\n",
            "  Batch 2560/3208 | Loss: 0.8375\n",
            "  Batch 2570/3208 | Loss: 0.8745\n",
            "  Batch 2580/3208 | Loss: 1.3171\n",
            "  Batch 2590/3208 | Loss: 0.9489\n",
            "  Batch 2600/3208 | Loss: 1.0566\n",
            "  Batch 2610/3208 | Loss: 0.8163\n",
            "  Batch 2620/3208 | Loss: 0.9059\n",
            "  Batch 2630/3208 | Loss: 1.4976\n",
            "  Batch 2640/3208 | Loss: 0.7299\n",
            "  Batch 2650/3208 | Loss: 1.1970\n",
            "  Batch 2660/3208 | Loss: 1.4133\n",
            "  Batch 2670/3208 | Loss: 1.5165\n",
            "  Batch 2680/3208 | Loss: 0.7800\n",
            "  Batch 2690/3208 | Loss: 0.9994\n",
            "  Batch 2700/3208 | Loss: 1.4094\n",
            "  Batch 2710/3208 | Loss: 1.2699\n",
            "  Batch 2720/3208 | Loss: 1.5179\n",
            "  Batch 2730/3208 | Loss: 1.2005\n",
            "  Batch 2740/3208 | Loss: 1.0830\n",
            "  Batch 2750/3208 | Loss: 0.8238\n",
            "  Batch 2760/3208 | Loss: 0.5807\n",
            "  Batch 2770/3208 | Loss: 1.3001\n",
            "  Batch 2780/3208 | Loss: 1.4614\n",
            "  Batch 2790/3208 | Loss: 1.2006\n",
            "  Batch 2800/3208 | Loss: 1.0269\n",
            "  Batch 2810/3208 | Loss: 1.9065\n",
            "  Batch 2820/3208 | Loss: 0.8166\n",
            "  Batch 2830/3208 | Loss: 0.9428\n",
            "  Batch 2840/3208 | Loss: 1.4541\n",
            "  Batch 2850/3208 | Loss: 0.9766\n",
            "  Batch 2860/3208 | Loss: 1.2762\n",
            "  Batch 2870/3208 | Loss: 0.4907\n",
            "  Batch 2880/3208 | Loss: 0.9885\n",
            "  Batch 2890/3208 | Loss: 0.8672\n",
            "  Batch 2900/3208 | Loss: 1.0567\n",
            "  Batch 2910/3208 | Loss: 1.0920\n",
            "  Batch 2920/3208 | Loss: 0.9349\n",
            "  Batch 2930/3208 | Loss: 1.4277\n",
            "  Batch 2940/3208 | Loss: 1.2415\n",
            "  Batch 2950/3208 | Loss: 1.0113\n",
            "  Batch 2960/3208 | Loss: 0.8439\n",
            "  Batch 2970/3208 | Loss: 0.7833\n",
            "  Batch 2980/3208 | Loss: 1.2843\n",
            "  Batch 2990/3208 | Loss: 2.0350\n",
            "  Batch 3000/3208 | Loss: 1.4392\n",
            "  Batch 3010/3208 | Loss: 0.9900\n",
            "  Batch 3020/3208 | Loss: 0.9475\n",
            "  Batch 3030/3208 | Loss: 0.8082\n",
            "  Batch 3040/3208 | Loss: 1.9156\n",
            "  Batch 3050/3208 | Loss: 1.3311\n",
            "  Batch 3060/3208 | Loss: 1.2252\n",
            "  Batch 3070/3208 | Loss: 0.7826\n",
            "  Batch 3080/3208 | Loss: 0.7927\n",
            "  Batch 3090/3208 | Loss: 1.1615\n",
            "  Batch 3100/3208 | Loss: 1.5034\n",
            "  Batch 3110/3208 | Loss: 0.9564\n",
            "  Batch 3120/3208 | Loss: 1.4862\n",
            "  Batch 3130/3208 | Loss: 1.3816\n",
            "  Batch 3140/3208 | Loss: 1.1231\n",
            "  Batch 3150/3208 | Loss: 0.8882\n",
            "  Batch 3160/3208 | Loss: 1.4384\n",
            "  Batch 3170/3208 | Loss: 1.1300\n",
            "  Batch 3180/3208 | Loss: 1.3330\n",
            "  Batch 3190/3208 | Loss: 0.7009\n",
            "  Batch 3200/3208 | Loss: 1.0623\n",
            " Epoch 14 Completed | Avg Loss: 1.1985 | Training Accuracy: 64.68%\n",
            " Validation Accuracy: 69.18%\n",
            " Best Model Saved (Epoch 14 | Accuracy: 69.18%)\n",
            "\n",
            " Epoch 15/20\n",
            "  Batch 10/3208 | Loss: 0.7329\n",
            "  Batch 20/3208 | Loss: 1.0898\n",
            "  Batch 30/3208 | Loss: 1.0945\n",
            "  Batch 40/3208 | Loss: 1.3186\n",
            "  Batch 50/3208 | Loss: 0.7640\n",
            "  Batch 60/3208 | Loss: 1.3863\n",
            "  Batch 70/3208 | Loss: 1.3884\n",
            "  Batch 80/3208 | Loss: 1.3080\n",
            "  Batch 90/3208 | Loss: 1.1804\n",
            "  Batch 100/3208 | Loss: 0.8954\n",
            "  Batch 110/3208 | Loss: 0.9314\n",
            "  Batch 120/3208 | Loss: 0.7121\n",
            "  Batch 130/3208 | Loss: 1.4559\n",
            "  Batch 140/3208 | Loss: 0.7155\n",
            "  Batch 150/3208 | Loss: 0.7028\n",
            "  Batch 160/3208 | Loss: 1.5143\n",
            "  Batch 170/3208 | Loss: 1.0299\n",
            "  Batch 180/3208 | Loss: 1.6336\n",
            "  Batch 190/3208 | Loss: 0.9954\n",
            "  Batch 200/3208 | Loss: 1.6027\n",
            "  Batch 210/3208 | Loss: 1.2159\n",
            "  Batch 220/3208 | Loss: 0.8928\n",
            "  Batch 230/3208 | Loss: 1.3957\n",
            "  Batch 240/3208 | Loss: 1.7249\n",
            "  Batch 250/3208 | Loss: 0.7742\n",
            "  Batch 260/3208 | Loss: 0.8128\n",
            "  Batch 270/3208 | Loss: 1.5615\n",
            "  Batch 280/3208 | Loss: 0.7318\n",
            "  Batch 290/3208 | Loss: 1.5594\n",
            "  Batch 300/3208 | Loss: 0.8295\n",
            "  Batch 310/3208 | Loss: 1.0891\n",
            "  Batch 320/3208 | Loss: 1.0182\n",
            "  Batch 330/3208 | Loss: 1.0983\n",
            "  Batch 340/3208 | Loss: 1.6827\n",
            "  Batch 350/3208 | Loss: 1.4172\n",
            "  Batch 360/3208 | Loss: 1.0628\n",
            "  Batch 370/3208 | Loss: 1.1521\n",
            "  Batch 380/3208 | Loss: 1.2496\n",
            "  Batch 390/3208 | Loss: 1.0129\n",
            "  Batch 400/3208 | Loss: 1.0483\n",
            "  Batch 410/3208 | Loss: 1.5568\n",
            "  Batch 420/3208 | Loss: 1.5296\n",
            "  Batch 430/3208 | Loss: 1.6930\n",
            "  Batch 440/3208 | Loss: 0.8651\n",
            "  Batch 450/3208 | Loss: 1.3596\n",
            "  Batch 460/3208 | Loss: 0.6983\n",
            "  Batch 470/3208 | Loss: 1.2315\n",
            "  Batch 480/3208 | Loss: 1.6885\n",
            "  Batch 490/3208 | Loss: 0.8035\n",
            "  Batch 500/3208 | Loss: 1.2913\n",
            "  Batch 510/3208 | Loss: 1.1498\n",
            "  Batch 520/3208 | Loss: 1.4358\n",
            "  Batch 530/3208 | Loss: 1.4288\n",
            "  Batch 540/3208 | Loss: 0.9851\n",
            "  Batch 550/3208 | Loss: 1.5217\n",
            "  Batch 560/3208 | Loss: 1.3716\n",
            "  Batch 570/3208 | Loss: 0.9836\n",
            "  Batch 580/3208 | Loss: 1.7163\n",
            "  Batch 590/3208 | Loss: 0.8229\n",
            "  Batch 600/3208 | Loss: 0.9279\n",
            "  Batch 610/3208 | Loss: 0.9213\n",
            "  Batch 620/3208 | Loss: 1.5389\n",
            "  Batch 630/3208 | Loss: 1.0990\n",
            "  Batch 640/3208 | Loss: 1.0038\n",
            "  Batch 650/3208 | Loss: 1.4901\n",
            "  Batch 660/3208 | Loss: 1.0819\n",
            "  Batch 670/3208 | Loss: 0.9992\n",
            "  Batch 680/3208 | Loss: 1.5256\n",
            "  Batch 690/3208 | Loss: 1.2362\n",
            "  Batch 700/3208 | Loss: 0.6460\n",
            "  Batch 710/3208 | Loss: 1.0686\n",
            "  Batch 720/3208 | Loss: 0.7158\n",
            "  Batch 730/3208 | Loss: 1.2288\n",
            "  Batch 740/3208 | Loss: 1.2294\n",
            "  Batch 750/3208 | Loss: 0.9119\n",
            "  Batch 760/3208 | Loss: 1.1310\n",
            "  Batch 770/3208 | Loss: 0.7570\n",
            "  Batch 780/3208 | Loss: 1.2757\n",
            "  Batch 790/3208 | Loss: 0.7567\n",
            "  Batch 800/3208 | Loss: 0.9793\n",
            "  Batch 810/3208 | Loss: 0.9593\n",
            "  Batch 820/3208 | Loss: 1.2989\n",
            "  Batch 830/3208 | Loss: 1.2272\n",
            "  Batch 840/3208 | Loss: 1.2338\n",
            "  Batch 850/3208 | Loss: 1.4180\n",
            "  Batch 860/3208 | Loss: 1.4715\n",
            "  Batch 870/3208 | Loss: 0.6336\n",
            "  Batch 880/3208 | Loss: 1.5408\n",
            "  Batch 890/3208 | Loss: 0.7080\n",
            "  Batch 900/3208 | Loss: 1.2014\n",
            "  Batch 910/3208 | Loss: 1.2974\n",
            "  Batch 920/3208 | Loss: 1.6966\n",
            "  Batch 930/3208 | Loss: 1.4257\n",
            "  Batch 940/3208 | Loss: 0.7351\n",
            "  Batch 950/3208 | Loss: 1.0518\n",
            "  Batch 960/3208 | Loss: 1.4805\n",
            "  Batch 970/3208 | Loss: 1.1846\n",
            "  Batch 980/3208 | Loss: 0.9731\n",
            "  Batch 990/3208 | Loss: 0.8974\n",
            "  Batch 1000/3208 | Loss: 1.5633\n",
            "  Batch 1010/3208 | Loss: 1.0468\n",
            "  Batch 1020/3208 | Loss: 1.1119\n",
            "  Batch 1030/3208 | Loss: 1.2734\n",
            "  Batch 1040/3208 | Loss: 1.5622\n",
            "  Batch 1050/3208 | Loss: 1.3709\n",
            "  Batch 1060/3208 | Loss: 1.5964\n",
            "  Batch 1070/3208 | Loss: 1.2254\n",
            "  Batch 1080/3208 | Loss: 1.3568\n",
            "  Batch 1090/3208 | Loss: 1.2831\n",
            "  Batch 1100/3208 | Loss: 1.0157\n",
            "  Batch 1110/3208 | Loss: 1.0450\n",
            "  Batch 1120/3208 | Loss: 2.0065\n",
            "  Batch 1130/3208 | Loss: 0.9648\n",
            "  Batch 1140/3208 | Loss: 1.8721\n",
            "  Batch 1150/3208 | Loss: 1.5234\n",
            "  Batch 1160/3208 | Loss: 0.8797\n",
            "  Batch 1170/3208 | Loss: 0.7606\n",
            "  Batch 1180/3208 | Loss: 1.0185\n",
            "  Batch 1190/3208 | Loss: 0.8242\n",
            "  Batch 1200/3208 | Loss: 1.2071\n",
            "  Batch 1210/3208 | Loss: 1.0635\n",
            "  Batch 1220/3208 | Loss: 0.8396\n",
            "  Batch 1230/3208 | Loss: 1.0986\n",
            "  Batch 1240/3208 | Loss: 1.7871\n",
            "  Batch 1250/3208 | Loss: 1.8202\n",
            "  Batch 1260/3208 | Loss: 0.9124\n",
            "  Batch 1270/3208 | Loss: 1.7455\n",
            "  Batch 1280/3208 | Loss: 0.7030\n",
            "  Batch 1290/3208 | Loss: 1.1808\n",
            "  Batch 1300/3208 | Loss: 0.7804\n",
            "  Batch 1310/3208 | Loss: 0.6438\n",
            "  Batch 1320/3208 | Loss: 0.7646\n",
            "  Batch 1330/3208 | Loss: 0.7562\n",
            "  Batch 1340/3208 | Loss: 1.2042\n",
            "  Batch 1350/3208 | Loss: 1.1857\n",
            "  Batch 1360/3208 | Loss: 1.4324\n",
            "  Batch 1370/3208 | Loss: 1.1313\n",
            "  Batch 1380/3208 | Loss: 0.9506\n",
            "  Batch 1390/3208 | Loss: 1.4110\n",
            "  Batch 1400/3208 | Loss: 1.0529\n",
            "  Batch 1410/3208 | Loss: 1.0341\n",
            "  Batch 1420/3208 | Loss: 0.8585\n",
            "  Batch 1430/3208 | Loss: 1.1259\n",
            "  Batch 1440/3208 | Loss: 1.1780\n",
            "  Batch 1450/3208 | Loss: 1.2542\n",
            "  Batch 1460/3208 | Loss: 0.8144\n",
            "  Batch 1470/3208 | Loss: 0.9826\n",
            "  Batch 1480/3208 | Loss: 1.8063\n",
            "  Batch 1490/3208 | Loss: 1.1462\n",
            "  Batch 1500/3208 | Loss: 1.1163\n",
            "  Batch 1510/3208 | Loss: 1.3007\n",
            "  Batch 1520/3208 | Loss: 0.7514\n",
            "  Batch 1530/3208 | Loss: 1.8598\n",
            "  Batch 1540/3208 | Loss: 1.1824\n",
            "  Batch 1550/3208 | Loss: 1.5128\n",
            "  Batch 1560/3208 | Loss: 0.8696\n",
            "  Batch 1570/3208 | Loss: 1.3557\n",
            "  Batch 1580/3208 | Loss: 1.1976\n",
            "  Batch 1590/3208 | Loss: 0.7275\n",
            "  Batch 1600/3208 | Loss: 1.6954\n",
            "  Batch 1610/3208 | Loss: 1.0255\n",
            "  Batch 1620/3208 | Loss: 0.9867\n",
            "  Batch 1630/3208 | Loss: 1.4942\n",
            "  Batch 1640/3208 | Loss: 1.7578\n",
            "  Batch 1650/3208 | Loss: 0.8814\n",
            "  Batch 1660/3208 | Loss: 0.7054\n",
            "  Batch 1670/3208 | Loss: 1.3946\n",
            "  Batch 1680/3208 | Loss: 0.9046\n",
            "  Batch 1690/3208 | Loss: 1.1882\n",
            "  Batch 1700/3208 | Loss: 0.6759\n",
            "  Batch 1710/3208 | Loss: 1.2774\n",
            "  Batch 1720/3208 | Loss: 0.9047\n",
            "  Batch 1730/3208 | Loss: 1.1198\n",
            "  Batch 1740/3208 | Loss: 0.9743\n",
            "  Batch 1750/3208 | Loss: 0.9725\n",
            "  Batch 1760/3208 | Loss: 1.3381\n",
            "  Batch 1770/3208 | Loss: 1.2251\n",
            "  Batch 1780/3208 | Loss: 0.7142\n",
            "  Batch 1790/3208 | Loss: 0.8003\n",
            "  Batch 1800/3208 | Loss: 1.3154\n",
            "  Batch 1810/3208 | Loss: 1.4626\n",
            "  Batch 1820/3208 | Loss: 1.1016\n",
            "  Batch 1830/3208 | Loss: 1.1769\n",
            "  Batch 1840/3208 | Loss: 1.1636\n",
            "  Batch 1850/3208 | Loss: 1.2902\n",
            "  Batch 1860/3208 | Loss: 1.8296\n",
            "  Batch 1870/3208 | Loss: 1.0064\n",
            "  Batch 1880/3208 | Loss: 0.9360\n",
            "  Batch 1890/3208 | Loss: 0.5562\n",
            "  Batch 1900/3208 | Loss: 1.4094\n",
            "  Batch 1910/3208 | Loss: 1.0573\n",
            "  Batch 1920/3208 | Loss: 1.4382\n",
            "  Batch 1930/3208 | Loss: 1.1673\n",
            "  Batch 1940/3208 | Loss: 1.4365\n",
            "  Batch 1950/3208 | Loss: 0.9457\n",
            "  Batch 1960/3208 | Loss: 1.2396\n",
            "  Batch 1970/3208 | Loss: 0.8103\n",
            "  Batch 1980/3208 | Loss: 1.8145\n",
            "  Batch 1990/3208 | Loss: 1.1202\n",
            "  Batch 2000/3208 | Loss: 0.9628\n",
            "  Batch 2010/3208 | Loss: 1.0352\n",
            "  Batch 2020/3208 | Loss: 1.0724\n",
            "  Batch 2030/3208 | Loss: 0.6736\n",
            "  Batch 2040/3208 | Loss: 0.8601\n",
            "  Batch 2050/3208 | Loss: 1.4353\n",
            "  Batch 2060/3208 | Loss: 1.4034\n",
            "  Batch 2070/3208 | Loss: 1.2615\n",
            "  Batch 2080/3208 | Loss: 1.0724\n",
            "  Batch 2090/3208 | Loss: 1.0195\n",
            "  Batch 2100/3208 | Loss: 1.4254\n",
            "  Batch 2110/3208 | Loss: 1.2233\n",
            "  Batch 2120/3208 | Loss: 0.7189\n",
            "  Batch 2130/3208 | Loss: 1.4863\n",
            "  Batch 2140/3208 | Loss: 1.5082\n",
            "  Batch 2150/3208 | Loss: 1.2459\n",
            "  Batch 2160/3208 | Loss: 1.1842\n",
            "  Batch 2170/3208 | Loss: 1.2945\n",
            "  Batch 2180/3208 | Loss: 1.0365\n",
            "  Batch 2190/3208 | Loss: 1.6410\n",
            "  Batch 2200/3208 | Loss: 1.0338\n",
            "  Batch 2210/3208 | Loss: 1.0389\n",
            "  Batch 2220/3208 | Loss: 0.8718\n",
            "  Batch 2230/3208 | Loss: 1.6954\n",
            "  Batch 2240/3208 | Loss: 1.7601\n",
            "  Batch 2250/3208 | Loss: 1.2188\n",
            "  Batch 2260/3208 | Loss: 1.4542\n",
            "  Batch 2270/3208 | Loss: 1.1268\n",
            "  Batch 2280/3208 | Loss: 0.6765\n",
            "  Batch 2290/3208 | Loss: 0.7521\n",
            "  Batch 2300/3208 | Loss: 1.0376\n",
            "  Batch 2310/3208 | Loss: 0.9329\n",
            "  Batch 2320/3208 | Loss: 0.7075\n",
            "  Batch 2330/3208 | Loss: 0.8981\n",
            "  Batch 2340/3208 | Loss: 0.9745\n",
            "  Batch 2350/3208 | Loss: 0.6196\n",
            "  Batch 2360/3208 | Loss: 0.8460\n",
            "  Batch 2370/3208 | Loss: 1.3897\n",
            "  Batch 2380/3208 | Loss: 0.9261\n",
            "  Batch 2390/3208 | Loss: 1.7715\n",
            "  Batch 2400/3208 | Loss: 1.0261\n",
            "  Batch 2410/3208 | Loss: 1.4366\n",
            "  Batch 2420/3208 | Loss: 1.2812\n",
            "  Batch 2430/3208 | Loss: 1.0521\n",
            "  Batch 2440/3208 | Loss: 0.7911\n",
            "  Batch 2450/3208 | Loss: 1.4755\n",
            "  Batch 2460/3208 | Loss: 1.0009\n",
            "  Batch 2470/3208 | Loss: 1.5804\n",
            "  Batch 2480/3208 | Loss: 1.1546\n",
            "  Batch 2490/3208 | Loss: 0.7409\n",
            "  Batch 2500/3208 | Loss: 1.7649\n",
            "  Batch 2510/3208 | Loss: 1.3589\n",
            "  Batch 2520/3208 | Loss: 1.3752\n",
            "  Batch 2530/3208 | Loss: 1.0801\n",
            "  Batch 2540/3208 | Loss: 1.4963\n",
            "  Batch 2550/3208 | Loss: 1.1141\n",
            "  Batch 2560/3208 | Loss: 1.5503\n",
            "  Batch 2570/3208 | Loss: 1.6783\n",
            "  Batch 2580/3208 | Loss: 0.8985\n",
            "  Batch 2590/3208 | Loss: 1.8516\n",
            "  Batch 2600/3208 | Loss: 1.1135\n",
            "  Batch 2610/3208 | Loss: 1.2605\n",
            "  Batch 2620/3208 | Loss: 0.7331\n",
            "  Batch 2630/3208 | Loss: 0.7657\n",
            "  Batch 2640/3208 | Loss: 1.3110\n",
            "  Batch 2650/3208 | Loss: 0.9521\n",
            "  Batch 2660/3208 | Loss: 1.1266\n",
            "  Batch 2670/3208 | Loss: 1.0132\n",
            "  Batch 2680/3208 | Loss: 1.0545\n",
            "  Batch 2690/3208 | Loss: 0.8650\n",
            "  Batch 2700/3208 | Loss: 1.1785\n",
            "  Batch 2710/3208 | Loss: 1.2567\n",
            "  Batch 2720/3208 | Loss: 1.2483\n",
            "  Batch 2730/3208 | Loss: 1.2560\n",
            "  Batch 2740/3208 | Loss: 1.0163\n",
            "  Batch 2750/3208 | Loss: 0.8164\n",
            "  Batch 2760/3208 | Loss: 1.2898\n",
            "  Batch 2770/3208 | Loss: 1.5453\n",
            "  Batch 2780/3208 | Loss: 1.0248\n",
            "  Batch 2790/3208 | Loss: 1.0258\n",
            "  Batch 2800/3208 | Loss: 1.3137\n",
            "  Batch 2810/3208 | Loss: 1.2913\n",
            "  Batch 2820/3208 | Loss: 0.9833\n",
            "  Batch 2830/3208 | Loss: 1.4100\n",
            "  Batch 2840/3208 | Loss: 1.3712\n",
            "  Batch 2850/3208 | Loss: 1.2053\n",
            "  Batch 2860/3208 | Loss: 0.6441\n",
            "  Batch 2870/3208 | Loss: 0.8717\n",
            "  Batch 2880/3208 | Loss: 1.2710\n",
            "  Batch 2890/3208 | Loss: 1.1615\n",
            "  Batch 2900/3208 | Loss: 1.2008\n",
            "  Batch 2910/3208 | Loss: 0.9258\n",
            "  Batch 2920/3208 | Loss: 1.3698\n",
            "  Batch 2930/3208 | Loss: 1.4002\n",
            "  Batch 2940/3208 | Loss: 1.1872\n",
            "  Batch 2950/3208 | Loss: 0.6540\n",
            "  Batch 2960/3208 | Loss: 1.5514\n",
            "  Batch 2970/3208 | Loss: 0.9842\n",
            "  Batch 2980/3208 | Loss: 0.9978\n",
            "  Batch 2990/3208 | Loss: 0.7083\n",
            "  Batch 3000/3208 | Loss: 1.1373\n",
            "  Batch 3010/3208 | Loss: 0.7189\n",
            "  Batch 3020/3208 | Loss: 0.5503\n",
            "  Batch 3030/3208 | Loss: 1.2986\n",
            "  Batch 3040/3208 | Loss: 1.2299\n",
            "  Batch 3050/3208 | Loss: 1.0866\n",
            "  Batch 3060/3208 | Loss: 0.9194\n",
            "  Batch 3070/3208 | Loss: 1.6096\n",
            "  Batch 3080/3208 | Loss: 1.1481\n",
            "  Batch 3090/3208 | Loss: 0.9315\n",
            "  Batch 3100/3208 | Loss: 1.0266\n",
            "  Batch 3110/3208 | Loss: 1.0874\n",
            "  Batch 3120/3208 | Loss: 0.7766\n",
            "  Batch 3130/3208 | Loss: 1.0187\n",
            "  Batch 3140/3208 | Loss: 1.2001\n",
            "  Batch 3150/3208 | Loss: 1.1139\n",
            "  Batch 3160/3208 | Loss: 0.5472\n",
            "  Batch 3170/3208 | Loss: 1.2100\n",
            "  Batch 3180/3208 | Loss: 1.0034\n",
            "  Batch 3190/3208 | Loss: 0.9976\n",
            "  Batch 3200/3208 | Loss: 0.9007\n",
            " Epoch 15 Completed | Avg Loss: 1.1522 | Training Accuracy: 66.16%\n",
            " Validation Accuracy: 69.48%\n",
            " Best Model Saved (Epoch 15 | Accuracy: 69.48%)\n",
            "\n",
            " Epoch 16/20\n",
            "  Batch 10/3208 | Loss: 1.1922\n",
            "  Batch 20/3208 | Loss: 1.0384\n",
            "  Batch 30/3208 | Loss: 1.0758\n",
            "  Batch 40/3208 | Loss: 1.4239\n",
            "  Batch 50/3208 | Loss: 1.0425\n",
            "  Batch 60/3208 | Loss: 0.9514\n",
            "  Batch 70/3208 | Loss: 1.5628\n",
            "  Batch 80/3208 | Loss: 1.3755\n",
            "  Batch 90/3208 | Loss: 1.4847\n",
            "  Batch 100/3208 | Loss: 1.4528\n",
            "  Batch 110/3208 | Loss: 1.1505\n",
            "  Batch 120/3208 | Loss: 1.2084\n",
            "  Batch 130/3208 | Loss: 1.2129\n",
            "  Batch 140/3208 | Loss: 1.0342\n",
            "  Batch 150/3208 | Loss: 1.3623\n",
            "  Batch 160/3208 | Loss: 1.0438\n",
            "  Batch 170/3208 | Loss: 0.9056\n",
            "  Batch 180/3208 | Loss: 0.9196\n",
            "  Batch 190/3208 | Loss: 2.5201\n",
            "  Batch 200/3208 | Loss: 0.9105\n",
            "  Batch 210/3208 | Loss: 1.0548\n",
            "  Batch 220/3208 | Loss: 1.0130\n",
            "  Batch 230/3208 | Loss: 0.7464\n",
            "  Batch 240/3208 | Loss: 1.0320\n",
            "  Batch 250/3208 | Loss: 1.2868\n",
            "  Batch 260/3208 | Loss: 0.8491\n",
            "  Batch 270/3208 | Loss: 0.9284\n",
            "  Batch 280/3208 | Loss: 1.3185\n",
            "  Batch 290/3208 | Loss: 0.9883\n",
            "  Batch 300/3208 | Loss: 1.1353\n",
            "  Batch 310/3208 | Loss: 1.7236\n",
            "  Batch 320/3208 | Loss: 1.0287\n",
            "  Batch 330/3208 | Loss: 1.0734\n",
            "  Batch 340/3208 | Loss: 0.8958\n",
            "  Batch 350/3208 | Loss: 1.7697\n",
            "  Batch 360/3208 | Loss: 1.4560\n",
            "  Batch 370/3208 | Loss: 1.1918\n",
            "  Batch 380/3208 | Loss: 1.1920\n",
            "  Batch 390/3208 | Loss: 1.4098\n",
            "  Batch 400/3208 | Loss: 0.8175\n",
            "  Batch 410/3208 | Loss: 1.3505\n",
            "  Batch 420/3208 | Loss: 0.8866\n",
            "  Batch 430/3208 | Loss: 0.9823\n",
            "  Batch 440/3208 | Loss: 1.2807\n",
            "  Batch 450/3208 | Loss: 1.0438\n",
            "  Batch 460/3208 | Loss: 0.9697\n",
            "  Batch 470/3208 | Loss: 1.1265\n",
            "  Batch 480/3208 | Loss: 0.6926\n",
            "  Batch 490/3208 | Loss: 0.8736\n",
            "  Batch 500/3208 | Loss: 0.7414\n",
            "  Batch 510/3208 | Loss: 1.1856\n",
            "  Batch 520/3208 | Loss: 0.9913\n",
            "  Batch 530/3208 | Loss: 0.9275\n",
            "  Batch 540/3208 | Loss: 1.3941\n",
            "  Batch 550/3208 | Loss: 0.8888\n",
            "  Batch 560/3208 | Loss: 1.2462\n",
            "  Batch 570/3208 | Loss: 1.4413\n",
            "  Batch 580/3208 | Loss: 1.3283\n",
            "  Batch 590/3208 | Loss: 0.7735\n",
            "  Batch 600/3208 | Loss: 1.5481\n",
            "  Batch 610/3208 | Loss: 0.9600\n",
            "  Batch 620/3208 | Loss: 0.8505\n",
            "  Batch 630/3208 | Loss: 1.3998\n",
            "  Batch 640/3208 | Loss: 1.2519\n",
            "  Batch 650/3208 | Loss: 1.0253\n",
            "  Batch 660/3208 | Loss: 1.4237\n",
            "  Batch 670/3208 | Loss: 0.8903\n",
            "  Batch 680/3208 | Loss: 1.1112\n",
            "  Batch 690/3208 | Loss: 1.3180\n",
            "  Batch 700/3208 | Loss: 1.0175\n",
            "  Batch 710/3208 | Loss: 1.0998\n",
            "  Batch 720/3208 | Loss: 1.0486\n",
            "  Batch 730/3208 | Loss: 1.1894\n",
            "  Batch 740/3208 | Loss: 0.9096\n",
            "  Batch 750/3208 | Loss: 0.8354\n",
            "  Batch 760/3208 | Loss: 0.9927\n",
            "  Batch 770/3208 | Loss: 0.9863\n",
            "  Batch 780/3208 | Loss: 1.0540\n",
            "  Batch 790/3208 | Loss: 1.6700\n",
            "  Batch 800/3208 | Loss: 1.0518\n",
            "  Batch 810/3208 | Loss: 1.0649\n",
            "  Batch 820/3208 | Loss: 0.8655\n",
            "  Batch 830/3208 | Loss: 1.0391\n",
            "  Batch 840/3208 | Loss: 0.9657\n",
            "  Batch 850/3208 | Loss: 0.9071\n",
            "  Batch 860/3208 | Loss: 0.8052\n",
            "  Batch 870/3208 | Loss: 1.1833\n",
            "  Batch 880/3208 | Loss: 1.1528\n",
            "  Batch 890/3208 | Loss: 1.1461\n",
            "  Batch 900/3208 | Loss: 0.9638\n",
            "  Batch 910/3208 | Loss: 1.2757\n",
            "  Batch 920/3208 | Loss: 1.3690\n",
            "  Batch 930/3208 | Loss: 1.2001\n",
            "  Batch 940/3208 | Loss: 0.6697\n",
            "  Batch 950/3208 | Loss: 1.2949\n",
            "  Batch 960/3208 | Loss: 1.0655\n",
            "  Batch 970/3208 | Loss: 0.8104\n",
            "  Batch 980/3208 | Loss: 1.0481\n",
            "  Batch 990/3208 | Loss: 1.3446\n",
            "  Batch 1000/3208 | Loss: 0.9440\n",
            "  Batch 1010/3208 | Loss: 1.1028\n",
            "  Batch 1020/3208 | Loss: 0.7001\n",
            "  Batch 1030/3208 | Loss: 1.2743\n",
            "  Batch 1040/3208 | Loss: 1.0283\n",
            "  Batch 1050/3208 | Loss: 1.4111\n",
            "  Batch 1060/3208 | Loss: 1.2949\n",
            "  Batch 1070/3208 | Loss: 1.0579\n",
            "  Batch 1080/3208 | Loss: 0.8659\n",
            "  Batch 1090/3208 | Loss: 1.1977\n",
            "  Batch 1100/3208 | Loss: 1.1632\n",
            "  Batch 1110/3208 | Loss: 0.9693\n",
            "  Batch 1120/3208 | Loss: 1.3143\n",
            "  Batch 1130/3208 | Loss: 1.2364\n",
            "  Batch 1140/3208 | Loss: 0.6830\n",
            "  Batch 1150/3208 | Loss: 1.7722\n",
            "  Batch 1160/3208 | Loss: 1.2902\n",
            "  Batch 1170/3208 | Loss: 1.0405\n",
            "  Batch 1180/3208 | Loss: 0.9191\n",
            "  Batch 1190/3208 | Loss: 0.9426\n",
            "  Batch 1200/3208 | Loss: 1.1378\n",
            "  Batch 1210/3208 | Loss: 1.3555\n",
            "  Batch 1220/3208 | Loss: 0.5916\n",
            "  Batch 1230/3208 | Loss: 1.3975\n",
            "  Batch 1240/3208 | Loss: 1.0319\n",
            "  Batch 1250/3208 | Loss: 1.1061\n",
            "  Batch 1260/3208 | Loss: 0.6006\n",
            "  Batch 1270/3208 | Loss: 0.7800\n",
            "  Batch 1280/3208 | Loss: 2.1278\n",
            "  Batch 1290/3208 | Loss: 0.9243\n",
            "  Batch 1300/3208 | Loss: 1.0305\n",
            "  Batch 1310/3208 | Loss: 1.2696\n",
            "  Batch 1320/3208 | Loss: 0.7764\n",
            "  Batch 1330/3208 | Loss: 0.8490\n",
            "  Batch 1340/3208 | Loss: 1.2901\n",
            "  Batch 1350/3208 | Loss: 1.5809\n",
            "  Batch 1360/3208 | Loss: 1.3632\n",
            "  Batch 1370/3208 | Loss: 0.7157\n",
            "  Batch 1380/3208 | Loss: 1.0131\n",
            "  Batch 1390/3208 | Loss: 1.2475\n",
            "  Batch 1400/3208 | Loss: 1.0655\n",
            "  Batch 1410/3208 | Loss: 1.5820\n",
            "  Batch 1420/3208 | Loss: 1.5829\n",
            "  Batch 1430/3208 | Loss: 0.7717\n",
            "  Batch 1440/3208 | Loss: 1.0664\n",
            "  Batch 1450/3208 | Loss: 1.3203\n",
            "  Batch 1460/3208 | Loss: 0.9799\n",
            "  Batch 1470/3208 | Loss: 0.7899\n",
            "  Batch 1480/3208 | Loss: 1.4278\n",
            "  Batch 1490/3208 | Loss: 1.7793\n",
            "  Batch 1500/3208 | Loss: 0.6442\n",
            "  Batch 1510/3208 | Loss: 0.9513\n",
            "  Batch 1520/3208 | Loss: 1.0453\n",
            "  Batch 1530/3208 | Loss: 1.8609\n",
            "  Batch 1540/3208 | Loss: 0.8989\n",
            "  Batch 1550/3208 | Loss: 0.6452\n",
            "  Batch 1560/3208 | Loss: 1.1137\n",
            "  Batch 1570/3208 | Loss: 0.9543\n",
            "  Batch 1580/3208 | Loss: 0.9239\n",
            "  Batch 1590/3208 | Loss: 1.0134\n",
            "  Batch 1600/3208 | Loss: 1.2830\n",
            "  Batch 1610/3208 | Loss: 0.9028\n",
            "  Batch 1620/3208 | Loss: 0.5808\n",
            "  Batch 1630/3208 | Loss: 0.6192\n",
            "  Batch 1640/3208 | Loss: 1.1322\n",
            "  Batch 1650/3208 | Loss: 1.6765\n",
            "  Batch 1660/3208 | Loss: 1.0235\n",
            "  Batch 1670/3208 | Loss: 0.9388\n",
            "  Batch 1680/3208 | Loss: 0.6759\n",
            "  Batch 1690/3208 | Loss: 0.9000\n",
            "  Batch 1700/3208 | Loss: 1.1350\n",
            "  Batch 1710/3208 | Loss: 1.0212\n",
            "  Batch 1720/3208 | Loss: 1.1855\n",
            "  Batch 1730/3208 | Loss: 1.1328\n",
            "  Batch 1740/3208 | Loss: 1.5882\n",
            "  Batch 1750/3208 | Loss: 0.4269\n",
            "  Batch 1760/3208 | Loss: 1.1215\n",
            "  Batch 1770/3208 | Loss: 1.1279\n",
            "  Batch 1780/3208 | Loss: 1.4184\n",
            "  Batch 1790/3208 | Loss: 1.3567\n",
            "  Batch 1800/3208 | Loss: 0.8979\n",
            "  Batch 1810/3208 | Loss: 1.2177\n",
            "  Batch 1820/3208 | Loss: 0.7894\n",
            "  Batch 1830/3208 | Loss: 1.4588\n",
            "  Batch 1840/3208 | Loss: 1.0464\n",
            "  Batch 1850/3208 | Loss: 0.8934\n",
            "  Batch 1860/3208 | Loss: 1.0912\n",
            "  Batch 1870/3208 | Loss: 1.3823\n",
            "  Batch 1880/3208 | Loss: 1.2027\n",
            "  Batch 1890/3208 | Loss: 1.2090\n",
            "  Batch 1900/3208 | Loss: 1.4768\n",
            "  Batch 1910/3208 | Loss: 1.0644\n",
            "  Batch 1920/3208 | Loss: 1.3164\n",
            "  Batch 1930/3208 | Loss: 1.3139\n",
            "  Batch 1940/3208 | Loss: 1.5440\n",
            "  Batch 1950/3208 | Loss: 0.9206\n",
            "  Batch 1960/3208 | Loss: 0.9369\n",
            "  Batch 1970/3208 | Loss: 1.5047\n",
            "  Batch 1980/3208 | Loss: 0.7214\n",
            "  Batch 1990/3208 | Loss: 1.3940\n",
            "  Batch 2000/3208 | Loss: 0.8836\n",
            "  Batch 2010/3208 | Loss: 1.6951\n",
            "  Batch 2020/3208 | Loss: 1.0920\n",
            "  Batch 2030/3208 | Loss: 1.2983\n",
            "  Batch 2040/3208 | Loss: 0.9063\n",
            "  Batch 2050/3208 | Loss: 0.6694\n",
            "  Batch 2060/3208 | Loss: 1.2846\n",
            "  Batch 2070/3208 | Loss: 1.2249\n",
            "  Batch 2080/3208 | Loss: 1.0960\n",
            "  Batch 2090/3208 | Loss: 0.9768\n",
            "  Batch 2100/3208 | Loss: 1.0749\n",
            "  Batch 2110/3208 | Loss: 0.7091\n",
            "  Batch 2120/3208 | Loss: 1.1256\n",
            "  Batch 2130/3208 | Loss: 1.3410\n",
            "  Batch 2140/3208 | Loss: 1.0754\n",
            "  Batch 2150/3208 | Loss: 0.9383\n",
            "  Batch 2160/3208 | Loss: 1.2806\n",
            "  Batch 2170/3208 | Loss: 0.9312\n",
            "  Batch 2180/3208 | Loss: 1.3675\n",
            "  Batch 2190/3208 | Loss: 0.9483\n",
            "  Batch 2200/3208 | Loss: 1.4911\n",
            "  Batch 2210/3208 | Loss: 1.2163\n",
            "  Batch 2220/3208 | Loss: 0.5201\n",
            "  Batch 2230/3208 | Loss: 0.9788\n",
            "  Batch 2240/3208 | Loss: 0.8982\n",
            "  Batch 2250/3208 | Loss: 1.0945\n",
            "  Batch 2260/3208 | Loss: 1.0150\n",
            "  Batch 2270/3208 | Loss: 0.9650\n",
            "  Batch 2280/3208 | Loss: 1.1520\n",
            "  Batch 2290/3208 | Loss: 1.3773\n",
            "  Batch 2300/3208 | Loss: 1.2400\n",
            "  Batch 2310/3208 | Loss: 1.0567\n",
            "  Batch 2320/3208 | Loss: 1.4584\n",
            "  Batch 2330/3208 | Loss: 1.5631\n",
            "  Batch 2340/3208 | Loss: 0.9345\n",
            "  Batch 2350/3208 | Loss: 1.8112\n",
            "  Batch 2360/3208 | Loss: 0.9168\n",
            "  Batch 2370/3208 | Loss: 0.7739\n",
            "  Batch 2380/3208 | Loss: 0.8587\n",
            "  Batch 2390/3208 | Loss: 0.6370\n",
            "  Batch 2400/3208 | Loss: 1.1482\n",
            "  Batch 2410/3208 | Loss: 1.1086\n",
            "  Batch 2420/3208 | Loss: 0.9222\n",
            "  Batch 2430/3208 | Loss: 0.8064\n",
            "  Batch 2440/3208 | Loss: 0.8359\n",
            "  Batch 2450/3208 | Loss: 1.1288\n",
            "  Batch 2460/3208 | Loss: 1.4411\n",
            "  Batch 2470/3208 | Loss: 0.8054\n",
            "  Batch 2480/3208 | Loss: 0.7730\n",
            "  Batch 2490/3208 | Loss: 1.3973\n",
            "  Batch 2500/3208 | Loss: 0.7424\n",
            "  Batch 2510/3208 | Loss: 0.9601\n",
            "  Batch 2520/3208 | Loss: 1.1071\n",
            "  Batch 2530/3208 | Loss: 1.2946\n",
            "  Batch 2540/3208 | Loss: 1.1822\n",
            "  Batch 2550/3208 | Loss: 1.1304\n",
            "  Batch 2560/3208 | Loss: 1.4544\n",
            "  Batch 2570/3208 | Loss: 1.2694\n",
            "  Batch 2580/3208 | Loss: 1.3475\n",
            "  Batch 2590/3208 | Loss: 1.3464\n",
            "  Batch 2600/3208 | Loss: 1.2886\n",
            "  Batch 2610/3208 | Loss: 0.3935\n",
            "  Batch 2620/3208 | Loss: 1.5553\n",
            "  Batch 2630/3208 | Loss: 0.8161\n",
            "  Batch 2640/3208 | Loss: 1.4700\n",
            "  Batch 2650/3208 | Loss: 1.1859\n",
            "  Batch 2660/3208 | Loss: 0.6197\n",
            "  Batch 2670/3208 | Loss: 1.7138\n",
            "  Batch 2680/3208 | Loss: 1.0714\n",
            "  Batch 2690/3208 | Loss: 1.5071\n",
            "  Batch 2700/3208 | Loss: 1.6770\n",
            "  Batch 2710/3208 | Loss: 1.3750\n",
            "  Batch 2720/3208 | Loss: 0.6151\n",
            "  Batch 2730/3208 | Loss: 2.1421\n",
            "  Batch 2740/3208 | Loss: 1.3486\n",
            "  Batch 2750/3208 | Loss: 1.0275\n",
            "  Batch 2760/3208 | Loss: 1.3856\n",
            "  Batch 2770/3208 | Loss: 1.5665\n",
            "  Batch 2780/3208 | Loss: 0.9096\n",
            "  Batch 2790/3208 | Loss: 0.9393\n",
            "  Batch 2800/3208 | Loss: 0.8597\n",
            "  Batch 2810/3208 | Loss: 1.3092\n",
            "  Batch 2820/3208 | Loss: 0.8827\n",
            "  Batch 2830/3208 | Loss: 1.4977\n",
            "  Batch 2840/3208 | Loss: 1.0448\n",
            "  Batch 2850/3208 | Loss: 1.0098\n",
            "  Batch 2860/3208 | Loss: 0.9754\n",
            "  Batch 2870/3208 | Loss: 0.6621\n",
            "  Batch 2880/3208 | Loss: 0.6423\n",
            "  Batch 2890/3208 | Loss: 1.2819\n",
            "  Batch 2900/3208 | Loss: 1.0579\n",
            "  Batch 2910/3208 | Loss: 1.1490\n",
            "  Batch 2920/3208 | Loss: 1.2884\n",
            "  Batch 2930/3208 | Loss: 1.2651\n",
            "  Batch 2940/3208 | Loss: 0.9808\n",
            "  Batch 2950/3208 | Loss: 1.3084\n",
            "  Batch 2960/3208 | Loss: 0.9124\n",
            "  Batch 2970/3208 | Loss: 1.0258\n",
            "  Batch 2980/3208 | Loss: 0.5741\n",
            "  Batch 2990/3208 | Loss: 1.2998\n",
            "  Batch 3000/3208 | Loss: 0.8349\n",
            "  Batch 3010/3208 | Loss: 0.7294\n",
            "  Batch 3020/3208 | Loss: 1.8792\n",
            "  Batch 3030/3208 | Loss: 0.9216\n",
            "  Batch 3040/3208 | Loss: 0.9333\n",
            "  Batch 3050/3208 | Loss: 0.8303\n",
            "  Batch 3060/3208 | Loss: 1.2628\n",
            "  Batch 3070/3208 | Loss: 1.2785\n",
            "  Batch 3080/3208 | Loss: 1.2755\n",
            "  Batch 3090/3208 | Loss: 0.8647\n",
            "  Batch 3100/3208 | Loss: 1.1267\n",
            "  Batch 3110/3208 | Loss: 0.6020\n",
            "  Batch 3120/3208 | Loss: 1.1414\n",
            "  Batch 3130/3208 | Loss: 1.4070\n",
            "  Batch 3140/3208 | Loss: 1.4494\n",
            "  Batch 3150/3208 | Loss: 0.7592\n",
            "  Batch 3160/3208 | Loss: 0.8152\n",
            "  Batch 3170/3208 | Loss: 1.0057\n",
            "  Batch 3180/3208 | Loss: 1.0188\n",
            "  Batch 3190/3208 | Loss: 0.9485\n",
            "  Batch 3200/3208 | Loss: 1.5490\n",
            " Epoch 16 Completed | Avg Loss: 1.1120 | Training Accuracy: 67.36%\n",
            " Validation Accuracy: 71.67%\n",
            " Best Model Saved (Epoch 16 | Accuracy: 71.67%)\n",
            "\n",
            " Epoch 17/20\n",
            "  Batch 10/3208 | Loss: 1.0532\n",
            "  Batch 20/3208 | Loss: 0.7687\n",
            "  Batch 30/3208 | Loss: 0.6926\n",
            "  Batch 40/3208 | Loss: 0.9818\n",
            "  Batch 50/3208 | Loss: 0.9103\n",
            "  Batch 60/3208 | Loss: 0.6840\n",
            "  Batch 70/3208 | Loss: 1.0774\n",
            "  Batch 80/3208 | Loss: 1.2740\n",
            "  Batch 90/3208 | Loss: 1.2180\n",
            "  Batch 100/3208 | Loss: 0.9834\n",
            "  Batch 110/3208 | Loss: 1.0401\n",
            "  Batch 120/3208 | Loss: 1.9187\n",
            "  Batch 130/3208 | Loss: 1.0921\n",
            "  Batch 140/3208 | Loss: 1.1004\n",
            "  Batch 150/3208 | Loss: 1.0648\n",
            "  Batch 160/3208 | Loss: 1.8479\n",
            "  Batch 170/3208 | Loss: 1.6182\n",
            "  Batch 180/3208 | Loss: 0.9730\n",
            "  Batch 190/3208 | Loss: 1.1792\n",
            "  Batch 200/3208 | Loss: 1.0744\n",
            "  Batch 210/3208 | Loss: 0.9961\n",
            "  Batch 220/3208 | Loss: 0.8143\n",
            "  Batch 230/3208 | Loss: 1.0022\n",
            "  Batch 240/3208 | Loss: 0.7590\n",
            "  Batch 250/3208 | Loss: 1.4084\n",
            "  Batch 260/3208 | Loss: 0.9102\n",
            "  Batch 270/3208 | Loss: 0.7528\n",
            "  Batch 280/3208 | Loss: 1.2553\n",
            "  Batch 290/3208 | Loss: 1.4044\n",
            "  Batch 300/3208 | Loss: 1.2459\n",
            "  Batch 310/3208 | Loss: 1.0195\n",
            "  Batch 320/3208 | Loss: 1.1088\n",
            "  Batch 330/3208 | Loss: 0.8673\n",
            "  Batch 340/3208 | Loss: 1.1340\n",
            "  Batch 350/3208 | Loss: 1.1049\n",
            "  Batch 360/3208 | Loss: 1.0266\n",
            "  Batch 370/3208 | Loss: 1.8490\n",
            "  Batch 380/3208 | Loss: 0.9555\n",
            "  Batch 390/3208 | Loss: 0.8292\n",
            "  Batch 400/3208 | Loss: 1.1429\n",
            "  Batch 410/3208 | Loss: 0.8650\n",
            "  Batch 420/3208 | Loss: 1.7532\n",
            "  Batch 430/3208 | Loss: 0.7644\n",
            "  Batch 440/3208 | Loss: 0.8328\n",
            "  Batch 450/3208 | Loss: 1.4090\n",
            "  Batch 460/3208 | Loss: 1.2028\n",
            "  Batch 470/3208 | Loss: 1.3091\n",
            "  Batch 480/3208 | Loss: 0.6924\n",
            "  Batch 490/3208 | Loss: 0.8371\n",
            "  Batch 500/3208 | Loss: 1.0619\n",
            "  Batch 510/3208 | Loss: 1.1640\n",
            "  Batch 520/3208 | Loss: 1.4542\n",
            "  Batch 530/3208 | Loss: 0.8330\n",
            "  Batch 540/3208 | Loss: 1.2581\n",
            "  Batch 550/3208 | Loss: 0.7573\n",
            "  Batch 560/3208 | Loss: 0.8762\n",
            "  Batch 570/3208 | Loss: 0.8524\n",
            "  Batch 580/3208 | Loss: 1.1859\n",
            "  Batch 590/3208 | Loss: 0.9414\n",
            "  Batch 600/3208 | Loss: 1.0144\n",
            "  Batch 610/3208 | Loss: 0.7720\n",
            "  Batch 620/3208 | Loss: 0.6136\n",
            "  Batch 630/3208 | Loss: 1.5063\n",
            "  Batch 640/3208 | Loss: 0.9069\n",
            "  Batch 650/3208 | Loss: 0.7408\n",
            "  Batch 660/3208 | Loss: 1.3304\n",
            "  Batch 670/3208 | Loss: 0.9295\n",
            "  Batch 680/3208 | Loss: 1.0940\n",
            "  Batch 690/3208 | Loss: 1.3597\n",
            "  Batch 700/3208 | Loss: 1.1656\n",
            "  Batch 710/3208 | Loss: 2.0019\n",
            "  Batch 720/3208 | Loss: 1.1009\n",
            "  Batch 730/3208 | Loss: 0.7335\n",
            "  Batch 740/3208 | Loss: 1.2866\n",
            "  Batch 750/3208 | Loss: 1.1087\n",
            "  Batch 760/3208 | Loss: 1.6076\n",
            "  Batch 770/3208 | Loss: 0.6606\n",
            "  Batch 780/3208 | Loss: 0.7703\n",
            "  Batch 790/3208 | Loss: 1.1693\n",
            "  Batch 800/3208 | Loss: 0.9787\n",
            "  Batch 810/3208 | Loss: 0.7251\n",
            "  Batch 820/3208 | Loss: 1.1006\n",
            "  Batch 830/3208 | Loss: 1.3767\n",
            "  Batch 840/3208 | Loss: 0.3195\n",
            "  Batch 850/3208 | Loss: 1.2249\n",
            "  Batch 860/3208 | Loss: 0.9691\n",
            "  Batch 870/3208 | Loss: 1.0242\n",
            "  Batch 880/3208 | Loss: 1.4961\n",
            "  Batch 890/3208 | Loss: 0.8786\n",
            "  Batch 900/3208 | Loss: 1.1503\n",
            "  Batch 910/3208 | Loss: 0.6888\n",
            "  Batch 920/3208 | Loss: 1.6836\n",
            "  Batch 930/3208 | Loss: 1.0614\n",
            "  Batch 940/3208 | Loss: 1.1199\n",
            "  Batch 950/3208 | Loss: 0.9331\n",
            "  Batch 960/3208 | Loss: 0.6653\n",
            "  Batch 970/3208 | Loss: 1.3703\n",
            "  Batch 980/3208 | Loss: 1.4352\n",
            "  Batch 990/3208 | Loss: 0.9369\n",
            "  Batch 1000/3208 | Loss: 1.0884\n",
            "  Batch 1010/3208 | Loss: 0.9741\n",
            "  Batch 1020/3208 | Loss: 1.1360\n",
            "  Batch 1030/3208 | Loss: 1.1130\n",
            "  Batch 1040/3208 | Loss: 1.4929\n",
            "  Batch 1050/3208 | Loss: 1.7386\n",
            "  Batch 1060/3208 | Loss: 0.7356\n",
            "  Batch 1070/3208 | Loss: 1.3582\n",
            "  Batch 1080/3208 | Loss: 1.0207\n",
            "  Batch 1090/3208 | Loss: 1.5536\n",
            "  Batch 1100/3208 | Loss: 1.7701\n",
            "  Batch 1110/3208 | Loss: 1.2240\n",
            "  Batch 1120/3208 | Loss: 1.1140\n",
            "  Batch 1130/3208 | Loss: 1.4571\n",
            "  Batch 1140/3208 | Loss: 1.0274\n",
            "  Batch 1150/3208 | Loss: 0.7589\n",
            "  Batch 1160/3208 | Loss: 1.1007\n",
            "  Batch 1170/3208 | Loss: 0.7232\n",
            "  Batch 1180/3208 | Loss: 0.8626\n",
            "  Batch 1190/3208 | Loss: 1.1429\n",
            "  Batch 1200/3208 | Loss: 0.7206\n",
            "  Batch 1210/3208 | Loss: 1.2871\n",
            "  Batch 1220/3208 | Loss: 1.0257\n",
            "  Batch 1230/3208 | Loss: 0.8776\n",
            "  Batch 1240/3208 | Loss: 1.2284\n",
            "  Batch 1250/3208 | Loss: 0.7408\n",
            "  Batch 1260/3208 | Loss: 1.3790\n",
            "  Batch 1270/3208 | Loss: 1.3979\n",
            "  Batch 1280/3208 | Loss: 1.2817\n",
            "  Batch 1290/3208 | Loss: 0.9524\n",
            "  Batch 1300/3208 | Loss: 0.8401\n",
            "  Batch 1310/3208 | Loss: 1.0613\n",
            "  Batch 1320/3208 | Loss: 1.1551\n",
            "  Batch 1330/3208 | Loss: 0.7290\n",
            "  Batch 1340/3208 | Loss: 1.7486\n",
            "  Batch 1350/3208 | Loss: 0.9628\n",
            "  Batch 1360/3208 | Loss: 1.0204\n",
            "  Batch 1370/3208 | Loss: 1.1056\n",
            "  Batch 1380/3208 | Loss: 1.1468\n",
            "  Batch 1390/3208 | Loss: 1.0940\n",
            "  Batch 1400/3208 | Loss: 0.8378\n",
            "  Batch 1410/3208 | Loss: 1.1999\n",
            "  Batch 1420/3208 | Loss: 1.1597\n",
            "  Batch 1430/3208 | Loss: 0.5884\n",
            "  Batch 1440/3208 | Loss: 0.8315\n",
            "  Batch 1450/3208 | Loss: 1.0284\n",
            "  Batch 1460/3208 | Loss: 1.1730\n",
            "  Batch 1470/3208 | Loss: 0.6179\n",
            "  Batch 1480/3208 | Loss: 0.6275\n",
            "  Batch 1490/3208 | Loss: 1.0532\n",
            "  Batch 1500/3208 | Loss: 1.0428\n",
            "  Batch 1510/3208 | Loss: 1.3385\n",
            "  Batch 1520/3208 | Loss: 0.5445\n",
            "  Batch 1530/3208 | Loss: 1.2044\n",
            "  Batch 1540/3208 | Loss: 0.8538\n",
            "  Batch 1550/3208 | Loss: 0.9283\n",
            "  Batch 1560/3208 | Loss: 1.3393\n",
            "  Batch 1570/3208 | Loss: 0.9099\n",
            "  Batch 1580/3208 | Loss: 1.4896\n",
            "  Batch 1590/3208 | Loss: 1.1682\n",
            "  Batch 1600/3208 | Loss: 1.0941\n",
            "  Batch 1610/3208 | Loss: 0.8941\n",
            "  Batch 1620/3208 | Loss: 0.8253\n",
            "  Batch 1630/3208 | Loss: 1.0408\n",
            "  Batch 1640/3208 | Loss: 0.7144\n",
            "  Batch 1650/3208 | Loss: 1.6227\n",
            "  Batch 1660/3208 | Loss: 1.0607\n",
            "  Batch 1670/3208 | Loss: 1.5815\n",
            "  Batch 1680/3208 | Loss: 1.3324\n",
            "  Batch 1690/3208 | Loss: 0.6858\n",
            "  Batch 1700/3208 | Loss: 0.4439\n",
            "  Batch 1710/3208 | Loss: 0.8389\n",
            "  Batch 1720/3208 | Loss: 0.9735\n",
            "  Batch 1730/3208 | Loss: 1.0546\n",
            "  Batch 1740/3208 | Loss: 0.7665\n",
            "  Batch 1750/3208 | Loss: 1.5299\n",
            "  Batch 1760/3208 | Loss: 1.8089\n",
            "  Batch 1770/3208 | Loss: 0.8988\n",
            "  Batch 1780/3208 | Loss: 1.0578\n",
            "  Batch 1790/3208 | Loss: 0.9056\n",
            "  Batch 1800/3208 | Loss: 0.8419\n",
            "  Batch 1810/3208 | Loss: 0.5799\n",
            "  Batch 1820/3208 | Loss: 0.8947\n",
            "  Batch 1830/3208 | Loss: 0.8984\n",
            "  Batch 1840/3208 | Loss: 0.8358\n",
            "  Batch 1850/3208 | Loss: 0.9616\n",
            "  Batch 1860/3208 | Loss: 1.2843\n",
            "  Batch 1870/3208 | Loss: 0.6907\n",
            "  Batch 1880/3208 | Loss: 1.8645\n",
            "  Batch 1890/3208 | Loss: 0.7419\n",
            "  Batch 1900/3208 | Loss: 1.0672\n",
            "  Batch 1910/3208 | Loss: 0.8331\n",
            "  Batch 1920/3208 | Loss: 0.9943\n",
            "  Batch 1930/3208 | Loss: 0.9621\n",
            "  Batch 1940/3208 | Loss: 1.7711\n",
            "  Batch 1950/3208 | Loss: 1.0410\n",
            "  Batch 1960/3208 | Loss: 0.8990\n",
            "  Batch 1970/3208 | Loss: 1.2074\n",
            "  Batch 1980/3208 | Loss: 0.9375\n",
            "  Batch 1990/3208 | Loss: 1.2729\n",
            "  Batch 2000/3208 | Loss: 0.9826\n",
            "  Batch 2010/3208 | Loss: 1.6207\n",
            "  Batch 2020/3208 | Loss: 1.2891\n",
            "  Batch 2030/3208 | Loss: 1.1335\n",
            "  Batch 2040/3208 | Loss: 1.1824\n",
            "  Batch 2050/3208 | Loss: 1.5207\n",
            "  Batch 2060/3208 | Loss: 1.3393\n",
            "  Batch 2070/3208 | Loss: 0.7477\n",
            "  Batch 2080/3208 | Loss: 1.5933\n",
            "  Batch 2090/3208 | Loss: 1.1218\n",
            "  Batch 2100/3208 | Loss: 1.2019\n",
            "  Batch 2110/3208 | Loss: 1.1053\n",
            "  Batch 2120/3208 | Loss: 1.1003\n",
            "  Batch 2130/3208 | Loss: 0.7978\n",
            "  Batch 2140/3208 | Loss: 0.8221\n",
            "  Batch 2150/3208 | Loss: 1.2082\n",
            "  Batch 2160/3208 | Loss: 0.7090\n",
            "  Batch 2170/3208 | Loss: 1.6974\n",
            "  Batch 2180/3208 | Loss: 1.0976\n",
            "  Batch 2190/3208 | Loss: 0.9906\n",
            "  Batch 2200/3208 | Loss: 1.0186\n",
            "  Batch 2210/3208 | Loss: 0.9566\n",
            "  Batch 2220/3208 | Loss: 0.9353\n",
            "  Batch 2230/3208 | Loss: 0.8368\n",
            "  Batch 2240/3208 | Loss: 1.0924\n",
            "  Batch 2250/3208 | Loss: 1.2203\n",
            "  Batch 2260/3208 | Loss: 1.0811\n",
            "  Batch 2270/3208 | Loss: 1.3188\n",
            "  Batch 2280/3208 | Loss: 0.7836\n",
            "  Batch 2290/3208 | Loss: 1.1209\n",
            "  Batch 2300/3208 | Loss: 1.4146\n",
            "  Batch 2310/3208 | Loss: 1.1571\n",
            "  Batch 2320/3208 | Loss: 1.2206\n",
            "  Batch 2330/3208 | Loss: 1.2750\n",
            "  Batch 2340/3208 | Loss: 1.3837\n",
            "  Batch 2350/3208 | Loss: 0.9688\n",
            "  Batch 2360/3208 | Loss: 1.4890\n",
            "  Batch 2370/3208 | Loss: 0.9043\n",
            "  Batch 2380/3208 | Loss: 0.8502\n",
            "  Batch 2390/3208 | Loss: 1.1381\n",
            "  Batch 2400/3208 | Loss: 1.6006\n",
            "  Batch 2410/3208 | Loss: 0.7327\n",
            "  Batch 2420/3208 | Loss: 1.3095\n",
            "  Batch 2430/3208 | Loss: 1.2668\n",
            "  Batch 2440/3208 | Loss: 0.9681\n",
            "  Batch 2450/3208 | Loss: 1.0272\n",
            "  Batch 2460/3208 | Loss: 1.0531\n",
            "  Batch 2470/3208 | Loss: 0.9654\n",
            "  Batch 2480/3208 | Loss: 1.2973\n",
            "  Batch 2490/3208 | Loss: 1.4711\n",
            "  Batch 2500/3208 | Loss: 0.7947\n",
            "  Batch 2510/3208 | Loss: 1.2185\n",
            "  Batch 2520/3208 | Loss: 0.8037\n",
            "  Batch 2530/3208 | Loss: 1.3374\n",
            "  Batch 2540/3208 | Loss: 1.0107\n",
            "  Batch 2550/3208 | Loss: 1.0594\n",
            "  Batch 2560/3208 | Loss: 1.0729\n",
            "  Batch 2570/3208 | Loss: 1.0781\n",
            "  Batch 2580/3208 | Loss: 0.7811\n",
            "  Batch 2590/3208 | Loss: 1.0747\n",
            "  Batch 2600/3208 | Loss: 1.0762\n",
            "  Batch 2610/3208 | Loss: 0.9921\n",
            "  Batch 2620/3208 | Loss: 0.6328\n",
            "  Batch 2630/3208 | Loss: 0.7774\n",
            "  Batch 2640/3208 | Loss: 1.3455\n",
            "  Batch 2650/3208 | Loss: 0.9608\n",
            "  Batch 2660/3208 | Loss: 1.0905\n",
            "  Batch 2670/3208 | Loss: 1.4920\n",
            "  Batch 2680/3208 | Loss: 1.0930\n",
            "  Batch 2690/3208 | Loss: 0.5847\n",
            "  Batch 2700/3208 | Loss: 0.9709\n",
            "  Batch 2710/3208 | Loss: 0.9422\n",
            "  Batch 2720/3208 | Loss: 1.0263\n",
            "  Batch 2730/3208 | Loss: 0.9638\n",
            "  Batch 2740/3208 | Loss: 0.8922\n",
            "  Batch 2750/3208 | Loss: 1.6957\n",
            "  Batch 2760/3208 | Loss: 0.9942\n",
            "  Batch 2770/3208 | Loss: 1.0951\n",
            "  Batch 2780/3208 | Loss: 1.5755\n",
            "  Batch 2790/3208 | Loss: 0.9897\n",
            "  Batch 2800/3208 | Loss: 1.1939\n",
            "  Batch 2810/3208 | Loss: 0.9945\n",
            "  Batch 2820/3208 | Loss: 1.1718\n",
            "  Batch 2830/3208 | Loss: 0.7485\n",
            "  Batch 2840/3208 | Loss: 0.8302\n",
            "  Batch 2850/3208 | Loss: 1.4308\n",
            "  Batch 2860/3208 | Loss: 1.0815\n",
            "  Batch 2870/3208 | Loss: 1.3646\n",
            "  Batch 2880/3208 | Loss: 0.6803\n",
            "  Batch 2890/3208 | Loss: 0.8529\n",
            "  Batch 2900/3208 | Loss: 1.1297\n",
            "  Batch 2910/3208 | Loss: 0.4138\n",
            "  Batch 2920/3208 | Loss: 0.7983\n",
            "  Batch 2930/3208 | Loss: 0.7430\n",
            "  Batch 2940/3208 | Loss: 1.3056\n",
            "  Batch 2950/3208 | Loss: 1.0445\n",
            "  Batch 2960/3208 | Loss: 0.8606\n",
            "  Batch 2970/3208 | Loss: 1.2244\n",
            "  Batch 2980/3208 | Loss: 1.0910\n",
            "  Batch 2990/3208 | Loss: 0.8904\n",
            "  Batch 3000/3208 | Loss: 0.8524\n",
            "  Batch 3010/3208 | Loss: 1.1883\n",
            "  Batch 3020/3208 | Loss: 0.7408\n",
            "  Batch 3030/3208 | Loss: 0.9511\n",
            "  Batch 3040/3208 | Loss: 0.5307\n",
            "  Batch 3050/3208 | Loss: 0.9933\n",
            "  Batch 3060/3208 | Loss: 0.9706\n",
            "  Batch 3070/3208 | Loss: 0.7904\n",
            "  Batch 3080/3208 | Loss: 1.0266\n",
            "  Batch 3090/3208 | Loss: 1.0936\n",
            "  Batch 3100/3208 | Loss: 1.0308\n",
            "  Batch 3110/3208 | Loss: 1.1401\n",
            "  Batch 3120/3208 | Loss: 1.0502\n",
            "  Batch 3130/3208 | Loss: 1.0137\n",
            "  Batch 3140/3208 | Loss: 1.1035\n",
            "  Batch 3150/3208 | Loss: 1.7161\n",
            "  Batch 3160/3208 | Loss: 0.6742\n",
            "  Batch 3170/3208 | Loss: 1.0275\n",
            "  Batch 3180/3208 | Loss: 0.8145\n",
            "  Batch 3190/3208 | Loss: 1.1556\n",
            "  Batch 3200/3208 | Loss: 1.1025\n",
            " Epoch 17 Completed | Avg Loss: 1.0786 | Training Accuracy: 67.82%\n",
            " Validation Accuracy: 71.74%\n",
            " Best Model Saved (Epoch 17 | Accuracy: 71.74%)\n",
            "\n",
            " Epoch 18/20\n",
            "  Batch 10/3208 | Loss: 0.5717\n",
            "  Batch 20/3208 | Loss: 0.7200\n",
            "  Batch 30/3208 | Loss: 1.4451\n",
            "  Batch 40/3208 | Loss: 0.6246\n",
            "  Batch 50/3208 | Loss: 1.2065\n",
            "  Batch 60/3208 | Loss: 1.0788\n",
            "  Batch 70/3208 | Loss: 1.0894\n",
            "  Batch 80/3208 | Loss: 0.7885\n",
            "  Batch 90/3208 | Loss: 0.9341\n",
            "  Batch 100/3208 | Loss: 1.2013\n",
            "  Batch 110/3208 | Loss: 1.0755\n",
            "  Batch 120/3208 | Loss: 0.8125\n",
            "  Batch 130/3208 | Loss: 0.7867\n",
            "  Batch 140/3208 | Loss: 0.7956\n",
            "  Batch 150/3208 | Loss: 1.0538\n",
            "  Batch 160/3208 | Loss: 1.0600\n",
            "  Batch 170/3208 | Loss: 1.0071\n",
            "  Batch 180/3208 | Loss: 0.6357\n",
            "  Batch 190/3208 | Loss: 0.7537\n",
            "  Batch 200/3208 | Loss: 0.8241\n",
            "  Batch 210/3208 | Loss: 1.0022\n",
            "  Batch 220/3208 | Loss: 1.2111\n",
            "  Batch 230/3208 | Loss: 1.1230\n",
            "  Batch 240/3208 | Loss: 0.7302\n",
            "  Batch 250/3208 | Loss: 0.9033\n",
            "  Batch 260/3208 | Loss: 0.7579\n",
            "  Batch 270/3208 | Loss: 1.0739\n",
            "  Batch 280/3208 | Loss: 1.2539\n",
            "  Batch 290/3208 | Loss: 1.4631\n",
            "  Batch 300/3208 | Loss: 0.9445\n",
            "  Batch 310/3208 | Loss: 1.0176\n",
            "  Batch 320/3208 | Loss: 1.4876\n",
            "  Batch 330/3208 | Loss: 1.6884\n",
            "  Batch 340/3208 | Loss: 1.2175\n",
            "  Batch 350/3208 | Loss: 1.5264\n",
            "  Batch 360/3208 | Loss: 0.6181\n",
            "  Batch 370/3208 | Loss: 1.1282\n",
            "  Batch 380/3208 | Loss: 0.8830\n",
            "  Batch 390/3208 | Loss: 0.6166\n",
            "  Batch 400/3208 | Loss: 0.7021\n",
            "  Batch 410/3208 | Loss: 0.9030\n",
            "  Batch 420/3208 | Loss: 1.2668\n",
            "  Batch 430/3208 | Loss: 0.6182\n",
            "  Batch 440/3208 | Loss: 0.9799\n",
            "  Batch 450/3208 | Loss: 1.0341\n",
            "  Batch 460/3208 | Loss: 1.1680\n",
            "  Batch 470/3208 | Loss: 0.9396\n",
            "  Batch 480/3208 | Loss: 1.2143\n",
            "  Batch 490/3208 | Loss: 1.2125\n",
            "  Batch 500/3208 | Loss: 1.0495\n",
            "  Batch 510/3208 | Loss: 1.4992\n",
            "  Batch 520/3208 | Loss: 1.0923\n",
            "  Batch 530/3208 | Loss: 0.7962\n",
            "  Batch 540/3208 | Loss: 1.1356\n",
            "  Batch 550/3208 | Loss: 0.7301\n",
            "  Batch 560/3208 | Loss: 1.0261\n",
            "  Batch 570/3208 | Loss: 0.7603\n",
            "  Batch 580/3208 | Loss: 1.2212\n",
            "  Batch 590/3208 | Loss: 1.4824\n",
            "  Batch 600/3208 | Loss: 1.3178\n",
            "  Batch 610/3208 | Loss: 1.3151\n",
            "  Batch 620/3208 | Loss: 1.6628\n",
            "  Batch 630/3208 | Loss: 1.0868\n",
            "  Batch 640/3208 | Loss: 0.8500\n",
            "  Batch 650/3208 | Loss: 1.4496\n",
            "  Batch 660/3208 | Loss: 1.0743\n",
            "  Batch 670/3208 | Loss: 0.8576\n",
            "  Batch 680/3208 | Loss: 1.0252\n",
            "  Batch 690/3208 | Loss: 1.0334\n",
            "  Batch 700/3208 | Loss: 1.2329\n",
            "  Batch 710/3208 | Loss: 0.7120\n",
            "  Batch 720/3208 | Loss: 0.8812\n",
            "  Batch 730/3208 | Loss: 0.7577\n",
            "  Batch 740/3208 | Loss: 1.9055\n",
            "  Batch 750/3208 | Loss: 1.2493\n",
            "  Batch 760/3208 | Loss: 0.6392\n",
            "  Batch 770/3208 | Loss: 1.3891\n",
            "  Batch 780/3208 | Loss: 0.7323\n",
            "  Batch 790/3208 | Loss: 1.1212\n",
            "  Batch 800/3208 | Loss: 1.2559\n",
            "  Batch 810/3208 | Loss: 1.4561\n",
            "  Batch 820/3208 | Loss: 0.9078\n",
            "  Batch 830/3208 | Loss: 1.3580\n",
            "  Batch 840/3208 | Loss: 1.0055\n",
            "  Batch 850/3208 | Loss: 1.0848\n",
            "  Batch 860/3208 | Loss: 1.0235\n",
            "  Batch 870/3208 | Loss: 1.3511\n",
            "  Batch 880/3208 | Loss: 1.4321\n",
            "  Batch 890/3208 | Loss: 1.0366\n",
            "  Batch 900/3208 | Loss: 0.8407\n",
            "  Batch 910/3208 | Loss: 0.9025\n",
            "  Batch 920/3208 | Loss: 1.0008\n",
            "  Batch 930/3208 | Loss: 0.8165\n",
            "  Batch 940/3208 | Loss: 0.7820\n",
            "  Batch 950/3208 | Loss: 1.2743\n",
            "  Batch 960/3208 | Loss: 0.7966\n",
            "  Batch 970/3208 | Loss: 1.0397\n",
            "  Batch 980/3208 | Loss: 0.7937\n",
            "  Batch 990/3208 | Loss: 0.6990\n",
            "  Batch 1000/3208 | Loss: 1.4280\n",
            "  Batch 1010/3208 | Loss: 0.6549\n",
            "  Batch 1020/3208 | Loss: 1.0869\n",
            "  Batch 1030/3208 | Loss: 1.4533\n",
            "  Batch 1040/3208 | Loss: 0.6041\n",
            "  Batch 1050/3208 | Loss: 1.2166\n",
            "  Batch 1060/3208 | Loss: 0.9605\n",
            "  Batch 1070/3208 | Loss: 1.4471\n",
            "  Batch 1080/3208 | Loss: 1.2050\n",
            "  Batch 1090/3208 | Loss: 1.0962\n",
            "  Batch 1100/3208 | Loss: 1.1754\n",
            "  Batch 1110/3208 | Loss: 0.8404\n",
            "  Batch 1120/3208 | Loss: 1.0327\n",
            "  Batch 1130/3208 | Loss: 1.1718\n",
            "  Batch 1140/3208 | Loss: 0.8349\n",
            "  Batch 1150/3208 | Loss: 0.6476\n",
            "  Batch 1160/3208 | Loss: 0.7528\n",
            "  Batch 1170/3208 | Loss: 1.0120\n",
            "  Batch 1180/3208 | Loss: 1.1352\n",
            "  Batch 1190/3208 | Loss: 0.9761\n",
            "  Batch 1200/3208 | Loss: 0.7025\n",
            "  Batch 1210/3208 | Loss: 1.0647\n",
            "  Batch 1220/3208 | Loss: 1.5501\n",
            "  Batch 1230/3208 | Loss: 1.4131\n",
            "  Batch 1240/3208 | Loss: 1.0337\n",
            "  Batch 1250/3208 | Loss: 0.8735\n",
            "  Batch 1260/3208 | Loss: 1.2186\n",
            "  Batch 1270/3208 | Loss: 1.0806\n",
            "  Batch 1280/3208 | Loss: 1.1783\n",
            "  Batch 1290/3208 | Loss: 0.8782\n",
            "  Batch 1300/3208 | Loss: 1.5923\n",
            "  Batch 1310/3208 | Loss: 0.9781\n",
            "  Batch 1320/3208 | Loss: 0.6515\n",
            "  Batch 1330/3208 | Loss: 1.0185\n",
            "  Batch 1340/3208 | Loss: 0.8056\n",
            "  Batch 1350/3208 | Loss: 1.7576\n",
            "  Batch 1360/3208 | Loss: 1.4435\n",
            "  Batch 1370/3208 | Loss: 0.9657\n",
            "  Batch 1380/3208 | Loss: 1.5024\n",
            "  Batch 1390/3208 | Loss: 0.9066\n",
            "  Batch 1400/3208 | Loss: 0.7402\n",
            "  Batch 1410/3208 | Loss: 0.9825\n",
            "  Batch 1420/3208 | Loss: 1.1419\n",
            "  Batch 1430/3208 | Loss: 0.8339\n",
            "  Batch 1440/3208 | Loss: 1.2891\n",
            "  Batch 1450/3208 | Loss: 0.8423\n",
            "  Batch 1460/3208 | Loss: 1.1077\n",
            "  Batch 1470/3208 | Loss: 1.3376\n",
            "  Batch 1480/3208 | Loss: 0.8285\n",
            "  Batch 1490/3208 | Loss: 0.4899\n",
            "  Batch 1500/3208 | Loss: 0.7336\n",
            "  Batch 1510/3208 | Loss: 1.1373\n",
            "  Batch 1520/3208 | Loss: 0.8295\n",
            "  Batch 1530/3208 | Loss: 1.2627\n",
            "  Batch 1540/3208 | Loss: 0.7794\n",
            "  Batch 1550/3208 | Loss: 1.1462\n",
            "  Batch 1560/3208 | Loss: 0.7699\n",
            "  Batch 1570/3208 | Loss: 0.9100\n",
            "  Batch 1580/3208 | Loss: 1.5116\n",
            "  Batch 1590/3208 | Loss: 0.7431\n",
            "  Batch 1600/3208 | Loss: 1.1974\n",
            "  Batch 1610/3208 | Loss: 1.2149\n",
            "  Batch 1620/3208 | Loss: 1.4516\n",
            "  Batch 1630/3208 | Loss: 1.0480\n",
            "  Batch 1640/3208 | Loss: 1.0896\n",
            "  Batch 1650/3208 | Loss: 0.8820\n",
            "  Batch 1660/3208 | Loss: 0.8986\n",
            "  Batch 1670/3208 | Loss: 1.2625\n",
            "  Batch 1680/3208 | Loss: 1.2099\n",
            "  Batch 1690/3208 | Loss: 1.1637\n",
            "  Batch 1700/3208 | Loss: 0.5843\n",
            "  Batch 1710/3208 | Loss: 1.2708\n",
            "  Batch 1720/3208 | Loss: 0.4586\n",
            "  Batch 1730/3208 | Loss: 0.8311\n",
            "  Batch 1740/3208 | Loss: 1.3113\n",
            "  Batch 1750/3208 | Loss: 0.4972\n",
            "  Batch 1760/3208 | Loss: 1.3308\n",
            "  Batch 1770/3208 | Loss: 0.9433\n",
            "  Batch 1780/3208 | Loss: 0.8089\n",
            "  Batch 1790/3208 | Loss: 1.3382\n",
            "  Batch 1800/3208 | Loss: 1.5184\n",
            "  Batch 1810/3208 | Loss: 1.3012\n",
            "  Batch 1820/3208 | Loss: 0.8838\n",
            "  Batch 1830/3208 | Loss: 1.1052\n",
            "  Batch 1840/3208 | Loss: 0.7946\n",
            "  Batch 1850/3208 | Loss: 0.9519\n",
            "  Batch 1860/3208 | Loss: 1.5648\n",
            "  Batch 1870/3208 | Loss: 0.8797\n",
            "  Batch 1880/3208 | Loss: 1.2084\n",
            "  Batch 1890/3208 | Loss: 0.7381\n",
            "  Batch 1900/3208 | Loss: 0.7788\n",
            "  Batch 1910/3208 | Loss: 0.8855\n",
            "  Batch 1920/3208 | Loss: 0.7882\n",
            "  Batch 1930/3208 | Loss: 1.3254\n",
            "  Batch 1940/3208 | Loss: 1.1758\n",
            "  Batch 1950/3208 | Loss: 1.3676\n",
            "  Batch 1960/3208 | Loss: 1.3427\n",
            "  Batch 1970/3208 | Loss: 0.9559\n",
            "  Batch 1980/3208 | Loss: 1.5291\n",
            "  Batch 1990/3208 | Loss: 0.9927\n",
            "  Batch 2000/3208 | Loss: 1.1885\n",
            "  Batch 2010/3208 | Loss: 0.7497\n",
            "  Batch 2020/3208 | Loss: 0.9020\n",
            "  Batch 2030/3208 | Loss: 1.1212\n",
            "  Batch 2040/3208 | Loss: 0.9337\n",
            "  Batch 2050/3208 | Loss: 1.4806\n",
            "  Batch 2060/3208 | Loss: 1.0788\n",
            "  Batch 2070/3208 | Loss: 1.1685\n",
            "  Batch 2080/3208 | Loss: 0.8536\n",
            "  Batch 2090/3208 | Loss: 1.1690\n",
            "  Batch 2100/3208 | Loss: 0.8895\n",
            "  Batch 2110/3208 | Loss: 1.5157\n",
            "  Batch 2120/3208 | Loss: 1.0194\n",
            "  Batch 2130/3208 | Loss: 1.6039\n",
            "  Batch 2140/3208 | Loss: 1.1338\n",
            "  Batch 2150/3208 | Loss: 0.6437\n",
            "  Batch 2160/3208 | Loss: 0.7232\n",
            "  Batch 2170/3208 | Loss: 0.9611\n",
            "  Batch 2180/3208 | Loss: 1.1682\n",
            "  Batch 2190/3208 | Loss: 1.1622\n",
            "  Batch 2200/3208 | Loss: 0.9724\n",
            "  Batch 2210/3208 | Loss: 0.7347\n",
            "  Batch 2220/3208 | Loss: 0.9833\n",
            "  Batch 2230/3208 | Loss: 0.9751\n",
            "  Batch 2240/3208 | Loss: 0.9271\n",
            "  Batch 2250/3208 | Loss: 0.8371\n",
            "  Batch 2260/3208 | Loss: 1.7238\n",
            "  Batch 2270/3208 | Loss: 0.4348\n",
            "  Batch 2280/3208 | Loss: 1.3111\n",
            "  Batch 2290/3208 | Loss: 1.1053\n",
            "  Batch 2300/3208 | Loss: 0.7867\n",
            "  Batch 2310/3208 | Loss: 0.6961\n",
            "  Batch 2320/3208 | Loss: 0.8582\n",
            "  Batch 2330/3208 | Loss: 0.7240\n",
            "  Batch 2340/3208 | Loss: 0.9470\n",
            "  Batch 2350/3208 | Loss: 1.0015\n",
            "  Batch 2360/3208 | Loss: 1.1503\n",
            "  Batch 2370/3208 | Loss: 0.4944\n",
            "  Batch 2380/3208 | Loss: 0.9321\n",
            "  Batch 2390/3208 | Loss: 1.1849\n",
            "  Batch 2400/3208 | Loss: 0.9787\n",
            "  Batch 2410/3208 | Loss: 0.7866\n",
            "  Batch 2420/3208 | Loss: 1.4428\n",
            "  Batch 2430/3208 | Loss: 1.1619\n",
            "  Batch 2440/3208 | Loss: 0.7705\n",
            "  Batch 2450/3208 | Loss: 0.8946\n",
            "  Batch 2460/3208 | Loss: 0.8622\n",
            "  Batch 2470/3208 | Loss: 1.0611\n",
            "  Batch 2480/3208 | Loss: 1.1027\n",
            "  Batch 2490/3208 | Loss: 0.9082\n",
            "  Batch 2500/3208 | Loss: 0.9586\n",
            "  Batch 2510/3208 | Loss: 1.1378\n",
            "  Batch 2520/3208 | Loss: 1.3053\n",
            "  Batch 2530/3208 | Loss: 0.6130\n",
            "  Batch 2540/3208 | Loss: 1.2209\n",
            "  Batch 2550/3208 | Loss: 1.6825\n",
            "  Batch 2560/3208 | Loss: 1.4506\n",
            "  Batch 2570/3208 | Loss: 0.9993\n",
            "  Batch 2580/3208 | Loss: 1.3701\n",
            "  Batch 2590/3208 | Loss: 0.8148\n",
            "  Batch 2600/3208 | Loss: 1.0134\n",
            "  Batch 2610/3208 | Loss: 0.9095\n",
            "  Batch 2620/3208 | Loss: 0.8029\n",
            "  Batch 2630/3208 | Loss: 1.1982\n",
            "  Batch 2640/3208 | Loss: 0.8758\n",
            "  Batch 2650/3208 | Loss: 1.3361\n",
            "  Batch 2660/3208 | Loss: 1.0975\n",
            "  Batch 2670/3208 | Loss: 0.8561\n",
            "  Batch 2680/3208 | Loss: 1.4835\n",
            "  Batch 2690/3208 | Loss: 0.7971\n",
            "  Batch 2700/3208 | Loss: 1.3834\n",
            "  Batch 2710/3208 | Loss: 1.0482\n",
            "  Batch 2720/3208 | Loss: 0.7098\n",
            "  Batch 2730/3208 | Loss: 0.9622\n",
            "  Batch 2740/3208 | Loss: 0.8680\n",
            "  Batch 2750/3208 | Loss: 1.2281\n",
            "  Batch 2760/3208 | Loss: 0.5000\n",
            "  Batch 2770/3208 | Loss: 0.9056\n",
            "  Batch 2780/3208 | Loss: 1.2754\n",
            "  Batch 2790/3208 | Loss: 1.6716\n",
            "  Batch 2800/3208 | Loss: 1.0909\n",
            "  Batch 2810/3208 | Loss: 0.7568\n",
            "  Batch 2820/3208 | Loss: 1.3763\n",
            "  Batch 2830/3208 | Loss: 1.2760\n",
            "  Batch 2840/3208 | Loss: 1.5105\n",
            "  Batch 2850/3208 | Loss: 1.6896\n",
            "  Batch 2860/3208 | Loss: 1.3923\n",
            "  Batch 2870/3208 | Loss: 1.4137\n",
            "  Batch 2880/3208 | Loss: 1.2730\n",
            "  Batch 2890/3208 | Loss: 1.0377\n",
            "  Batch 2900/3208 | Loss: 1.2832\n",
            "  Batch 2910/3208 | Loss: 0.9588\n",
            "  Batch 2920/3208 | Loss: 0.9638\n",
            "  Batch 2930/3208 | Loss: 0.5880\n",
            "  Batch 2940/3208 | Loss: 1.2164\n",
            "  Batch 2950/3208 | Loss: 0.7363\n",
            "  Batch 2960/3208 | Loss: 0.9193\n",
            "  Batch 2970/3208 | Loss: 1.0456\n",
            "  Batch 2980/3208 | Loss: 0.9508\n",
            "  Batch 2990/3208 | Loss: 1.2485\n",
            "  Batch 3000/3208 | Loss: 0.4799\n",
            "  Batch 3010/3208 | Loss: 1.3219\n",
            "  Batch 3020/3208 | Loss: 0.7649\n",
            "  Batch 3030/3208 | Loss: 1.0047\n",
            "  Batch 3040/3208 | Loss: 0.9107\n",
            "  Batch 3050/3208 | Loss: 1.2336\n",
            "  Batch 3060/3208 | Loss: 1.4788\n",
            "  Batch 3070/3208 | Loss: 1.2267\n",
            "  Batch 3080/3208 | Loss: 0.5268\n",
            "  Batch 3090/3208 | Loss: 1.0934\n",
            "  Batch 3100/3208 | Loss: 2.2051\n",
            "  Batch 3110/3208 | Loss: 1.2490\n",
            "  Batch 3120/3208 | Loss: 0.8286\n",
            "  Batch 3130/3208 | Loss: 1.5778\n",
            "  Batch 3140/3208 | Loss: 0.5561\n",
            "  Batch 3150/3208 | Loss: 0.9553\n",
            "  Batch 3160/3208 | Loss: 1.0161\n",
            "  Batch 3170/3208 | Loss: 1.2506\n",
            "  Batch 3180/3208 | Loss: 1.3185\n",
            "  Batch 3190/3208 | Loss: 0.5157\n",
            "  Batch 3200/3208 | Loss: 0.9484\n",
            " Epoch 18 Completed | Avg Loss: 1.0464 | Training Accuracy: 68.93%\n",
            " Validation Accuracy: 72.46%\n",
            " Best Model Saved (Epoch 18 | Accuracy: 72.46%)\n",
            "\n",
            " Epoch 19/20\n",
            "  Batch 10/3208 | Loss: 0.6371\n",
            "  Batch 20/3208 | Loss: 0.7697\n",
            "  Batch 30/3208 | Loss: 0.8724\n",
            "  Batch 40/3208 | Loss: 0.9022\n",
            "  Batch 50/3208 | Loss: 0.5023\n",
            "  Batch 60/3208 | Loss: 0.6708\n",
            "  Batch 70/3208 | Loss: 1.0577\n",
            "  Batch 80/3208 | Loss: 0.7573\n",
            "  Batch 90/3208 | Loss: 0.7405\n",
            "  Batch 100/3208 | Loss: 1.3465\n",
            "  Batch 110/3208 | Loss: 1.5659\n",
            "  Batch 120/3208 | Loss: 1.1645\n",
            "  Batch 130/3208 | Loss: 1.3563\n",
            "  Batch 140/3208 | Loss: 1.1455\n",
            "  Batch 150/3208 | Loss: 0.6449\n",
            "  Batch 160/3208 | Loss: 0.7746\n",
            "  Batch 170/3208 | Loss: 1.2477\n",
            "  Batch 180/3208 | Loss: 1.1291\n",
            "  Batch 190/3208 | Loss: 0.8749\n",
            "  Batch 200/3208 | Loss: 0.9863\n",
            "  Batch 210/3208 | Loss: 0.8989\n",
            "  Batch 220/3208 | Loss: 0.8481\n",
            "  Batch 230/3208 | Loss: 1.6398\n",
            "  Batch 240/3208 | Loss: 0.8150\n",
            "  Batch 250/3208 | Loss: 1.0067\n",
            "  Batch 260/3208 | Loss: 1.2969\n",
            "  Batch 270/3208 | Loss: 1.3297\n",
            "  Batch 280/3208 | Loss: 1.2248\n",
            "  Batch 290/3208 | Loss: 1.9262\n",
            "  Batch 300/3208 | Loss: 1.0143\n",
            "  Batch 310/3208 | Loss: 0.8514\n",
            "  Batch 320/3208 | Loss: 1.4773\n",
            "  Batch 330/3208 | Loss: 1.2822\n",
            "  Batch 340/3208 | Loss: 0.9388\n",
            "  Batch 350/3208 | Loss: 0.9673\n",
            "  Batch 360/3208 | Loss: 1.3895\n",
            "  Batch 370/3208 | Loss: 0.7207\n",
            "  Batch 380/3208 | Loss: 0.9924\n",
            "  Batch 390/3208 | Loss: 1.2487\n",
            "  Batch 400/3208 | Loss: 0.6848\n",
            "  Batch 410/3208 | Loss: 0.6943\n",
            "  Batch 420/3208 | Loss: 1.2021\n",
            "  Batch 430/3208 | Loss: 0.4353\n",
            "  Batch 440/3208 | Loss: 0.7858\n",
            "  Batch 450/3208 | Loss: 0.8927\n",
            "  Batch 460/3208 | Loss: 0.7413\n",
            "  Batch 470/3208 | Loss: 0.9236\n",
            "  Batch 480/3208 | Loss: 0.9512\n",
            "  Batch 490/3208 | Loss: 0.8023\n",
            "  Batch 500/3208 | Loss: 0.8550\n",
            "  Batch 510/3208 | Loss: 1.0732\n",
            "  Batch 520/3208 | Loss: 0.8101\n",
            "  Batch 530/3208 | Loss: 0.8627\n",
            "  Batch 540/3208 | Loss: 1.3526\n",
            "  Batch 550/3208 | Loss: 0.7519\n",
            "  Batch 560/3208 | Loss: 1.1770\n",
            "  Batch 570/3208 | Loss: 1.0574\n",
            "  Batch 580/3208 | Loss: 1.2430\n",
            "  Batch 590/3208 | Loss: 1.0379\n",
            "  Batch 600/3208 | Loss: 0.7476\n",
            "  Batch 610/3208 | Loss: 0.9680\n",
            "  Batch 620/3208 | Loss: 0.9298\n",
            "  Batch 630/3208 | Loss: 1.3977\n",
            "  Batch 640/3208 | Loss: 1.1354\n",
            "  Batch 650/3208 | Loss: 0.9554\n",
            "  Batch 660/3208 | Loss: 0.9406\n",
            "  Batch 670/3208 | Loss: 1.0003\n",
            "  Batch 680/3208 | Loss: 1.3028\n",
            "  Batch 690/3208 | Loss: 0.5884\n",
            "  Batch 700/3208 | Loss: 1.1941\n",
            "  Batch 710/3208 | Loss: 0.8778\n",
            "  Batch 720/3208 | Loss: 1.1840\n",
            "  Batch 730/3208 | Loss: 1.0771\n",
            "  Batch 740/3208 | Loss: 1.1115\n",
            "  Batch 750/3208 | Loss: 0.7197\n",
            "  Batch 760/3208 | Loss: 0.9292\n",
            "  Batch 770/3208 | Loss: 0.8206\n",
            "  Batch 780/3208 | Loss: 0.9756\n",
            "  Batch 790/3208 | Loss: 1.4184\n",
            "  Batch 800/3208 | Loss: 1.0888\n",
            "  Batch 810/3208 | Loss: 1.0540\n",
            "  Batch 820/3208 | Loss: 0.8618\n",
            "  Batch 830/3208 | Loss: 0.8758\n",
            "  Batch 840/3208 | Loss: 0.9855\n",
            "  Batch 850/3208 | Loss: 0.9887\n",
            "  Batch 860/3208 | Loss: 0.5783\n",
            "  Batch 870/3208 | Loss: 0.9758\n",
            "  Batch 880/3208 | Loss: 0.9933\n",
            "  Batch 890/3208 | Loss: 0.9336\n",
            "  Batch 900/3208 | Loss: 1.0893\n",
            "  Batch 910/3208 | Loss: 1.1610\n",
            "  Batch 920/3208 | Loss: 0.8267\n",
            "  Batch 930/3208 | Loss: 0.7857\n",
            "  Batch 940/3208 | Loss: 0.5825\n",
            "  Batch 950/3208 | Loss: 1.2660\n",
            "  Batch 960/3208 | Loss: 1.3942\n",
            "  Batch 970/3208 | Loss: 0.8682\n",
            "  Batch 980/3208 | Loss: 0.6760\n",
            "  Batch 990/3208 | Loss: 1.2702\n",
            "  Batch 1000/3208 | Loss: 0.9160\n",
            "  Batch 1010/3208 | Loss: 1.3836\n",
            "  Batch 1020/3208 | Loss: 1.0039\n",
            "  Batch 1030/3208 | Loss: 1.6484\n",
            "  Batch 1040/3208 | Loss: 1.3328\n",
            "  Batch 1050/3208 | Loss: 1.5905\n",
            "  Batch 1060/3208 | Loss: 1.3004\n",
            "  Batch 1070/3208 | Loss: 1.2051\n",
            "  Batch 1080/3208 | Loss: 0.6477\n",
            "  Batch 1090/3208 | Loss: 1.1675\n",
            "  Batch 1100/3208 | Loss: 1.1014\n",
            "  Batch 1110/3208 | Loss: 0.9696\n",
            "  Batch 1120/3208 | Loss: 1.0197\n",
            "  Batch 1130/3208 | Loss: 0.5080\n",
            "  Batch 1140/3208 | Loss: 0.7673\n",
            "  Batch 1150/3208 | Loss: 1.0747\n",
            "  Batch 1160/3208 | Loss: 1.2537\n",
            "  Batch 1170/3208 | Loss: 0.9290\n",
            "  Batch 1180/3208 | Loss: 1.2991\n",
            "  Batch 1190/3208 | Loss: 0.9818\n",
            "  Batch 1200/3208 | Loss: 0.7712\n",
            "  Batch 1210/3208 | Loss: 1.2150\n",
            "  Batch 1220/3208 | Loss: 1.2262\n",
            "  Batch 1230/3208 | Loss: 1.0125\n",
            "  Batch 1240/3208 | Loss: 0.5728\n",
            "  Batch 1250/3208 | Loss: 1.1846\n",
            "  Batch 1260/3208 | Loss: 1.3548\n",
            "  Batch 1270/3208 | Loss: 1.1688\n",
            "  Batch 1280/3208 | Loss: 0.5874\n",
            "  Batch 1290/3208 | Loss: 0.8198\n",
            "  Batch 1300/3208 | Loss: 1.0236\n",
            "  Batch 1310/3208 | Loss: 0.9343\n",
            "  Batch 1320/3208 | Loss: 1.0305\n",
            "  Batch 1330/3208 | Loss: 1.4160\n",
            "  Batch 1340/3208 | Loss: 0.9555\n",
            "  Batch 1350/3208 | Loss: 0.5715\n",
            "  Batch 1360/3208 | Loss: 1.0561\n",
            "  Batch 1370/3208 | Loss: 0.8906\n",
            "  Batch 1380/3208 | Loss: 0.7909\n",
            "  Batch 1390/3208 | Loss: 1.1370\n",
            "  Batch 1400/3208 | Loss: 0.7226\n",
            "  Batch 1410/3208 | Loss: 0.8275\n",
            "  Batch 1420/3208 | Loss: 1.2958\n",
            "  Batch 1430/3208 | Loss: 0.9734\n",
            "  Batch 1440/3208 | Loss: 0.5660\n",
            "  Batch 1450/3208 | Loss: 0.5852\n",
            "  Batch 1460/3208 | Loss: 1.1576\n",
            "  Batch 1470/3208 | Loss: 1.1530\n",
            "  Batch 1480/3208 | Loss: 0.7376\n",
            "  Batch 1490/3208 | Loss: 0.7268\n",
            "  Batch 1500/3208 | Loss: 1.6383\n",
            "  Batch 1510/3208 | Loss: 0.7181\n",
            "  Batch 1520/3208 | Loss: 1.0842\n",
            "  Batch 1530/3208 | Loss: 0.7150\n",
            "  Batch 1540/3208 | Loss: 0.5665\n",
            "  Batch 1550/3208 | Loss: 0.8853\n",
            "  Batch 1560/3208 | Loss: 0.9705\n",
            "  Batch 1570/3208 | Loss: 0.5837\n",
            "  Batch 1580/3208 | Loss: 1.3085\n",
            "  Batch 1590/3208 | Loss: 1.2126\n",
            "  Batch 1600/3208 | Loss: 1.0356\n",
            "  Batch 1610/3208 | Loss: 0.7127\n",
            "  Batch 1620/3208 | Loss: 0.9585\n",
            "  Batch 1630/3208 | Loss: 1.7874\n",
            "  Batch 1640/3208 | Loss: 1.1209\n",
            "  Batch 1650/3208 | Loss: 0.7062\n",
            "  Batch 1660/3208 | Loss: 1.7360\n",
            "  Batch 1670/3208 | Loss: 1.7694\n",
            "  Batch 1680/3208 | Loss: 1.1025\n",
            "  Batch 1690/3208 | Loss: 0.7774\n",
            "  Batch 1700/3208 | Loss: 0.8043\n",
            "  Batch 1710/3208 | Loss: 1.2734\n",
            "  Batch 1720/3208 | Loss: 0.5632\n",
            "  Batch 1730/3208 | Loss: 1.1175\n",
            "  Batch 1740/3208 | Loss: 0.9919\n",
            "  Batch 1750/3208 | Loss: 0.9339\n",
            "  Batch 1760/3208 | Loss: 1.4267\n",
            "  Batch 1770/3208 | Loss: 0.5952\n",
            "  Batch 1780/3208 | Loss: 0.7972\n",
            "  Batch 1790/3208 | Loss: 0.6910\n",
            "  Batch 1800/3208 | Loss: 1.1236\n",
            "  Batch 1810/3208 | Loss: 1.1342\n",
            "  Batch 1820/3208 | Loss: 1.0283\n",
            "  Batch 1830/3208 | Loss: 1.3127\n",
            "  Batch 1840/3208 | Loss: 1.7047\n",
            "  Batch 1850/3208 | Loss: 1.0898\n",
            "  Batch 1860/3208 | Loss: 0.7656\n",
            "  Batch 1870/3208 | Loss: 1.0359\n",
            "  Batch 1880/3208 | Loss: 0.7851\n",
            "  Batch 1890/3208 | Loss: 1.2352\n",
            "  Batch 1900/3208 | Loss: 1.0600\n",
            "  Batch 1910/3208 | Loss: 0.7326\n",
            "  Batch 1920/3208 | Loss: 0.6043\n",
            "  Batch 1930/3208 | Loss: 1.1883\n",
            "  Batch 1940/3208 | Loss: 0.7172\n",
            "  Batch 1950/3208 | Loss: 1.2280\n",
            "  Batch 1960/3208 | Loss: 1.0500\n",
            "  Batch 1970/3208 | Loss: 0.9760\n",
            "  Batch 1980/3208 | Loss: 1.0317\n",
            "  Batch 1990/3208 | Loss: 0.4738\n",
            "  Batch 2000/3208 | Loss: 0.7062\n",
            "  Batch 2010/3208 | Loss: 1.0917\n",
            "  Batch 2020/3208 | Loss: 0.8042\n",
            "  Batch 2030/3208 | Loss: 1.4830\n",
            "  Batch 2040/3208 | Loss: 1.5651\n",
            "  Batch 2050/3208 | Loss: 1.3290\n",
            "  Batch 2060/3208 | Loss: 0.8232\n",
            "  Batch 2070/3208 | Loss: 0.9366\n",
            "  Batch 2080/3208 | Loss: 1.0694\n",
            "  Batch 2090/3208 | Loss: 1.1631\n",
            "  Batch 2100/3208 | Loss: 1.1590\n",
            "  Batch 2110/3208 | Loss: 1.1212\n",
            "  Batch 2120/3208 | Loss: 1.6292\n",
            "  Batch 2130/3208 | Loss: 1.2472\n",
            "  Batch 2140/3208 | Loss: 1.5437\n",
            "  Batch 2150/3208 | Loss: 0.7848\n",
            "  Batch 2160/3208 | Loss: 1.2808\n",
            "  Batch 2170/3208 | Loss: 0.6637\n",
            "  Batch 2180/3208 | Loss: 0.9247\n",
            "  Batch 2190/3208 | Loss: 0.7226\n",
            "  Batch 2200/3208 | Loss: 1.1003\n",
            "  Batch 2210/3208 | Loss: 0.7246\n",
            "  Batch 2220/3208 | Loss: 1.3998\n",
            "  Batch 2230/3208 | Loss: 1.2918\n",
            "  Batch 2240/3208 | Loss: 1.4369\n",
            "  Batch 2250/3208 | Loss: 0.7225\n",
            "  Batch 2260/3208 | Loss: 0.6340\n",
            "  Batch 2270/3208 | Loss: 1.0381\n",
            "  Batch 2280/3208 | Loss: 0.9410\n",
            "  Batch 2290/3208 | Loss: 1.5614\n",
            "  Batch 2300/3208 | Loss: 0.5188\n",
            "  Batch 2310/3208 | Loss: 1.4753\n",
            "  Batch 2320/3208 | Loss: 0.6723\n",
            "  Batch 2330/3208 | Loss: 1.2513\n",
            "  Batch 2340/3208 | Loss: 0.6415\n",
            "  Batch 2350/3208 | Loss: 0.7712\n",
            "  Batch 2360/3208 | Loss: 0.9599\n",
            "  Batch 2370/3208 | Loss: 0.8651\n",
            "  Batch 2380/3208 | Loss: 0.8605\n",
            "  Batch 2390/3208 | Loss: 0.9149\n",
            "  Batch 2400/3208 | Loss: 1.6215\n",
            "  Batch 2410/3208 | Loss: 0.7355\n",
            "  Batch 2420/3208 | Loss: 0.6294\n",
            "  Batch 2430/3208 | Loss: 1.1581\n",
            "  Batch 2440/3208 | Loss: 1.3741\n",
            "  Batch 2450/3208 | Loss: 0.7269\n",
            "  Batch 2460/3208 | Loss: 0.7519\n",
            "  Batch 2470/3208 | Loss: 0.6499\n",
            "  Batch 2480/3208 | Loss: 1.0616\n",
            "  Batch 2490/3208 | Loss: 1.3814\n",
            "  Batch 2500/3208 | Loss: 0.6968\n",
            "  Batch 2510/3208 | Loss: 1.0288\n",
            "  Batch 2520/3208 | Loss: 0.8295\n",
            "  Batch 2530/3208 | Loss: 0.9743\n",
            "  Batch 2540/3208 | Loss: 1.1727\n",
            "  Batch 2550/3208 | Loss: 0.6200\n",
            "  Batch 2560/3208 | Loss: 0.8104\n",
            "  Batch 2570/3208 | Loss: 1.4024\n",
            "  Batch 2580/3208 | Loss: 0.9661\n",
            "  Batch 2590/3208 | Loss: 1.6552\n",
            "  Batch 2600/3208 | Loss: 1.4411\n",
            "  Batch 2610/3208 | Loss: 1.2020\n",
            "  Batch 2620/3208 | Loss: 1.5525\n",
            "  Batch 2630/3208 | Loss: 1.1654\n",
            "  Batch 2640/3208 | Loss: 0.8690\n",
            "  Batch 2650/3208 | Loss: 0.8798\n",
            "  Batch 2660/3208 | Loss: 0.6452\n",
            "  Batch 2670/3208 | Loss: 0.8952\n",
            "  Batch 2680/3208 | Loss: 1.4878\n",
            "  Batch 2690/3208 | Loss: 0.8464\n",
            "  Batch 2700/3208 | Loss: 1.2415\n",
            "  Batch 2710/3208 | Loss: 0.9125\n",
            "  Batch 2720/3208 | Loss: 0.8383\n",
            "  Batch 2730/3208 | Loss: 1.1050\n",
            "  Batch 2740/3208 | Loss: 0.9994\n",
            "  Batch 2750/3208 | Loss: 0.9380\n",
            "  Batch 2760/3208 | Loss: 1.0336\n",
            "  Batch 2770/3208 | Loss: 1.2669\n",
            "  Batch 2780/3208 | Loss: 1.2150\n",
            "  Batch 2790/3208 | Loss: 1.2367\n",
            "  Batch 2800/3208 | Loss: 0.5688\n",
            "  Batch 2810/3208 | Loss: 1.2296\n",
            "  Batch 2820/3208 | Loss: 0.8285\n",
            "  Batch 2830/3208 | Loss: 0.8133\n",
            "  Batch 2840/3208 | Loss: 0.9711\n",
            "  Batch 2850/3208 | Loss: 0.7339\n",
            "  Batch 2860/3208 | Loss: 0.6795\n",
            "  Batch 2870/3208 | Loss: 1.2315\n",
            "  Batch 2880/3208 | Loss: 1.2205\n",
            "  Batch 2890/3208 | Loss: 1.7800\n",
            "  Batch 2900/3208 | Loss: 1.5187\n",
            "  Batch 2910/3208 | Loss: 1.0506\n",
            "  Batch 2920/3208 | Loss: 1.3822\n",
            "  Batch 2930/3208 | Loss: 0.7991\n",
            "  Batch 2940/3208 | Loss: 0.8844\n",
            "  Batch 2950/3208 | Loss: 0.7758\n",
            "  Batch 2960/3208 | Loss: 1.0619\n",
            "  Batch 2970/3208 | Loss: 0.7729\n",
            "  Batch 2980/3208 | Loss: 1.2595\n",
            "  Batch 2990/3208 | Loss: 1.4046\n",
            "  Batch 3000/3208 | Loss: 1.0127\n",
            "  Batch 3010/3208 | Loss: 0.8695\n",
            "  Batch 3020/3208 | Loss: 1.5216\n",
            "  Batch 3030/3208 | Loss: 1.1560\n",
            "  Batch 3040/3208 | Loss: 0.8395\n",
            "  Batch 3050/3208 | Loss: 0.8600\n",
            "  Batch 3060/3208 | Loss: 0.8527\n",
            "  Batch 3070/3208 | Loss: 0.4359\n",
            "  Batch 3080/3208 | Loss: 0.7913\n",
            "  Batch 3090/3208 | Loss: 0.8267\n",
            "  Batch 3100/3208 | Loss: 0.7328\n",
            "  Batch 3110/3208 | Loss: 0.9472\n",
            "  Batch 3120/3208 | Loss: 0.8961\n",
            "  Batch 3130/3208 | Loss: 1.2121\n",
            "  Batch 3140/3208 | Loss: 1.4195\n",
            "  Batch 3150/3208 | Loss: 1.4476\n",
            "  Batch 3160/3208 | Loss: 1.0200\n",
            "  Batch 3170/3208 | Loss: 1.0427\n",
            "  Batch 3180/3208 | Loss: 1.3103\n",
            "  Batch 3190/3208 | Loss: 1.4189\n",
            "  Batch 3200/3208 | Loss: 0.9277\n",
            " Epoch 19 Completed | Avg Loss: 1.0212 | Training Accuracy: 69.54%\n",
            " Validation Accuracy: 73.36%\n",
            " Best Model Saved (Epoch 19 | Accuracy: 73.36%)\n",
            "\n",
            " Epoch 20/20\n",
            "  Batch 10/3208 | Loss: 0.9141\n",
            "  Batch 20/3208 | Loss: 0.8523\n",
            "  Batch 30/3208 | Loss: 1.1309\n",
            "  Batch 40/3208 | Loss: 0.6035\n",
            "  Batch 50/3208 | Loss: 1.2343\n",
            "  Batch 60/3208 | Loss: 0.8277\n",
            "  Batch 70/3208 | Loss: 0.6721\n",
            "  Batch 80/3208 | Loss: 0.7457\n",
            "  Batch 90/3208 | Loss: 0.5666\n",
            "  Batch 100/3208 | Loss: 0.6715\n",
            "  Batch 110/3208 | Loss: 1.3443\n",
            "  Batch 120/3208 | Loss: 1.2850\n",
            "  Batch 130/3208 | Loss: 0.7909\n",
            "  Batch 140/3208 | Loss: 0.7819\n",
            "  Batch 150/3208 | Loss: 1.2186\n",
            "  Batch 160/3208 | Loss: 1.0719\n",
            "  Batch 170/3208 | Loss: 1.0653\n",
            "  Batch 180/3208 | Loss: 0.9027\n",
            "  Batch 190/3208 | Loss: 1.0405\n",
            "  Batch 200/3208 | Loss: 0.6797\n",
            "  Batch 210/3208 | Loss: 1.8173\n",
            "  Batch 220/3208 | Loss: 0.8748\n",
            "  Batch 230/3208 | Loss: 0.9226\n",
            "  Batch 240/3208 | Loss: 1.4701\n",
            "  Batch 250/3208 | Loss: 0.9700\n",
            "  Batch 260/3208 | Loss: 0.6720\n",
            "  Batch 270/3208 | Loss: 1.3470\n",
            "  Batch 280/3208 | Loss: 1.6990\n",
            "  Batch 290/3208 | Loss: 1.0251\n",
            "  Batch 300/3208 | Loss: 1.1266\n",
            "  Batch 310/3208 | Loss: 1.3326\n",
            "  Batch 320/3208 | Loss: 0.9430\n",
            "  Batch 330/3208 | Loss: 0.9815\n",
            "  Batch 340/3208 | Loss: 0.8042\n",
            "  Batch 350/3208 | Loss: 0.8026\n",
            "  Batch 360/3208 | Loss: 0.6804\n",
            "  Batch 370/3208 | Loss: 0.7710\n",
            "  Batch 380/3208 | Loss: 0.7573\n",
            "  Batch 390/3208 | Loss: 0.8655\n",
            "  Batch 400/3208 | Loss: 0.7050\n",
            "  Batch 410/3208 | Loss: 1.2874\n",
            "  Batch 420/3208 | Loss: 0.5834\n",
            "  Batch 430/3208 | Loss: 0.6132\n",
            "  Batch 440/3208 | Loss: 1.0675\n",
            "  Batch 450/3208 | Loss: 1.6246\n",
            "  Batch 460/3208 | Loss: 0.9861\n",
            "  Batch 470/3208 | Loss: 0.5202\n",
            "  Batch 480/3208 | Loss: 0.7109\n",
            "  Batch 490/3208 | Loss: 1.1899\n",
            "  Batch 500/3208 | Loss: 1.0160\n",
            "  Batch 510/3208 | Loss: 0.8316\n",
            "  Batch 520/3208 | Loss: 0.9679\n",
            "  Batch 530/3208 | Loss: 0.7640\n",
            "  Batch 540/3208 | Loss: 0.9019\n",
            "  Batch 550/3208 | Loss: 0.6829\n",
            "  Batch 560/3208 | Loss: 0.9832\n",
            "  Batch 570/3208 | Loss: 0.8538\n",
            "  Batch 580/3208 | Loss: 0.9375\n",
            "  Batch 590/3208 | Loss: 0.8864\n",
            "  Batch 600/3208 | Loss: 1.1391\n",
            "  Batch 610/3208 | Loss: 1.4851\n",
            "  Batch 620/3208 | Loss: 1.1396\n",
            "  Batch 630/3208 | Loss: 0.8692\n",
            "  Batch 640/3208 | Loss: 0.8222\n",
            "  Batch 650/3208 | Loss: 1.0086\n",
            "  Batch 660/3208 | Loss: 1.1398\n",
            "  Batch 670/3208 | Loss: 1.0069\n",
            "  Batch 680/3208 | Loss: 0.7731\n",
            "  Batch 690/3208 | Loss: 0.5239\n",
            "  Batch 700/3208 | Loss: 0.7578\n",
            "  Batch 710/3208 | Loss: 1.2025\n",
            "  Batch 720/3208 | Loss: 0.8186\n",
            "  Batch 730/3208 | Loss: 0.9712\n",
            "  Batch 740/3208 | Loss: 1.2251\n",
            "  Batch 750/3208 | Loss: 1.0331\n",
            "  Batch 760/3208 | Loss: 1.3299\n",
            "  Batch 770/3208 | Loss: 1.3505\n",
            "  Batch 780/3208 | Loss: 0.8063\n",
            "  Batch 790/3208 | Loss: 0.6163\n",
            "  Batch 800/3208 | Loss: 0.6954\n",
            "  Batch 810/3208 | Loss: 0.7930\n",
            "  Batch 820/3208 | Loss: 0.7402\n",
            "  Batch 830/3208 | Loss: 0.6072\n",
            "  Batch 840/3208 | Loss: 0.9876\n",
            "  Batch 850/3208 | Loss: 1.3486\n",
            "  Batch 860/3208 | Loss: 0.5091\n",
            "  Batch 870/3208 | Loss: 1.3466\n",
            "  Batch 880/3208 | Loss: 1.2997\n",
            "  Batch 890/3208 | Loss: 1.0665\n",
            "  Batch 900/3208 | Loss: 1.1469\n",
            "  Batch 910/3208 | Loss: 1.1706\n",
            "  Batch 920/3208 | Loss: 1.0400\n",
            "  Batch 930/3208 | Loss: 0.7847\n",
            "  Batch 940/3208 | Loss: 1.0487\n",
            "  Batch 950/3208 | Loss: 1.5052\n",
            "  Batch 960/3208 | Loss: 0.8617\n",
            "  Batch 970/3208 | Loss: 1.2398\n",
            "  Batch 980/3208 | Loss: 1.0613\n",
            "  Batch 990/3208 | Loss: 1.0538\n",
            "  Batch 1000/3208 | Loss: 1.3485\n",
            "  Batch 1010/3208 | Loss: 0.7146\n",
            "  Batch 1020/3208 | Loss: 0.9371\n",
            "  Batch 1030/3208 | Loss: 0.6548\n",
            "  Batch 1040/3208 | Loss: 0.6494\n",
            "  Batch 1050/3208 | Loss: 1.1673\n",
            "  Batch 1060/3208 | Loss: 1.2257\n",
            "  Batch 1070/3208 | Loss: 1.5136\n",
            "  Batch 1080/3208 | Loss: 0.7075\n",
            "  Batch 1090/3208 | Loss: 1.0888\n",
            "  Batch 1100/3208 | Loss: 1.2617\n",
            "  Batch 1110/3208 | Loss: 0.5247\n",
            "  Batch 1120/3208 | Loss: 0.7966\n",
            "  Batch 1130/3208 | Loss: 1.2432\n",
            "  Batch 1140/3208 | Loss: 1.1250\n",
            "  Batch 1150/3208 | Loss: 1.6450\n",
            "  Batch 1160/3208 | Loss: 0.9371\n",
            "  Batch 1170/3208 | Loss: 0.8572\n",
            "  Batch 1180/3208 | Loss: 0.8235\n",
            "  Batch 1190/3208 | Loss: 1.1175\n",
            "  Batch 1200/3208 | Loss: 0.8831\n",
            "  Batch 1210/3208 | Loss: 0.8218\n",
            "  Batch 1220/3208 | Loss: 0.7958\n",
            "  Batch 1230/3208 | Loss: 1.4147\n",
            "  Batch 1240/3208 | Loss: 1.0630\n",
            "  Batch 1250/3208 | Loss: 1.3222\n",
            "  Batch 1260/3208 | Loss: 1.0831\n",
            "  Batch 1270/3208 | Loss: 0.8452\n",
            "  Batch 1280/3208 | Loss: 1.2262\n",
            "  Batch 1290/3208 | Loss: 0.2860\n",
            "  Batch 1300/3208 | Loss: 1.2675\n",
            "  Batch 1310/3208 | Loss: 1.1385\n",
            "  Batch 1320/3208 | Loss: 0.9955\n",
            "  Batch 1330/3208 | Loss: 0.9101\n",
            "  Batch 1340/3208 | Loss: 0.7259\n",
            "  Batch 1350/3208 | Loss: 1.0540\n",
            "  Batch 1360/3208 | Loss: 0.6146\n",
            "  Batch 1370/3208 | Loss: 0.5540\n",
            "  Batch 1380/3208 | Loss: 1.1611\n",
            "  Batch 1390/3208 | Loss: 0.8402\n",
            "  Batch 1400/3208 | Loss: 1.4135\n",
            "  Batch 1410/3208 | Loss: 1.3582\n",
            "  Batch 1420/3208 | Loss: 0.5022\n",
            "  Batch 1430/3208 | Loss: 1.0774\n",
            "  Batch 1440/3208 | Loss: 1.1030\n",
            "  Batch 1450/3208 | Loss: 0.9922\n",
            "  Batch 1460/3208 | Loss: 0.6839\n",
            "  Batch 1470/3208 | Loss: 0.8049\n",
            "  Batch 1480/3208 | Loss: 1.0230\n",
            "  Batch 1490/3208 | Loss: 0.5680\n",
            "  Batch 1500/3208 | Loss: 1.2478\n",
            "  Batch 1510/3208 | Loss: 1.3391\n",
            "  Batch 1520/3208 | Loss: 1.2990\n",
            "  Batch 1530/3208 | Loss: 1.0002\n",
            "  Batch 1540/3208 | Loss: 0.7134\n",
            "  Batch 1550/3208 | Loss: 0.8798\n",
            "  Batch 1560/3208 | Loss: 0.7955\n",
            "  Batch 1570/3208 | Loss: 0.9214\n",
            "  Batch 1580/3208 | Loss: 1.4019\n",
            "  Batch 1590/3208 | Loss: 0.9929\n",
            "  Batch 1600/3208 | Loss: 1.0448\n",
            "  Batch 1610/3208 | Loss: 1.1032\n",
            "  Batch 1620/3208 | Loss: 1.3370\n",
            "  Batch 1630/3208 | Loss: 1.2655\n",
            "  Batch 1640/3208 | Loss: 0.9640\n",
            "  Batch 1650/3208 | Loss: 0.8243\n",
            "  Batch 1660/3208 | Loss: 1.0567\n",
            "  Batch 1670/3208 | Loss: 0.9130\n",
            "  Batch 1680/3208 | Loss: 1.0578\n",
            "  Batch 1690/3208 | Loss: 1.3424\n",
            "  Batch 1700/3208 | Loss: 1.7741\n",
            "  Batch 1710/3208 | Loss: 1.1518\n",
            "  Batch 1720/3208 | Loss: 0.8863\n",
            "  Batch 1730/3208 | Loss: 0.9368\n",
            "  Batch 1740/3208 | Loss: 1.0770\n",
            "  Batch 1750/3208 | Loss: 0.7440\n",
            "  Batch 1760/3208 | Loss: 0.9612\n",
            "  Batch 1770/3208 | Loss: 0.5700\n",
            "  Batch 1780/3208 | Loss: 1.0519\n",
            "  Batch 1790/3208 | Loss: 0.6158\n",
            "  Batch 1800/3208 | Loss: 0.7113\n",
            "  Batch 1810/3208 | Loss: 0.4930\n",
            "  Batch 1820/3208 | Loss: 1.1685\n",
            "  Batch 1830/3208 | Loss: 1.2067\n",
            "  Batch 1840/3208 | Loss: 0.5542\n",
            "  Batch 1850/3208 | Loss: 0.8868\n",
            "  Batch 1860/3208 | Loss: 1.1224\n",
            "  Batch 1870/3208 | Loss: 1.5696\n",
            "  Batch 1880/3208 | Loss: 0.9461\n",
            "  Batch 1890/3208 | Loss: 1.3542\n",
            "  Batch 1900/3208 | Loss: 1.3387\n",
            "  Batch 1910/3208 | Loss: 1.5839\n",
            "  Batch 1920/3208 | Loss: 1.2041\n",
            "  Batch 1930/3208 | Loss: 1.3422\n",
            "  Batch 1940/3208 | Loss: 0.8147\n",
            "  Batch 1950/3208 | Loss: 0.7975\n",
            "  Batch 1960/3208 | Loss: 1.0026\n",
            "  Batch 1970/3208 | Loss: 1.0641\n",
            "  Batch 1980/3208 | Loss: 1.0057\n",
            "  Batch 1990/3208 | Loss: 1.0515\n",
            "  Batch 2000/3208 | Loss: 1.0134\n",
            "  Batch 2010/3208 | Loss: 0.8881\n",
            "  Batch 2020/3208 | Loss: 1.3662\n",
            "  Batch 2030/3208 | Loss: 1.4193\n",
            "  Batch 2040/3208 | Loss: 0.9966\n",
            "  Batch 2050/3208 | Loss: 0.5975\n",
            "  Batch 2060/3208 | Loss: 0.6925\n",
            "  Batch 2070/3208 | Loss: 0.2809\n",
            "  Batch 2080/3208 | Loss: 1.3074\n",
            "  Batch 2090/3208 | Loss: 0.8781\n",
            "  Batch 2100/3208 | Loss: 0.6857\n",
            "  Batch 2110/3208 | Loss: 1.4782\n",
            "  Batch 2120/3208 | Loss: 1.0282\n",
            "  Batch 2130/3208 | Loss: 1.0808\n",
            "  Batch 2140/3208 | Loss: 1.3321\n",
            "  Batch 2150/3208 | Loss: 0.8281\n",
            "  Batch 2160/3208 | Loss: 0.7621\n",
            "  Batch 2170/3208 | Loss: 1.1948\n",
            "  Batch 2180/3208 | Loss: 0.8254\n",
            "  Batch 2190/3208 | Loss: 1.4834\n",
            "  Batch 2200/3208 | Loss: 1.5778\n",
            "  Batch 2210/3208 | Loss: 0.6337\n",
            "  Batch 2220/3208 | Loss: 1.2191\n",
            "  Batch 2230/3208 | Loss: 0.9694\n",
            "  Batch 2240/3208 | Loss: 1.5145\n",
            "  Batch 2250/3208 | Loss: 1.1347\n",
            "  Batch 2260/3208 | Loss: 0.9139\n",
            "  Batch 2270/3208 | Loss: 1.1744\n",
            "  Batch 2280/3208 | Loss: 0.6595\n",
            "  Batch 2290/3208 | Loss: 0.5422\n",
            "  Batch 2300/3208 | Loss: 0.5461\n",
            "  Batch 2310/3208 | Loss: 0.9131\n",
            "  Batch 2320/3208 | Loss: 1.1216\n",
            "  Batch 2330/3208 | Loss: 1.1810\n",
            "  Batch 2340/3208 | Loss: 0.8098\n",
            "  Batch 2350/3208 | Loss: 1.3421\n",
            "  Batch 2360/3208 | Loss: 1.2769\n",
            "  Batch 2370/3208 | Loss: 0.8292\n",
            "  Batch 2380/3208 | Loss: 0.7039\n",
            "  Batch 2390/3208 | Loss: 0.9767\n",
            "  Batch 2400/3208 | Loss: 1.0420\n",
            "  Batch 2410/3208 | Loss: 1.0733\n",
            "  Batch 2420/3208 | Loss: 0.6670\n",
            "  Batch 2430/3208 | Loss: 1.4740\n",
            "  Batch 2440/3208 | Loss: 0.9368\n",
            "  Batch 2450/3208 | Loss: 0.7331\n",
            "  Batch 2460/3208 | Loss: 0.6658\n",
            "  Batch 2470/3208 | Loss: 0.9440\n",
            "  Batch 2480/3208 | Loss: 1.0362\n",
            "  Batch 2490/3208 | Loss: 0.9893\n",
            "  Batch 2500/3208 | Loss: 1.4909\n",
            "  Batch 2510/3208 | Loss: 0.8655\n",
            "  Batch 2520/3208 | Loss: 0.5650\n",
            "  Batch 2530/3208 | Loss: 1.4804\n",
            "  Batch 2540/3208 | Loss: 1.3525\n",
            "  Batch 2550/3208 | Loss: 0.7811\n",
            "  Batch 2560/3208 | Loss: 0.4820\n",
            "  Batch 2570/3208 | Loss: 0.7703\n",
            "  Batch 2580/3208 | Loss: 0.6070\n",
            "  Batch 2590/3208 | Loss: 1.0141\n",
            "  Batch 2600/3208 | Loss: 0.7643\n",
            "  Batch 2610/3208 | Loss: 0.8908\n",
            "  Batch 2620/3208 | Loss: 0.8828\n",
            "  Batch 2630/3208 | Loss: 0.5403\n",
            "  Batch 2640/3208 | Loss: 0.9134\n",
            "  Batch 2650/3208 | Loss: 1.4091\n",
            "  Batch 2660/3208 | Loss: 0.5479\n",
            "  Batch 2670/3208 | Loss: 0.8548\n",
            "  Batch 2680/3208 | Loss: 1.4478\n",
            "  Batch 2690/3208 | Loss: 0.8206\n",
            "  Batch 2700/3208 | Loss: 0.6368\n",
            "  Batch 2710/3208 | Loss: 0.9327\n",
            "  Batch 2720/3208 | Loss: 0.6452\n",
            "  Batch 2730/3208 | Loss: 0.4145\n",
            "  Batch 2740/3208 | Loss: 1.3781\n",
            "  Batch 2750/3208 | Loss: 0.9486\n",
            "  Batch 2760/3208 | Loss: 0.7686\n",
            "  Batch 2770/3208 | Loss: 1.0937\n",
            "  Batch 2780/3208 | Loss: 0.6196\n",
            "  Batch 2790/3208 | Loss: 1.0023\n",
            "  Batch 2800/3208 | Loss: 1.0542\n",
            "  Batch 2810/3208 | Loss: 1.0582\n",
            "  Batch 2820/3208 | Loss: 0.5549\n",
            "  Batch 2830/3208 | Loss: 1.5416\n",
            "  Batch 2840/3208 | Loss: 0.9012\n",
            "  Batch 2850/3208 | Loss: 1.3375\n",
            "  Batch 2860/3208 | Loss: 0.9015\n",
            "  Batch 2870/3208 | Loss: 1.4300\n",
            "  Batch 2880/3208 | Loss: 1.4959\n",
            "  Batch 2890/3208 | Loss: 1.0969\n",
            "  Batch 2900/3208 | Loss: 0.7339\n",
            "  Batch 2910/3208 | Loss: 1.0376\n",
            "  Batch 2920/3208 | Loss: 0.8675\n",
            "  Batch 2930/3208 | Loss: 0.6578\n",
            "  Batch 2940/3208 | Loss: 0.9980\n",
            "  Batch 2950/3208 | Loss: 1.0837\n",
            "  Batch 2960/3208 | Loss: 0.4774\n",
            "  Batch 2970/3208 | Loss: 0.6478\n",
            "  Batch 2980/3208 | Loss: 0.3765\n",
            "  Batch 2990/3208 | Loss: 0.7629\n",
            "  Batch 3000/3208 | Loss: 0.7532\n",
            "  Batch 3010/3208 | Loss: 0.9117\n",
            "  Batch 3020/3208 | Loss: 1.1007\n",
            "  Batch 3030/3208 | Loss: 1.3115\n",
            "  Batch 3040/3208 | Loss: 0.8250\n",
            "  Batch 3050/3208 | Loss: 0.6990\n",
            "  Batch 3060/3208 | Loss: 0.8212\n",
            "  Batch 3070/3208 | Loss: 1.0255\n",
            "  Batch 3080/3208 | Loss: 0.9140\n",
            "  Batch 3090/3208 | Loss: 0.5732\n",
            "  Batch 3100/3208 | Loss: 1.3371\n",
            "  Batch 3110/3208 | Loss: 1.1749\n",
            "  Batch 3120/3208 | Loss: 0.7891\n",
            "  Batch 3130/3208 | Loss: 0.6533\n",
            "  Batch 3140/3208 | Loss: 0.6684\n",
            "  Batch 3150/3208 | Loss: 0.8939\n",
            "  Batch 3160/3208 | Loss: 0.8044\n",
            "  Batch 3170/3208 | Loss: 1.3139\n",
            "  Batch 3180/3208 | Loss: 0.7362\n",
            "  Batch 3190/3208 | Loss: 1.1515\n",
            "  Batch 3200/3208 | Loss: 1.8309\n",
            " Epoch 20 Completed | Avg Loss: 0.9925 | Training Accuracy: 70.21%\n",
            " Validation Accuracy: 73.93%\n",
            " Best Model Saved (Epoch 20 | Accuracy: 73.93%)\n",
            " Training Complete!\n",
            "Figure(800x500)\n",
            "Figure(800x500)\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEpGZmGX1BNj",
        "outputId": "b45f0bda-7049-42e1-a463-a745b4f693e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln771YkrEWjn",
        "outputId": "8fac7233-bfd6-4529-80c6-26d3c56faf40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\n",
            "🚀 Evaluating Model on Test Data...\n",
            "\n",
            " Test Accuracy: 74.97%\n",
            " Confusion Matrix saved at: /content/drive/MyDrive/Keyword_spotting_transformer/plots/confusion_matrix_whitenoise_0dB.png\n",
            "/content/drive/MyDrive/Keyword_spotting_transformer/test.py:101: FutureWarning: \n",
            "\n",
            "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
            "\n",
            "  sns.barplot(x=list(sorted_misclassified.keys())[:10], y=list(sorted_misclassified.values())[:10], palette=\"coolwarm\")\n",
            "Misclassified Words Bar Graph saved at: /content/drive/MyDrive/Keyword_spotting_transformer/plots/whitenoise_misclassified_words.png\n"
          ]
        }
      ],
      "source": [
        "!python test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8rSr9Vwr01k",
        "outputId": "2be16fa3-a123-43a8-92e6-3b5efabce689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting model evaluations...\n",
            "\n",
            "Evaluating model: no_noise\n",
            " Testing on dataset: no_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/test_processed\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_no_noise.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 87.11%\n",
            "\n",
            " Testing on dataset: pink_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_pinknoise\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_no_noise.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 53.39%\n",
            "\n",
            " Testing on dataset: white_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_whitenoise\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_no_noise.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 27.86%\n",
            "\n",
            " Testing on dataset: bike_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_exercise_bike\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_no_noise.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 22.97%\n",
            "\n",
            "Evaluating model: pink_noise\n",
            " Testing on dataset: no_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/test_processed\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_pinknoise_10dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 60.99%\n",
            "\n",
            " Testing on dataset: pink_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_pinknoise\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_pinknoise_10dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 84.35%\n",
            "\n",
            " Testing on dataset: white_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_whitenoise\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_pinknoise_10dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 60.86%\n",
            "\n",
            " Testing on dataset: bike_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_exercise_bike\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_pinknoise_10dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 52.60%\n",
            "\n",
            "Evaluating model: white_noise\n",
            " Testing on dataset: no_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/test_processed\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_whitenoise_0dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 22.44%\n",
            "\n",
            " Testing on dataset: pink_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_pinknoise\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_whitenoise_0dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 18.45%\n",
            "\n",
            " Testing on dataset: white_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_whitenoise\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_whitenoise_0dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 74.97%\n",
            "\n",
            " Testing on dataset: bike_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_exercise_bike\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_whitenoise_0dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 24.73%\n",
            "\n",
            "Evaluating model: bike_noise\n",
            " Testing on dataset: no_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/test_processed\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_exercise_bike_noise_0dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 26.03%\n",
            "\n",
            " Testing on dataset: pink_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_pinknoise\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_exercise_bike_noise_0dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 25.02%\n",
            "\n",
            " Testing on dataset: white_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_whitenoise\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_exercise_bike_noise_0dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 64.08%\n",
            "\n",
            " Testing on dataset: bike_noise\n",
            "  Loading test data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_exercise_bike\n",
            "  Initializing and loading model from: /content/drive/MyDrive/Keyword_spotting_transformer/models/model_exercise_bike_noise_0dB.pth\n",
            "  Starting evaluation...\n",
            "  Finished. Accuracy: 73.77%\n",
            "\n",
            "\n",
            "=== Accuracy Matrix (Models x Test Sets) ===\n",
            "                       no_noise     pink_noise    white_noise     bike_noise\n",
            "no_noise                  87.11          53.39          27.86          22.97\n",
            "pink_noise                60.99          84.35          60.86          52.60\n",
            "white_noise               22.44          18.45          74.97          24.73\n",
            "bike_noise                26.03          25.02          64.08          73.77\n"
          ]
        }
      ],
      "source": [
        "!python test_metrics.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHPiEyol-USz",
        "outputId": "8d47bb95-5594-48df-ba05-cbfda3efe0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FlPOj0lhClrK",
        "outputId": "eb462741-f90b-4521-f7af-34a83bada294"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYYxighHg4n9",
        "outputId": "694ba97b-7b53-40f4-a8f6-d153115f6c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0f_k41Z8qLf",
        "outputId": "535ba847-3caf-4f67-c9d5-a05e0dfd0177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " SNR (torchmetrics): -0.00 dB\n",
            " Duration analyzed: 2.64 seconds\n"
          ]
        }
      ],
      "source": [
        "!python snr.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB5gXE_agc84",
        "outputId": "c71ded25-c80d-4492-e65f-9fd23552b48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/augmentation\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hfDO9k3fWHx",
        "outputId": "ae8bf113-c201-4000-c694-68e1b3c8f871"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading background noise from: /content/drive/MyDrive/Keyword_spotting_transformer/data/background_noises/exercise_bike.wav\n",
            "\n",
            " Augmenting: /content/drive/MyDrive/Keyword_spotting_transformer/data/train → /content/drive/MyDrive/Keyword_spotting_transformer/data/train_augmented_exercise_bike_10dB\n",
            "Processing train/bed: 100% 1358/1358 [01:27<00:00, 15.50it/s]\n",
            "Processing train/bird: 100% 1421/1421 [00:57<00:00, 24.54it/s]\n",
            "Processing train/cat: 100% 1409/1409 [01:00<00:00, 23.25it/s]\n",
            "Processing train/dog: 100% 1396/1396 [00:56<00:00, 24.61it/s]\n",
            "Processing train/down: 100% 1852/1852 [01:27<00:00, 21.25it/s]\n",
            "Processing train/eight: 100% 1852/1852 [01:28<00:00, 21.03it/s]\n",
            "Processing train/five: 100% 1861/1861 [01:29<00:00, 20.79it/s]\n",
            "Processing train/four: 100% 1899/1899 [02:12<00:00, 14.32it/s]\n",
            "Processing train/go: 100% 1881/1881 [01:30<00:00, 20.74it/s]\n",
            "Processing train/happy: 100% 1373/1373 [01:07<00:00, 20.34it/s]\n",
            "Processing train/house: 100% 1427/1427 [01:05<00:00, 21.93it/s]\n",
            "Processing train/left: 100% 1839/1839 [01:32<00:00, 19.88it/s]\n",
            "Processing train/marvin: 100% 1444/1444 [01:07<00:00, 21.42it/s]\n",
            "Processing train/nine: 100% 1875/1875 [01:31<00:00, 20.43it/s]\n",
            "Processing train/no: 100% 1853/1853 [01:33<00:00, 19.72it/s]\n",
            "Processing train/off: 100% 1839/1839 [01:30<00:00, 20.27it/s]\n",
            "Processing train/on: 100% 1884/1884 [01:31<00:00, 20.66it/s]\n",
            "Processing train/one: 100% 1892/1892 [01:35<00:00, 19.82it/s]\n",
            "Processing train/right: 100% 1852/1852 [01:39<00:00, 18.70it/s]\n",
            "Processing train/seven: 100% 1876/1876 [01:33<00:00, 20.17it/s]\n",
            "Processing train/sheila: 100% 1382/1382 [01:02<00:00, 22.01it/s]\n",
            "Processing train/six: 100% 1863/1863 [01:33<00:00, 19.86it/s]\n",
            "Processing train/stop: 100% 1895/1895 [01:39<00:00, 19.06it/s]\n",
            "Processing train/three: 100% 1851/1851 [01:34<00:00, 19.48it/s]\n",
            "Processing train/tree: 100% 1374/1374 [01:03<00:00, 21.64it/s]\n",
            "Processing train/two: 100% 1873/1873 [01:41<00:00, 18.54it/s]\n",
            "Processing train/up: 100% 1857/1857 [01:35<00:00, 19.40it/s]\n",
            "Processing train/wow: 100% 1414/1414 [01:06<00:00, 21.11it/s]\n",
            "Processing train/yes: 100% 1860/1860 [01:35<00:00, 19.52it/s]\n",
            "Processing train/zero: 100% 1866/1866 [01:31<00:00, 20.33it/s]\n",
            "\n",
            " Augmenting: /content/drive/MyDrive/Keyword_spotting_transformer/data/valid → /content/drive/MyDrive/Keyword_spotting_transformer/data/valid_augmented_exercise_bike_10dB\n",
            "Processing valid/bed: 100% 197/197 [00:07<00:00, 25.08it/s]\n",
            "Processing valid/bird: 100% 162/162 [00:05<00:00, 29.96it/s]\n",
            "Processing valid/cat: 100% 168/168 [00:06<00:00, 26.50it/s]\n",
            "Processing valid/dog: 100% 170/170 [00:05<00:00, 30.59it/s]\n",
            "Processing valid/down: 100% 264/264 [00:08<00:00, 30.26it/s]\n",
            "Processing valid/eight: 100% 243/243 [00:08<00:00, 28.35it/s]\n",
            "Processing valid/five: 100% 242/242 [00:08<00:00, 29.73it/s]\n",
            "Processing valid/four: 100% 280/280 [00:09<00:00, 28.62it/s]\n",
            "Processing valid/go: 100% 260/260 [00:09<00:00, 26.99it/s]\n",
            "Processing valid/happy: 100% 189/189 [00:06<00:00, 29.16it/s]\n",
            "Processing valid/house: 100% 173/173 [00:06<00:00, 27.79it/s]\n",
            "Processing valid/left: 100% 247/247 [00:08<00:00, 28.57it/s]\n",
            "Processing valid/marvin: 100% 160/160 [00:06<00:00, 26.01it/s]\n",
            "Processing valid/nine: 100% 230/230 [00:08<00:00, 25.73it/s]\n",
            "Processing valid/no: 100% 270/270 [00:08<00:00, 30.77it/s]\n",
            "Processing valid/off: 100% 256/256 [00:09<00:00, 26.30it/s]\n",
            "Processing valid/on: 100% 257/257 [00:08<00:00, 30.95it/s]\n",
            "Processing valid/one: 100% 230/230 [00:07<00:00, 30.56it/s]\n",
            "Processing valid/right: 100% 256/256 [00:09<00:00, 28.02it/s]\n",
            "Processing valid/seven: 100% 263/263 [00:08<00:00, 31.34it/s]\n",
            "Processing valid/sheila: 100% 176/176 [00:06<00:00, 28.05it/s]\n",
            "Processing valid/six: 100% 262/262 [00:12<00:00, 21.01it/s]\n",
            "Processing valid/stop: 100% 246/246 [00:09<00:00, 27.11it/s]\n",
            "Processing valid/three: 100% 248/248 [00:08<00:00, 28.70it/s]\n",
            "Processing valid/tree: 100% 166/166 [00:05<00:00, 27.69it/s]\n",
            "Processing valid/two: 100% 236/236 [00:09<00:00, 24.40it/s]\n",
            "Processing valid/up: 100% 260/260 [00:10<00:00, 24.15it/s]\n",
            "Processing valid/wow: 100% 166/166 [00:05<00:00, 31.17it/s]\n",
            "Processing valid/yes: 100% 261/261 [00:15<00:00, 17.08it/s]\n",
            "Processing valid/zero: 100% 260/260 [00:09<00:00, 28.02it/s]\n",
            "\n",
            " All augmented audio files saved successfully!\n"
          ]
        }
      ],
      "source": [
        "!python augment_with_backgroundnoise.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-mScFB9tDPX",
        "outputId": "ffbc2314-6018-4884-dc34-8b068aa7f495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Successfully loaded spectrogram!\n",
            "Spectrogram Shape: (40, 101)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/train_processed/bed_0a7c2a8d_nohash_0.npy\"\n",
        "\n",
        "# Check if the file exists\n",
        "if not os.path.exists(file_path):\n",
        "    print(\"File does not exist!\")\n",
        "else:\n",
        "    try:\n",
        "        spectrogram = np.load(file_path)\n",
        "        print(\" Successfully loaded spectrogram!\")\n",
        "        print(\"Spectrogram Shape:\", spectrogram.shape)\n",
        "    except OSError as e:\n",
        "        print(f\" OSError: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EEVAP27-k_r"
      },
      "source": [
        "COPY FILES FROM DIFFERENT FOLDERS TO MIXED DATA FOLDER\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DKz_acj-yKK",
        "outputId": "bbb9f317-e304-4490-c673-518e41f52636"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE_cWdzvT0GG",
        "outputId": "814893aa-d6be-488d-b117-89a2ddb017d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of .npy files in folder: 51318\n"
          ]
        }
      ],
      "source": [
        "!python count_files.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8ANlk9O-kbM",
        "outputId": "ce59dc07-f1a3-4373-a5c9-98cd6474ce3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copied: zero_73dda36a_nohash_1_exercise_bike_noise_snr0.npy  (1)\n",
            "Copied: zero_49af4432_nohash_0_exercise_bike_noise_snr0.npy  (2)\n",
            "Copied: zero_c6ca5d01_nohash_0_exercise_bike_noise_snr0.npy  (3)\n",
            "Copied: zero_c38720cb_nohash_1_exercise_bike_noise_snr0.npy  (4)\n",
            "Copied: zero_0132a06d_nohash_3_exercise_bike_noise_snr0.npy  (5)\n",
            "Copied: zero_d53e25ba_nohash_1_exercise_bike_noise_snr0.npy  (6)\n",
            "Copied: zero_aff582a1_nohash_2_exercise_bike_noise_snr0.npy  (7)\n",
            "Copied: zero_b16f2d0d_nohash_0_exercise_bike_noise_snr0.npy  (8)\n",
            "Copied: zero_88120683_nohash_0_exercise_bike_noise_snr0.npy  (9)\n",
            "Copied: zero_24694eb6_nohash_0_exercise_bike_noise_snr0.npy  (10)\n",
            "Copied: zero_99081f4d_nohash_0_exercise_bike_noise_snr0.npy  (11)\n",
            "Copied: zero_36050ef3_nohash_4_exercise_bike_noise_snr0.npy  (12)\n",
            "Copied: zero_c8db14a8_nohash_0_exercise_bike_noise_snr0.npy  (13)\n",
            "Copied: zero_cd7f8c1b_nohash_0_exercise_bike_noise_snr0.npy  (14)\n",
            "Copied: zero_e652590d_nohash_0_exercise_bike_noise_snr0.npy  (15)\n",
            "Copied: zero_1365dd89_nohash_0_exercise_bike_noise_snr0.npy  (16)\n",
            "Copied: zero_857366dd_nohash_2_exercise_bike_noise_snr0.npy  (17)\n",
            "Copied: zero_5ab63b0a_nohash_0_exercise_bike_noise_snr0.npy  (18)\n",
            "Copied: zero_106a6183_nohash_4_exercise_bike_noise_snr0.npy  (19)\n",
            "Copied: zero_da15e796_nohash_0_exercise_bike_noise_snr0.npy  (20)\n",
            "Copied: zero_118261a9_nohash_1_exercise_bike_noise_snr0.npy  (21)\n",
            "Copied: zero_3ae5c04f_nohash_1_exercise_bike_noise_snr0.npy  (22)\n",
            "Copied: zero_b8897f1c_nohash_0_exercise_bike_noise_snr0.npy  (23)\n",
            "Copied: zero_5f1b1051_nohash_1_exercise_bike_noise_snr0.npy  (24)\n",
            "Copied: zero_a1cff772_nohash_3_exercise_bike_noise_snr0.npy  (25)\n",
            "Copied: zero_ca4eeab0_nohash_0_exercise_bike_noise_snr0.npy  (26)\n",
            "Copied: zero_4a1e736b_nohash_3_exercise_bike_noise_snr0.npy  (27)\n",
            "Copied: zero_2275edbb_nohash_0_exercise_bike_noise_snr0.npy  (28)\n",
            "Copied: zero_834f03fe_nohash_0_exercise_bike_noise_snr0.npy  (29)\n",
            "Copied: zero_c2aeb59d_nohash_0_exercise_bike_noise_snr0.npy  (30)\n",
            "Copied: zero_f59d0771_nohash_1_exercise_bike_noise_snr0.npy  (31)\n",
            "Copied: zero_f92e49f3_nohash_0_exercise_bike_noise_snr0.npy  (32)\n",
            "Copied: zero_f875f965_nohash_0_exercise_bike_noise_snr0.npy  (33)\n",
            "Copied: zero_01b4757a_nohash_0_exercise_bike_noise_snr0.npy  (34)\n",
            "Copied: zero_c137814b_nohash_0_exercise_bike_noise_snr0.npy  (35)\n",
            "Copied: zero_7e6bd776_nohash_0_exercise_bike_noise_snr0.npy  (36)\n",
            "Copied: zero_421ed23f_nohash_2_exercise_bike_noise_snr0.npy  (37)\n",
            "Copied: zero_0eb48e10_nohash_0_exercise_bike_noise_snr0.npy  (38)\n",
            "Copied: zero_7d149b38_nohash_0_exercise_bike_noise_snr0.npy  (39)\n",
            "Copied: zero_7ea032f3_nohash_0_exercise_bike_noise_snr0.npy  (40)\n",
            "Copied: zero_2903efb3_nohash_0_exercise_bike_noise_snr0.npy  (41)\n",
            "Copied: zero_e0a7c5a0_nohash_0_exercise_bike_noise_snr0.npy  (42)\n",
            "Copied: zero_92a9c5e6_nohash_4_exercise_bike_noise_snr0.npy  (43)\n",
            "Copied: zero_833a0279_nohash_0_exercise_bike_noise_snr0.npy  (44)\n",
            "Copied: zero_016e2c6d_nohash_0_exercise_bike_noise_snr0.npy  (45)\n",
            "Copied: zero_ce7a8e92_nohash_1_exercise_bike_noise_snr0.npy  (46)\n",
            "Copied: zero_cb8f8307_nohash_3_exercise_bike_noise_snr0.npy  (47)\n",
            "Copied: zero_31583d30_nohash_1_exercise_bike_noise_snr0.npy  (48)\n",
            "Copied: zero_5fb88f4c_nohash_0_exercise_bike_noise_snr0.npy  (49)\n",
            "Copied: zero_413997c1_nohash_0_exercise_bike_noise_snr0.npy  (50)\n",
            "Copied: zero_3ab9ba07_nohash_0_exercise_bike_noise_snr0.npy  (51)\n",
            "Copied: zero_982babaf_nohash_0_exercise_bike_noise_snr0.npy  (52)\n",
            "Copied: zero_ad5aeec2_nohash_3_exercise_bike_noise_snr0.npy  (53)\n",
            "Copied: zero_21832144_nohash_4_exercise_bike_noise_snr0.npy  (54)\n",
            "Copied: zero_66a412a7_nohash_1_exercise_bike_noise_snr0.npy  (55)\n",
            "Copied: zero_c7b4049e_nohash_0_exercise_bike_noise_snr0.npy  (56)\n",
            "Copied: zero_c120e80e_nohash_5_exercise_bike_noise_snr0.npy  (57)\n",
            "Copied: zero_0447d7c1_nohash_1_exercise_bike_noise_snr0.npy  (58)\n",
            "Copied: zero_a42a88ff_nohash_2_exercise_bike_noise_snr0.npy  (59)\n",
            "Copied: zero_8c7c9168_nohash_0_exercise_bike_noise_snr0.npy  (60)\n",
            "Copied: zero_0b56bcfe_nohash_0_exercise_bike_noise_snr0.npy  (61)\n",
            "Copied: zero_012c8314_nohash_1_exercise_bike_noise_snr0.npy  (62)\n",
            "Copied: zero_0bd689d7_nohash_0_exercise_bike_noise_snr0.npy  (63)\n",
            "Copied: zero_617de221_nohash_4_exercise_bike_noise_snr0.npy  (64)\n",
            "Copied: zero_6727b579_nohash_0_exercise_bike_noise_snr0.npy  (65)\n",
            "Copied: zero_cc6ee39b_nohash_1_exercise_bike_noise_snr0.npy  (66)\n",
            "Copied: zero_2bdbe5f7_nohash_1_exercise_bike_noise_snr0.npy  (67)\n",
            "Copied: zero_d1a17cbe_nohash_0_exercise_bike_noise_snr0.npy  (68)\n",
            "Copied: zero_a5b24175_nohash_0_exercise_bike_noise_snr0.npy  (69)\n",
            "Copied: zero_76b58705_nohash_2_exercise_bike_noise_snr0.npy  (70)\n",
            "Copied: zero_988e2f9a_nohash_1_exercise_bike_noise_snr0.npy  (71)\n",
            "Copied: zero_eb3f7d82_nohash_3_exercise_bike_noise_snr0.npy  (72)\n",
            "Copied: zero_c79159aa_nohash_1_exercise_bike_noise_snr0.npy  (73)\n",
            "Copied: zero_2579e514_nohash_0_exercise_bike_noise_snr0.npy  (74)\n",
            "Copied: zero_01bb6a2a_nohash_3_exercise_bike_noise_snr0.npy  (75)\n",
            "Copied: zero_96a48d28_nohash_0_exercise_bike_noise_snr0.npy  (76)\n",
            "Copied: zero_8012c69d_nohash_1_exercise_bike_noise_snr0.npy  (77)\n",
            "Copied: zero_9e6bb505_nohash_1_exercise_bike_noise_snr0.npy  (78)\n",
            "Copied: zero_db8a3397_nohash_0_exercise_bike_noise_snr0.npy  (79)\n",
            "Copied: zero_1cec8d71_nohash_0_exercise_bike_noise_snr0.npy  (80)\n",
            "Copied: zero_b43de700_nohash_0_exercise_bike_noise_snr0.npy  (81)\n",
            "Copied: zero_a331d9cb_nohash_2_exercise_bike_noise_snr0.npy  (82)\n",
            "Copied: zero_ccfd721c_nohash_0_exercise_bike_noise_snr0.npy  (83)\n",
            "Copied: zero_772db621_nohash_1_exercise_bike_noise_snr0.npy  (84)\n",
            "Copied: zero_89865a6f_nohash_0_exercise_bike_noise_snr0.npy  (85)\n",
            "Copied: zero_d90b4138_nohash_1_exercise_bike_noise_snr0.npy  (86)\n",
            "Copied: zero_b12bef84_nohash_1_exercise_bike_noise_snr0.npy  (87)\n",
            "Copied: zero_70a00e98_nohash_2_exercise_bike_noise_snr0.npy  (88)\n",
            "Copied: zero_1b755c65_nohash_0_exercise_bike_noise_snr0.npy  (89)\n",
            "Copied: zero_aac5b7c1_nohash_2_exercise_bike_noise_snr0.npy  (90)\n",
            "Copied: zero_106a6183_nohash_0_exercise_bike_noise_snr0.npy  (91)\n",
            "Copied: zero_8a28231e_nohash_0_exercise_bike_noise_snr0.npy  (92)\n",
            "Copied: zero_51055bda_nohash_4_exercise_bike_noise_snr0.npy  (93)\n",
            "Copied: zero_3e31dffe_nohash_2_exercise_bike_noise_snr0.npy  (94)\n",
            "Copied: zero_02746d24_nohash_0_exercise_bike_noise_snr0.npy  (95)\n",
            "Copied: zero_c79159aa_nohash_4_exercise_bike_noise_snr0.npy  (96)\n",
            "Copied: zero_735845ab_nohash_4_exercise_bike_noise_snr0.npy  (97)\n",
            "Copied: zero_14587ff0_nohash_0_exercise_bike_noise_snr0.npy  (98)\n",
            "Copied: zero_1ed557b9_nohash_0_exercise_bike_noise_snr0.npy  (99)\n",
            "Copied: zero_c4500713_nohash_1_exercise_bike_noise_snr0.npy  (100)\n",
            "Copied: zero_69086eb0_nohash_0_exercise_bike_noise_snr0.npy  (101)\n",
            "Copied: zero_1625acd8_nohash_0_exercise_bike_noise_snr0.npy  (102)\n",
            "Copied: zero_d1a17cbe_nohash_1_exercise_bike_noise_snr0.npy  (103)\n",
            "Copied: zero_c1e0e8e3_nohash_3_exercise_bike_noise_snr0.npy  (104)\n",
            "Copied: zero_bd8412df_nohash_0_exercise_bike_noise_snr0.npy  (105)\n",
            "Copied: zero_c0445658_nohash_4_exercise_bike_noise_snr0.npy  (106)\n",
            "Copied: zero_0bd689d7_nohash_2_exercise_bike_noise_snr0.npy  (107)\n",
            "Copied: zero_126a31d2_nohash_0_exercise_bike_noise_snr0.npy  (108)\n",
            "Copied: zero_f2a90886_nohash_0_exercise_bike_noise_snr0.npy  (109)\n",
            "Copied: zero_becd5a53_nohash_0_exercise_bike_noise_snr0.npy  (110)\n",
            "Copied: zero_413bcfe1_nohash_0_exercise_bike_noise_snr0.npy  (111)\n",
            "Copied: zero_15574821_nohash_2_exercise_bike_noise_snr0.npy  (112)\n",
            "Copied: zero_f7879738_nohash_0_exercise_bike_noise_snr0.npy  (113)\n",
            "Copied: zero_f6581345_nohash_1_exercise_bike_noise_snr0.npy  (114)\n",
            "Copied: zero_afb9e62e_nohash_0_exercise_bike_noise_snr0.npy  (115)\n",
            "Copied: zero_da8fa823_nohash_0_exercise_bike_noise_snr0.npy  (116)\n",
            "Copied: zero_a3255f5c_nohash_0_exercise_bike_noise_snr0.npy  (117)\n",
            "Copied: zero_a0f93943_nohash_0_exercise_bike_noise_snr0.npy  (118)\n",
            "Copied: zero_b1114e4f_nohash_0_exercise_bike_noise_snr0.npy  (119)\n",
            "Copied: zero_6aafb34f_nohash_0_exercise_bike_noise_snr0.npy  (120)\n",
            "Copied: zero_1cec8d71_nohash_1_exercise_bike_noise_snr0.npy  (121)\n",
            "Copied: zero_a13e0a74_nohash_0_exercise_bike_noise_snr0.npy  (122)\n",
            "Copied: zero_b06c19b0_nohash_0_exercise_bike_noise_snr0.npy  (123)\n",
            "Copied: zero_15574821_nohash_3_exercise_bike_noise_snr0.npy  (124)\n",
            "Copied: zero_7b301939_nohash_0_exercise_bike_noise_snr0.npy  (125)\n",
            "Copied: zero_b7669804_nohash_1_exercise_bike_noise_snr0.npy  (126)\n",
            "Copied: zero_3e31dffe_nohash_4_exercise_bike_noise_snr0.npy  (127)\n",
            "Copied: zero_28ce0c58_nohash_2_exercise_bike_noise_snr0.npy  (128)\n",
            "Copied: zero_418e7158_nohash_0_exercise_bike_noise_snr0.npy  (129)\n",
            "Copied: zero_b59fa113_nohash_1_exercise_bike_noise_snr0.npy  (130)\n",
            "Copied: zero_38d78313_nohash_4_exercise_bike_noise_snr0.npy  (131)\n",
            "Copied: zero_ffd2ba2f_nohash_2_exercise_bike_noise_snr0.npy  (132)\n",
            "Copied: zero_8dd788d8_nohash_1_exercise_bike_noise_snr0.npy  (133)\n",
            "Copied: zero_9acd0254_nohash_1_exercise_bike_noise_snr0.npy  (134)\n",
            "Copied: zero_aba19127_nohash_2_exercise_bike_noise_snr0.npy  (135)\n",
            "Copied: zero_76e53db9_nohash_0_exercise_bike_noise_snr0.npy  (136)\n",
            "Copied: zero_cb2929ce_nohash_0_exercise_bike_noise_snr0.npy  (137)\n",
            "Copied: zero_15c563d7_nohash_4_exercise_bike_noise_snr0.npy  (138)\n",
            "Copied: zero_190821dc_nohash_2_exercise_bike_noise_snr0.npy  (139)\n",
            "Copied: zero_da584bc0_nohash_4_exercise_bike_noise_snr0.npy  (140)\n",
            "Copied: zero_cb2929ce_nohash_2_exercise_bike_noise_snr0.npy  (141)\n",
            "Copied: zero_db9cd41d_nohash_0_exercise_bike_noise_snr0.npy  (142)\n",
            "Copied: zero_b7a0754f_nohash_3_exercise_bike_noise_snr0.npy  (143)\n",
            "Copied: zero_21e8c417_nohash_0_exercise_bike_noise_snr0.npy  (144)\n",
            "Copied: zero_df6bd83f_nohash_0_exercise_bike_noise_snr0.npy  (145)\n",
            "Copied: zero_f852895b_nohash_1_exercise_bike_noise_snr0.npy  (146)\n",
            "Copied: zero_b9515bf3_nohash_3_exercise_bike_noise_snr0.npy  (147)\n",
            "Copied: zero_89f3ab7d_nohash_0_exercise_bike_noise_snr0.npy  (148)\n",
            "Copied: zero_e102119e_nohash_0_exercise_bike_noise_snr0.npy  (149)\n",
            "Copied: zero_44f68a83_nohash_0_exercise_bike_noise_snr0.npy  (150)\n",
            "Copied: zero_72320401_nohash_0_exercise_bike_noise_snr0.npy  (151)\n",
            "Copied: zero_b087aa0e_nohash_0_exercise_bike_noise_snr0.npy  (152)\n",
            "Copied: zero_73124b26_nohash_0_exercise_bike_noise_snr0.npy  (153)\n",
            "Copied: zero_2b3f509b_nohash_0_exercise_bike_noise_snr0.npy  (154)\n",
            "Copied: zero_3c257192_nohash_2_exercise_bike_noise_snr0.npy  (155)\n",
            "Copied: zero_21832144_nohash_0_exercise_bike_noise_snr0.npy  (156)\n",
            "Copied: zero_cdbd6969_nohash_0_exercise_bike_noise_snr0.npy  (157)\n",
            "Copied: zero_61d3e51e_nohash_0_exercise_bike_noise_snr0.npy  (158)\n",
            "Copied: zero_fc28c8d8_nohash_0_exercise_bike_noise_snr0.npy  (159)\n",
            "Copied: zero_8281a2a8_nohash_3_exercise_bike_noise_snr0.npy  (160)\n",
            "Copied: zero_f9af823e_nohash_0_exercise_bike_noise_snr0.npy  (161)\n",
            "Copied: zero_24befdb3_nohash_4_exercise_bike_noise_snr0.npy  (162)\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Keyword_spotting_transformer/copy_files.py\", line 14, in <module>\n",
            "    shutil.copy2(os.path.join(source_folder, file), destination_folder)\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 475, in copy2\n",
            "    copyfile(src, dst, follow_symlinks=follow_symlinks)\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 273, in copyfile\n",
            "    _fastcopy_sendfile(fsrc, fdst)\n",
            "  File \"/usr/lib/python3.12/shutil.py\", line 150, in _fastcopy_sendfile\n",
            "    sent = os.sendfile(outfd, infd, offset, blocksize)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python copy_files.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYHjUE9nRHRJ",
        "outputId": "7dfa5cdb-4bce-44c7-8f4b-73ee9238875b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spectrogram Shape: (40, 101)\n",
            "Mean Value: -8.0343\n",
            "Standard Deviation: 3.8233\n"
          ]
        }
      ],
      "source": [
        "!python feature_visualization.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL-jQZFkVK3_",
        "outputId": "f13ed71f-6e18-42eb-9540-d63068cd263e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Figure(600x200)\n",
            " Spectrogram saved at: /content/drive/MyDrive/Keyword_spotting_transformer/plots/up_e5e54cee_nohash_0.png\n"
          ]
        }
      ],
      "source": [
        "!python plot_spectrograms.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "G5rN-D9lwPTq",
        "outputId": "abc43ade-1d7c-4f12-e607-3d201576be76"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 600x200 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "import os\n",
        "\n",
        "# Path to your .wav file\n",
        "file_path = '/content/drive/MyDrive/Keyword_spotting_transformer/recorded_audio_files/wav_files/up right down left up right down left.wav'  # ← replace with your actual file\n",
        "\n",
        "# Load audio\n",
        "sr, data = wav.read(file_path)\n",
        "\n",
        "# Convert stereo to mono if needed\n",
        "if data.ndim > 1:\n",
        "    data = data[:, 0]\n",
        "\n",
        "# Create time axis\n",
        "time = np.linspace(0, len(data) / sr, num=len(data))\n",
        "\n",
        "# Extract just the filename (without path)\n",
        "file_name = os.path.basename(file_path)\n",
        "\n",
        "# Plot waveform\n",
        "plt.figure(figsize=(6, 2))\n",
        "plt.plot(time, data, color='steelblue')\n",
        "plt.title(f'Waveform: {file_name}')\n",
        "plt.xlabel('Time [s]')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9Rk351uY4lT",
        "outputId": "cf4270c2-d2aa-4ae5-fde6-555b03ddc904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Processing custom audio...\n",
            " Processed & Saved: /content/drive/MyDrive/Keyword_spotting_transformer/recorded_audio_files/npy_files/Recorded.npy\n",
            "\n",
            "🎯 Predicted Keyword: house (Confidence: 91.15%)\n",
            "Figure(1000x400)\n"
          ]
        }
      ],
      "source": [
        "!python testing_real_audio.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-C3lIqOGlKC",
        "outputId": "29811711-2523-4588-ed0b-4afa700fd2ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/VAD\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/VAD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yCQPoRHORdTo",
        "outputId": "aba799d1-11d6-4783-b0dd-cb7ab91d4a10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/snakers4/silero-vad.git\n",
            "  Cloning https://github.com/snakers4/silero-vad.git to /tmp/pip-req-build-jr6ir69a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/snakers4/silero-vad.git /tmp/pip-req-build-jr6ir69a\n",
            "  Resolved https://github.com/snakers4/silero-vad.git to commit 0dd45f0bcd7271463c234f3bae5ad25181f9df8b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime>=1.16.1 (from silero-vad==5.1.2)\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from silero-vad==5.1.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from silero-vad==5.1.2) (2.6.0+cu124)\n",
            "Collecting coloredlogs (from onnxruntime>=1.16.1->silero-vad==5.1.2)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.16.1->silero-vad==5.1.2) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.16.1->silero-vad==5.1.2) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.16.1->silero-vad==5.1.2) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.16.1->silero-vad==5.1.2) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.16.1->silero-vad==5.1.2) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->silero-vad==5.1.2) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.16.1->silero-vad==5.1.2) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.16.1->silero-vad==5.1.2)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->silero-vad==5.1.2) (3.0.2)\n",
            "Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: silero-vad\n",
            "  Building wheel for silero-vad (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for silero-vad: filename=silero_vad-5.1.2-py3-none-any.whl size=6114545 sha256=4e038b631f770f3d6eaf6c459944ded59e3239ff0dccff066d477d0f92f13b33\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dnp38yv0/wheels/1c/1c/5e/8c174c41c5c4f7ee115aa59b0f567a87cd5152e4010ed500d2\n",
            "Successfully built silero-vad\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime, silero-vad\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.21.1 silero-vad-5.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/snakers4/silero-vad.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "alVmBeXNB_t7",
        "outputId": "a1cf6ebc-0f05-4042-e3cc-defe9582ba58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5GW7apkMZ6F",
        "outputId": "b08195ff-62f2-47a8-b143-2062bdd898fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "✅ Plot saved at: /content/drive/MyDrive/Keyword_spotting_transformer/plots/VAD_silero_plot.png\n",
            "Figure(1400x500)\n"
          ]
        }
      ],
      "source": [
        "!python VAD_silero.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9o9aFGgRTf9",
        "outputId": "89c7fe3f-4261-45ce-e6f5-dc63719e766c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/snakers4_silero-vad_master\n",
            "\n",
            " Detected Keywords:\n",
            "--------------------------------------------------\n",
            "'on' at 1.70s - 2.70s\n",
            "'on' at 2.20s - 3.20s\n",
            "'left' at 3.23s - 4.23s\n",
            "'right' at 4.87s - 5.87s\n",
            "Figure(1400x500)\n",
            "Plot saved and keyword detection completed!\n"
          ]
        }
      ],
      "source": [
        "!python KWTxVAD_silero.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E0hq-g_bqb5",
        "outputId": "1ca397fa-3b2d-40fb-81fe-9c89d7c819f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1DwcL7pbLig",
        "outputId": "b42383f0-00ff-490e-f084-f9a9123b4427"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🚀 Processing custom audio...\n",
            " Processed & Saved: /content/drive/MyDrive/Keyword_spotting_transformer/recorded_audio_files/npy_files/Recorded.npy\n",
            "\n",
            "🎯 Predicted Keyword: down (Confidence: 8.83%)\n",
            "Figure(1000x400)\n"
          ]
        }
      ],
      "source": [
        "!python testing_real_audio.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE5ag9CKBmJm"
      },
      "source": [
        "**TRAINING THE MODEL WITH MIXED DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nGeAVXQDyfX",
        "outputId": "cef2bb65-3291-4c7a-c05f-ab3623915beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Eco3_GJlSEN",
        "outputId": "a270e5ff-d2ed-48ac-f533-7802176a6ee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVwKcZgYj4oj",
        "outputId": "9be48bab-2c64-46de-c097-8f658e3c7ac7",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-2150264935.py\", line 2, in <cell line: 0>\n",
            "    from config import TEST_DIRS\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1322, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1262, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1532, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1504, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1483, in _path_importer_cache\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1714, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 970, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 999, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 983, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 415, in abspath\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-2150264935.py\", line 2, in <cell line: 0>\n",
            "    from config import TEST_DIRS\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1322, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1262, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1532, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1504, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1483, in _path_importer_cache\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1714, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 970, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 999, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 983, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 415, in abspath\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-2150264935.py\", line 2, in <cell line: 0>\n",
            "    from config import TEST_DIRS\n",
            "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1322, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 1262, in _find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1532, in find_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1504, in _get_spec\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1483, in _path_importer_cache\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'OSError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 1714, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 970, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 999, in getmodule\n",
            "    file = getabsfile(object, _filename)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/inspect.py\", line 983, in getabsfile\n",
            "    return os.path.normcase(os.path.abspath(_filename))\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 415, in abspath\n",
            "OSError: [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from config import TEST_DIRS\n",
        "for d in TEST_DIRS:\n",
        "    if os.path.exists(d):\n",
        "        print(d, \":\", len([f for f in os.listdir(d) if f.endswith(\".npy\")]), \"files\")\n",
        "    else:\n",
        "        print(d, \" ❌ directory not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wjhzx0ZtUFo",
        "outputId": "078976b9-c43a-44b7-f564-5b3942ee46f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: Transport endpoint is not connected\n",
            "python3: can't open file 'data_structuring.py': [Errno 107] Transport endpoint is not connected\n"
          ]
        }
      ],
      "source": [
        "!python data_structuring.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWOjaDx7Blc1",
        "outputId": "d7603d2e-632f-4b7e-a42c-665342862b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Keyword_spotting_transformer/train.py\", line 36, in <module>\n",
            "    subset_loader = DataLoader(subset_train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\", line 388, in __init__\n",
            "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/data/sampler.py\", line 156, in __init__\n",
            "    raise ValueError(\n",
            "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PDyclxOn3iz",
        "outputId": "cffdc5fa-ee99-4e4b-b54b-a5e7280baaa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampling rate: 16000 Hz\n"
          ]
        }
      ],
      "source": [
        "import torchaudio\n",
        "waveform, sample_rate = torchaudio.load(\"/content/drive/MyDrive/Keyword_spotting_transformer/data/train/bed/0a7c2a8d_nohash_0.wav\")\n",
        "print(\"Sampling rate:\", sample_rate, \"Hz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oYqnxjcOBtm"
      },
      "source": [
        "**START OF HDL PROJECT USING KEYWORD SPOTTING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9pKWKibn_ot",
        "outputId": "e7429ff5-9d7b-4300-cbd8-cb9b4e97d2c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0uflbz_7QeGI",
        "outputId": "42e95c9a-15d2-4182-b37d-887eff916342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting dataset loading process...\n",
            "\n",
            "🔄 No cache found. Loading raw data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/train_processed\n",
            "🔹 Loaded 500 spectrograms...\n",
            "🔹 Loaded 1000 spectrograms...\n",
            "🔹 Loaded 1500 spectrograms...\n",
            "🔹 Loaded 2000 spectrograms...\n",
            "🔹 Loaded 2500 spectrograms...\n",
            "🔹 Loaded 3000 spectrograms...\n",
            "🔹 Loaded 3500 spectrograms...\n",
            "🔹 Loaded 4000 spectrograms...\n",
            "🔹 Loaded 4500 spectrograms...\n",
            "🔹 Loaded 5000 spectrograms...\n",
            "🔹 Loaded 5500 spectrograms...\n",
            "🔹 Loaded 6000 spectrograms...\n",
            "🔹 Loaded 6500 spectrograms...\n",
            "🔹 Loaded 7000 spectrograms...\n",
            "🔹 Loaded 7500 spectrograms...\n",
            "🔹 Loaded 8000 spectrograms...\n",
            "🔹 Loaded 8500 spectrograms...\n",
            "🔹 Loaded 9000 spectrograms...\n",
            "🔹 Loaded 9500 spectrograms...\n",
            "🔹 Loaded 10000 spectrograms...\n",
            "🔹 Loaded 10500 spectrograms...\n",
            "🔹 Loaded 11000 spectrograms...\n",
            "🔹 Loaded 11500 spectrograms...\n",
            "🔹 Loaded 12000 spectrograms...\n",
            "🔹 Loaded 12500 spectrograms...\n",
            "🔹 Loaded 13000 spectrograms...\n",
            "🔹 Loaded 13500 spectrograms...\n",
            "🔹 Loaded 14000 spectrograms...\n",
            "\n",
            "💾 Caching 14000 samples to train_data.npz\n",
            "✅ Done caching!\n",
            "\n",
            "🔄 No cache found. Loading raw data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/valid_processed\n",
            "🔹 Loaded 500 spectrograms...\n",
            "🔹 Loaded 1000 spectrograms...\n",
            "🔹 Loaded 1500 spectrograms...\n",
            "🔹 Loaded 2000 spectrograms...\n",
            "🔹 Loaded 2500 spectrograms...\n",
            "🔹 Loaded 3000 spectrograms...\n",
            "🔹 Loaded 3500 spectrograms...\n",
            "\n",
            "💾 Caching 3549 samples to val_data.npz\n",
            "✅ Done caching!\n",
            "\n",
            "🔄 No cache found. Loading raw data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/test_processed\n",
            "🔹 Loaded 500 spectrograms...\n",
            "🔹 Loaded 1000 spectrograms...\n",
            "🔹 Loaded 1500 spectrograms...\n",
            "🔹 Loaded 2000 spectrograms...\n",
            "🔹 Loaded 2500 spectrograms...\n",
            "🔹 Loaded 3000 spectrograms...\n",
            "🔹 Loaded 3500 spectrograms...\n",
            "\n",
            "💾 Caching 3585 samples to test_data.npz\n",
            "✅ Done caching!\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (14000, 40, 101, 1), y = (14000,)\n",
            "Val:   X = (3549, 40, 101, 1), y = (3549,)\n",
            "Test:  X = (3585, 40, 101, 1), y = (3585,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n"
          ]
        }
      ],
      "source": [
        "!python data_loader.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eyJZU5FN-iD"
      },
      "source": [
        "FOR HDL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmX9oZOdjrDd",
        "outputId": "85eaee08-67d5-46f3-ae14-c7de34a3c610"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-10 07:16:42.982676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744269403.054151   66692 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744269403.079247   66692 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-10 07:16:43.799783: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "🚀 Starting dataset loading process...\n",
            "📦 Found cache: train_data.npz — Loading...\n",
            "✅ Loaded 14000 samples from cache.\n",
            "📦 Found cache: val_data.npz — Loading...\n",
            "✅ Loaded 3549 samples from cache.\n",
            "📦 Found cache: test_data.npz — Loading...\n",
            "✅ Loaded 3585 samples from cache.\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (14000, 40, 101, 1), y = (14000,)\n",
            "Val:   X = (3549, 40, 101, 1), y = (3549,)\n",
            "Test:  X = (3585, 40, 101, 1), y = (3585,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "2025-04-10 07:17:03.849848: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "🚀 Starting training...\n",
            "\n",
            "Epoch 1/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.2645 - loss: 2.2972\n",
            "Epoch 1: val_accuracy improved from -inf to 0.78416, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 97ms/step - accuracy: 0.2647 - loss: 2.2965 - val_accuracy: 0.7842 - val_loss: 0.7036\n",
            "Epoch 2/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.7687 - loss: 0.7265\n",
            "Epoch 2: val_accuracy improved from 0.78416 to 0.87997, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 100ms/step - accuracy: 0.7687 - loss: 0.7264 - val_accuracy: 0.8800 - val_loss: 0.4226\n",
            "Epoch 3/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8685 - loss: 0.4080\n",
            "Epoch 3: val_accuracy improved from 0.87997 to 0.88757, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 94ms/step - accuracy: 0.8685 - loss: 0.4079 - val_accuracy: 0.8876 - val_loss: 0.3818\n",
            "Epoch 4/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9123 - loss: 0.2733\n",
            "Epoch 4: val_accuracy improved from 0.88757 to 0.91293, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 96ms/step - accuracy: 0.9123 - loss: 0.2733 - val_accuracy: 0.9129 - val_loss: 0.3301\n",
            "Epoch 5/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9326 - loss: 0.2108\n",
            "Epoch 5: val_accuracy did not improve from 0.91293\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 93ms/step - accuracy: 0.9326 - loss: 0.2108 - val_accuracy: 0.9017 - val_loss: 0.3746\n",
            "Epoch 6/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9446 - loss: 0.1717\n",
            "Epoch 6: val_accuracy improved from 0.91293 to 0.91321, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 93ms/step - accuracy: 0.9446 - loss: 0.1717 - val_accuracy: 0.9132 - val_loss: 0.3661\n",
            "Epoch 7/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9475 - loss: 0.1661\n",
            "Epoch 7: val_accuracy improved from 0.91321 to 0.91941, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 100ms/step - accuracy: 0.9474 - loss: 0.1661 - val_accuracy: 0.9194 - val_loss: 0.3110\n",
            "Epoch 8/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9572 - loss: 0.1330\n",
            "Epoch 8: val_accuracy improved from 0.91941 to 0.92195, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 99ms/step - accuracy: 0.9572 - loss: 0.1330 - val_accuracy: 0.9219 - val_loss: 0.3715\n",
            "Epoch 9/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9622 - loss: 0.1203\n",
            "Epoch 9: val_accuracy did not improve from 0.92195\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 92ms/step - accuracy: 0.9622 - loss: 0.1203 - val_accuracy: 0.9146 - val_loss: 0.3130\n",
            "Epoch 10/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9654 - loss: 0.1147\n",
            "Epoch 10: val_accuracy improved from 0.92195 to 0.92815, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 98ms/step - accuracy: 0.9654 - loss: 0.1148 - val_accuracy: 0.9281 - val_loss: 0.3251\n",
            "Epoch 11/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9640 - loss: 0.1087\n",
            "Epoch 11: val_accuracy improved from 0.92815 to 0.93688, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 94ms/step - accuracy: 0.9640 - loss: 0.1087 - val_accuracy: 0.9369 - val_loss: 0.3500\n",
            "Epoch 12/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9662 - loss: 0.1026\n",
            "Epoch 12: val_accuracy did not improve from 0.93688\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 93ms/step - accuracy: 0.9662 - loss: 0.1026 - val_accuracy: 0.9211 - val_loss: 0.3756\n",
            "Epoch 13/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9710 - loss: 0.0920\n",
            "Epoch 13: val_accuracy did not improve from 0.93688\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 94ms/step - accuracy: 0.9710 - loss: 0.0920 - val_accuracy: 0.9352 - val_loss: 0.3329\n",
            "Epoch 14/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9710 - loss: 0.0969\n",
            "Epoch 14: val_accuracy did not improve from 0.93688\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 93ms/step - accuracy: 0.9710 - loss: 0.0969 - val_accuracy: 0.9318 - val_loss: 0.3663\n",
            "Epoch 15/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9714 - loss: 0.0948\n",
            "Epoch 15: val_accuracy did not improve from 0.93688\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 93ms/step - accuracy: 0.9714 - loss: 0.0948 - val_accuracy: 0.9310 - val_loss: 0.3731\n",
            "Epoch 16/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9751 - loss: 0.0835\n",
            "Epoch 16: val_accuracy did not improve from 0.93688\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 94ms/step - accuracy: 0.9751 - loss: 0.0836 - val_accuracy: 0.9338 - val_loss: 0.3075\n",
            "Epoch 17/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9774 - loss: 0.0732\n",
            "Epoch 17: val_accuracy improved from 0.93688 to 0.93717, saving model to models/binary_cnn_best.keras\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 93ms/step - accuracy: 0.9774 - loss: 0.0732 - val_accuracy: 0.9372 - val_loss: 0.3513\n",
            "Epoch 18/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9760 - loss: 0.0766\n",
            "Epoch 18: val_accuracy did not improve from 0.93717\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 93ms/step - accuracy: 0.9760 - loss: 0.0766 - val_accuracy: 0.9355 - val_loss: 0.4006\n",
            "Epoch 19/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9771 - loss: 0.0796\n",
            "Epoch 19: val_accuracy did not improve from 0.93717\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 94ms/step - accuracy: 0.9771 - loss: 0.0796 - val_accuracy: 0.9310 - val_loss: 0.3391\n",
            "Epoch 20/20\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9795 - loss: 0.0586\n",
            "Epoch 20: val_accuracy did not improve from 0.93717\n",
            "\u001b[1m875/875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 94ms/step - accuracy: 0.9795 - loss: 0.0586 - val_accuracy: 0.9338 - val_loss: 0.4219\n",
            "✅ Training complete!\n",
            "💾 Saving model architecture and weights...\n",
            "✅ Saved as JSON + H5\n",
            "📊 Saving sample input/output for hls4ml...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step\n",
            "📊 Done.\n",
            "🛠️ Generating config file for hls4ml...\n",
            "✅ hls4ml config saved.\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV91Knrnjtgu",
        "outputId": "9fd16b97-8b86-4579-caf3-d890525fdbd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-10 08:06:26.723730: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744272386.760141   79015 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744272386.770986   79015 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-10 08:06:26.819263: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "🚀 Starting dataset loading process...\n",
            "📦 Found cache: train_data.npz — Loading...\n",
            "✅ Loaded 14000 samples from cache.\n",
            "📦 Found cache: val_data.npz — Loading...\n",
            "✅ Loaded 3549 samples from cache.\n",
            "📦 Found cache: test_data.npz — Loading...\n",
            "✅ Loaded 3585 samples from cache.\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (14000, 40, 101, 1), y = (14000,)\n",
            "Val:   X = (3549, 40, 101, 1), y = (3549,)\n",
            "Test:  X = (3585, 40, 101, 1), y = (3585,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n",
            "📦 Loading model architecture and weights...\n",
            "2025-04-10 08:06:43.117291: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "✅ Model loaded successfully.\n",
            "\n",
            "🔍 Evaluating on test data...\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.9388 - loss: 0.4173\n",
            "\n",
            "✅ Test Accuracy: 93.81%\n",
            "✅ Test Loss: 0.3980\n",
            "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 47ms/step\n",
            "\n",
            "📊 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        zero       0.95      0.92      0.93       250\n",
            "         one       0.94      0.95      0.95       248\n",
            "         two       0.96      0.93      0.94       264\n",
            "       three       0.96      0.95      0.95       267\n",
            "        four       0.96      0.94      0.95       253\n",
            "        five       0.92      0.93      0.93       271\n",
            "         six       0.94      0.95      0.94       244\n",
            "       seven       0.88      0.98      0.93       239\n",
            "       eight       0.93      0.93      0.93       257\n",
            "        nine       0.95      0.93      0.94       259\n",
            "          up       0.91      0.96      0.93       272\n",
            "        down       0.91      0.93      0.92       253\n",
            "         yes       0.98      0.95      0.96       256\n",
            "          no       0.93      0.90      0.92       252\n",
            "\n",
            "    accuracy                           0.94      3585\n",
            "   macro avg       0.94      0.94      0.94      3585\n",
            "weighted avg       0.94      0.94      0.94      3585\n",
            "\n",
            "✅ Confusion matrix saved to: /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/plots/cnn_kwt_confusion_matrix.png\n",
            "Figure(1200x800)\n"
          ]
        }
      ],
      "source": [
        "!python test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jry4rDHTvDra",
        "outputId": "0c633662-68b0-492a-a940-e36714596273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-10 09:51:54.386171: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744278714.426878  104636 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744278714.439208  104636 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-10 09:51:54.478165: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "🚀 Starting dataset loading process...\n",
            "📦 Found cache: train_data.npz — Loading...\n",
            "✅ Loaded 14000 samples from cache.\n",
            "📦 Found cache: val_data.npz — Loading...\n",
            "✅ Loaded 3549 samples from cache.\n",
            "📦 Found cache: test_data.npz — Loading...\n",
            "✅ Loaded 3585 samples from cache.\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (14000, 40, 101, 1), y = (14000,)\n",
            "Val:   X = (3549, 40, 101, 1), y = (3549,)\n",
            "Test:  X = (3585, 40, 101, 1), y = (3585,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n",
            "2025-04-10 09:52:05.783332: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "✅ Model loaded.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\n",
            "\n",
            "🎙️ Prediction: seven (Confidence: 0.69)\n"
          ]
        }
      ],
      "source": [
        "!python predict_audio.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoK50Y6y83kH",
        "outputId": "36973834-d821-4b31-e8ed-06e809d90827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/augmentation\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB7rXv419cp3",
        "outputId": "9e4d73fb-08a6-4cff-8d0c-8ba9d4cdb6f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "augment_single_file.py  augment_with_backgroundnoise.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6b5ELbr8y2k"
      },
      "source": [
        "**DATA AUGMENTATION**\n",
        "with pink_noise at 10dB, white noise at 0dB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpM1t7kLyjM7",
        "outputId": "b3ebef09-e0bb-44a3-e8d2-9add2282e2a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Loading background noise from: /content/drive/MyDrive/Keyword_spotting_transformer/data/background_noises/exercise_bike.wav\n",
            "\n",
            " Augmenting: /content/drive/MyDrive/Keyword_spotting_transformer/data/train → /content/drive/MyDrive/Keyword_spotting_transformer/data/train_augmented_exercise_bike\n",
            "Processing train/bed: 100% 1358/1358 [01:51<00:00, 12.14it/s]\n",
            "Processing train/bird: 100% 1421/1421 [01:08<00:00, 20.78it/s]\n",
            "Processing train/cat: 100% 1409/1409 [01:05<00:00, 21.63it/s]\n",
            "Processing train/dog: 100% 1396/1396 [01:05<00:00, 21.30it/s]\n",
            "Processing train/down: 100% 1852/1852 [01:33<00:00, 19.80it/s]\n",
            "Processing train/eight: 100% 1852/1852 [01:41<00:00, 18.32it/s]\n",
            "Processing train/five: 100% 1861/1861 [01:34<00:00, 19.59it/s]\n",
            "Processing train/four: 100% 1899/1899 [01:38<00:00, 19.35it/s]\n",
            "Processing train/go: 100% 1881/1881 [01:39<00:00, 18.99it/s]\n",
            "Processing train/happy: 100% 1373/1373 [01:04<00:00, 21.16it/s]\n",
            "Processing train/house: 100% 1427/1427 [01:09<00:00, 20.41it/s]\n",
            "Processing train/left: 100% 1839/1839 [01:35<00:00, 19.17it/s]\n",
            "Processing train/marvin: 100% 1444/1444 [01:11<00:00, 20.26it/s]\n",
            "Processing train/nine: 100% 1875/1875 [01:44<00:00, 17.86it/s]\n",
            "Processing train/no: 100% 1853/1853 [01:34<00:00, 19.57it/s]\n",
            "Processing train/off: 100% 1839/1839 [01:40<00:00, 18.33it/s]\n",
            "Processing train/on: 100% 1884/1884 [01:41<00:00, 18.58it/s]\n",
            "Processing train/one: 100% 1892/1892 [01:39<00:00, 19.05it/s]\n",
            "Processing train/right: 100% 1852/1852 [01:35<00:00, 19.36it/s]\n",
            "Processing train/seven: 100% 1876/1876 [01:36<00:00, 19.41it/s]\n",
            "Processing train/sheila: 100% 1382/1382 [01:03<00:00, 21.70it/s]\n",
            "Processing train/six: 100% 1863/1863 [01:43<00:00, 17.95it/s]\n",
            "Processing train/stop: 100% 1895/1895 [01:38<00:00, 19.27it/s]\n",
            "Processing train/three: 100% 1851/1851 [01:34<00:00, 19.55it/s]\n",
            "Processing train/tree: 100% 1374/1374 [01:05<00:00, 20.83it/s]\n",
            "Processing train/two: 100% 1873/1873 [01:33<00:00, 20.09it/s]\n",
            "Processing train/up: 100% 1857/1857 [01:34<00:00, 19.55it/s]\n",
            "Processing train/wow: 100% 1414/1414 [01:07<00:00, 20.93it/s]\n",
            "Processing train/yes: 100% 1860/1860 [01:43<00:00, 18.02it/s]\n",
            "Processing train/zero: 100% 1866/1866 [01:36<00:00, 19.35it/s]\n",
            "\n",
            " Augmenting: /content/drive/MyDrive/Keyword_spotting_transformer/data/valid → /content/drive/MyDrive/Keyword_spotting_transformer/data/valid_augmented_exercise_bike\n",
            "Processing valid/bed: 100% 197/197 [00:09<00:00, 20.92it/s]\n",
            "Processing valid/bird: 100% 162/162 [00:06<00:00, 26.95it/s]\n",
            "Processing valid/cat: 100% 168/168 [00:06<00:00, 26.38it/s]\n",
            "Processing valid/dog: 100% 170/170 [00:06<00:00, 25.46it/s]\n",
            "Processing valid/down: 100% 264/264 [00:10<00:00, 25.63it/s]\n",
            "Processing valid/eight: 100% 243/243 [00:09<00:00, 26.14it/s]\n",
            "Processing valid/five: 100% 242/242 [00:09<00:00, 25.34it/s]\n",
            "Processing valid/four: 100% 280/280 [00:11<00:00, 23.67it/s]\n",
            "Processing valid/go: 100% 260/260 [00:09<00:00, 26.47it/s]\n",
            "Processing valid/happy: 100% 189/189 [00:07<00:00, 24.64it/s]\n",
            "Processing valid/house: 100% 173/173 [00:06<00:00, 25.12it/s]\n",
            "Processing valid/left: 100% 247/247 [00:10<00:00, 24.25it/s]\n",
            "Processing valid/marvin: 100% 160/160 [00:06<00:00, 24.30it/s]\n",
            "Processing valid/nine: 100% 230/230 [00:09<00:00, 23.54it/s]\n",
            "Processing valid/no: 100% 270/270 [00:10<00:00, 26.63it/s]\n",
            "Processing valid/off: 100% 256/256 [00:09<00:00, 25.80it/s]\n",
            "Processing valid/on: 100% 257/257 [00:10<00:00, 24.43it/s]\n",
            "Processing valid/one: 100% 230/230 [00:09<00:00, 23.96it/s]\n",
            "Processing valid/right: 100% 256/256 [00:09<00:00, 26.29it/s]\n",
            "Processing valid/seven: 100% 263/263 [00:10<00:00, 25.32it/s]\n",
            "Processing valid/sheila: 100% 176/176 [00:07<00:00, 24.38it/s]\n",
            "Processing valid/six: 100% 262/262 [00:09<00:00, 26.39it/s]\n",
            "Processing valid/stop: 100% 246/246 [00:10<00:00, 23.88it/s]\n",
            "Processing valid/three: 100% 248/248 [00:09<00:00, 25.73it/s]\n",
            "Processing valid/tree: 100% 166/166 [00:06<00:00, 25.75it/s]\n",
            "Processing valid/two: 100% 236/236 [00:09<00:00, 24.20it/s]\n",
            "Processing valid/up: 100% 260/260 [00:08<00:00, 29.69it/s]\n",
            "Processing valid/wow: 100% 166/166 [00:07<00:00, 23.47it/s]\n",
            "Processing valid/yes: 100% 261/261 [00:10<00:00, 25.83it/s]\n",
            "Processing valid/zero: 100% 260/260 [00:10<00:00, 24.75it/s]\n",
            "\n",
            " Augmenting: /content/drive/MyDrive/Keyword_spotting_transformer/data/test → /content/drive/MyDrive/Keyword_spotting_transformer/data/test_augmented_exercise_bike\n",
            "Processing test/bed: 100% 176/176 [00:08<00:00, 20.98it/s]\n",
            "Processing test/bird: 100% 158/158 [00:06<00:00, 23.85it/s]\n",
            "Processing test/cat: 100% 166/166 [00:06<00:00, 24.40it/s]\n",
            "Processing test/dog: 100% 180/180 [00:06<00:00, 25.82it/s]\n",
            "Processing test/down: 100% 253/253 [00:09<00:00, 26.12it/s]\n",
            "Processing test/eight: 100% 257/257 [00:10<00:00, 25.41it/s]\n",
            "Processing test/five: 100% 271/271 [00:10<00:00, 26.51it/s]\n",
            "Processing test/four: 100% 253/253 [00:09<00:00, 25.50it/s]\n",
            "Processing test/go: 100% 251/251 [00:09<00:00, 26.24it/s]\n",
            "Processing test/happy: 100% 180/180 [00:06<00:00, 26.30it/s]\n",
            "Processing test/house: 100% 150/150 [00:06<00:00, 22.12it/s]\n",
            "Processing test/left: 100% 267/267 [00:10<00:00, 25.36it/s]\n",
            "Processing test/marvin: 100% 162/162 [00:06<00:00, 26.22it/s]\n",
            "Processing test/nine: 100% 259/259 [00:09<00:00, 27.02it/s]\n",
            "Processing test/no: 100% 252/252 [00:10<00:00, 23.71it/s]\n",
            "Processing test/off: 100% 262/262 [00:10<00:00, 24.50it/s]\n",
            "Processing test/on: 100% 246/246 [00:09<00:00, 25.47it/s]\n",
            "Processing test/one: 100% 248/248 [00:10<00:00, 23.79it/s]\n",
            "Processing test/right: 100% 259/259 [00:09<00:00, 27.30it/s]\n",
            "Processing test/seven: 100% 239/239 [00:10<00:00, 23.61it/s]\n",
            "Processing test/sheila: 100% 186/186 [00:13<00:00, 13.62it/s]\n",
            "Processing test/six: 100% 244/244 [00:10<00:00, 23.82it/s]\n",
            "Processing test/stop: 100% 249/249 [00:09<00:00, 27.02it/s]\n",
            "Processing test/three: 100% 267/267 [00:10<00:00, 24.61it/s]\n",
            "Processing test/tree: 100% 193/193 [00:06<00:00, 28.22it/s]\n",
            "Processing test/two: 100% 264/264 [00:10<00:00, 25.26it/s]\n",
            "Processing test/up: 100% 272/272 [00:10<00:00, 26.78it/s]\n",
            "Processing test/wow: 100% 165/165 [00:06<00:00, 26.11it/s]\n",
            "Processing test/yes: 100% 256/256 [00:10<00:00, 25.40it/s]\n",
            "Processing test/zero: 100% 250/250 [00:09<00:00, 26.47it/s]\n",
            "\n",
            " All augmented audio files saved successfully!\n"
          ]
        }
      ],
      "source": [
        "!python augment_with_backgroundnoise.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOoBzYynZZtj"
      },
      "source": [
        "**TRAINING THE MODEL WITH AUGMENTION OF PINK NOISE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZs5o2emZYxJ",
        "outputId": "37fa3d97-4d78-4ace-afce-1e6b733d0961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHySaeGT9Oi6",
        "outputId": "10b5bac7-c2dc-4815-be53-ab2ee5701c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting dataset loading process...\n",
            "📦 Found cache: train_data.npz — Loading...\n",
            "✅ Loaded 0 samples from cache.\n",
            "📦 Found cache: val_data.npz — Loading...\n",
            "✅ Loaded 0 samples from cache.\n",
            "📦 Found cache: test_data.npz — Loading...\n",
            "✅ Loaded 0 samples from cache.\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (0, 1), y = (0,)\n",
            "Val:   X = (0, 1), y = (0,)\n",
            "Test:  X = (0, 1), y = (0,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n"
          ]
        }
      ],
      "source": [
        "!python data_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHGql8KCZ2zp",
        "outputId": "ab9799b5-0d3a-4092-b7e2-6cde79ac2aaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (40, 101)\n",
            "Dtype: float32\n",
            "Min value: -39.48077\n",
            "Max value: 6.748588\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Example path (adjust if needed)\n",
        "sample_path = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/processed_train_augmented_whitenoise/bed_0a7c2a8d_nohash_0_whitenoise_snr0.npy\"\n",
        "\n",
        "# Load and inspect\n",
        "sample = np.load(sample_path)\n",
        "print(\"Shape:\", sample.shape)\n",
        "print(\"Dtype:\", sample.dtype)\n",
        "print(\"Min value:\", np.min(sample))\n",
        "print(\"Max value:\", np.max(sample))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejCta1uLk2Pn"
      },
      "source": [
        "**WAV TO NPY FILES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJDDazrAk0kV",
        "outputId": "eb84756a-d7be-4730-a825-71c02aeaf57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4GDqf1aktvy",
        "outputId": "f5c5a185-ad4a-4a6b-bce7-8e9970456b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Conversion of exercise bike noise has started!\n",
            "\n",
            " Converting: /content/drive/MyDrive/Keyword_spotting_transformer/data/valid_augmented_exercise_bike_10dB → /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_valid_augmented_exercise_bike_10dB\n",
            "Processing label 'bed': 100% 197/197 [00:08<00:00, 23.31it/s]\n",
            "Processing label 'bird': 100% 162/162 [00:03<00:00, 41.54it/s]\n",
            "Processing label 'cat': 100% 168/168 [00:06<00:00, 26.46it/s]\n",
            "Processing label 'dog': 100% 170/170 [00:04<00:00, 42.48it/s]\n",
            "Processing label 'down': 100% 264/264 [00:06<00:00, 41.23it/s]\n",
            "Processing label 'eight': 100% 243/243 [00:06<00:00, 35.99it/s]\n",
            "Processing label 'five': 100% 242/242 [00:05<00:00, 44.76it/s]\n",
            "Processing label 'four': 100% 280/280 [00:08<00:00, 33.47it/s]\n",
            "Processing label 'go': 100% 260/260 [00:05<00:00, 45.18it/s]\n",
            "Processing label 'happy': 100% 189/189 [00:06<00:00, 27.97it/s]\n",
            "Processing label 'house': 100% 173/173 [00:03<00:00, 46.52it/s]\n",
            "Processing label 'left': 100% 247/247 [00:05<00:00, 48.31it/s]\n",
            "Processing label 'marvin': 100% 160/160 [00:05<00:00, 26.94it/s]\n",
            "Processing label 'nine': 100% 230/230 [00:04<00:00, 48.28it/s]\n",
            "Processing label 'no': 100% 270/270 [00:08<00:00, 30.89it/s]\n",
            "Processing label 'off': 100% 256/256 [00:05<00:00, 44.02it/s]\n",
            "Processing label 'on': 100% 257/257 [00:06<00:00, 41.27it/s]\n",
            "Processing label 'one': 100% 230/230 [00:06<00:00, 37.32it/s]\n",
            "Processing label 'right': 100% 256/256 [00:05<00:00, 45.35it/s]\n",
            "Processing label 'seven': 100% 263/263 [00:08<00:00, 32.74it/s]\n",
            "Processing label 'sheila': 100% 176/176 [00:04<00:00, 43.12it/s]\n",
            "Processing label 'six': 100% 262/262 [00:08<00:00, 31.09it/s]\n",
            "Processing label 'stop': 100% 246/246 [00:05<00:00, 44.44it/s]\n",
            "Processing label 'three': 100% 248/248 [00:07<00:00, 35.39it/s]\n",
            "Processing label 'tree': 100% 166/166 [00:03<00:00, 41.90it/s]\n",
            "Processing label 'two': 100% 236/236 [00:05<00:00, 46.33it/s]\n",
            "Processing label 'up': 100% 260/260 [00:08<00:00, 29.32it/s]\n",
            "Processing label 'wow': 100% 166/166 [00:03<00:00, 41.74it/s]\n",
            "Processing label 'yes': 100% 261/261 [00:08<00:00, 31.01it/s]\n",
            "Processing label 'zero': 100% 260/260 [00:05<00:00, 44.66it/s]\n",
            "\n",
            " All .wav files successfully converted to .npy format with label prefixes!\n"
          ]
        }
      ],
      "source": [
        "!python wav_to_npy.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWTcdF72-zmu",
        "outputId": "c82ea95c-f6ca-4213-c402-d4247c03dd0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/data/processed_valid_augmented_exercise_bike_10dB\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_valid_augmented_exercise_bike_10dB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aueeZR7slAml",
        "outputId": "053e3059-467b-41cc-b31c-d5543c929db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VazBZ6owEAZ",
        "outputId": "4bc6b973-f6d4-44b5-ab70-d03f5c8bd74c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting dataset loading process...\n",
            "\n",
            "🔄 No cache found. Loading raw data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_train_augmented_pinknoise\n",
            "🔹 Loaded 500 spectrograms...\n",
            "🔹 Loaded 1000 spectrograms...\n",
            "🔹 Loaded 1500 spectrograms...\n",
            "🔹 Loaded 2000 spectrograms...\n",
            "🔹 Loaded 2500 spectrograms...\n",
            "🔹 Loaded 3000 spectrograms...\n",
            "🔹 Loaded 3500 spectrograms...\n",
            "🔹 Loaded 4000 spectrograms...\n",
            "🔹 Loaded 4500 spectrograms...\n",
            "🔹 Loaded 5000 spectrograms...\n",
            "🔹 Loaded 5500 spectrograms...\n",
            "🔹 Loaded 6000 spectrograms...\n",
            "🔹 Loaded 6500 spectrograms...\n",
            "🔹 Loaded 7000 spectrograms...\n",
            "🔹 Loaded 7500 spectrograms...\n",
            "🔹 Loaded 8000 spectrograms...\n",
            "🔹 Loaded 8500 spectrograms...\n",
            "🔹 Loaded 9000 spectrograms...\n",
            "🔹 Loaded 9500 spectrograms...\n",
            "🔹 Loaded 10000 spectrograms...\n",
            "🔹 Loaded 10500 spectrograms...\n",
            "🔹 Loaded 11000 spectrograms...\n",
            "🔹 Loaded 11500 spectrograms...\n",
            "🔹 Loaded 12000 spectrograms...\n",
            "🔹 Loaded 12500 spectrograms...\n",
            "🔹 Loaded 13000 spectrograms...\n",
            "🔹 Loaded 13500 spectrograms...\n",
            "🔹 Loaded 14000 spectrograms...\n",
            "🔹 Loaded 14500 spectrograms...\n",
            "🔹 Loaded 15000 spectrograms...\n",
            "🔹 Loaded 15500 spectrograms...\n",
            "🔹 Loaded 16000 spectrograms...\n",
            "🔹 Loaded 16500 spectrograms...\n",
            "🔹 Loaded 17000 spectrograms...\n",
            "🔹 Loaded 17500 spectrograms...\n",
            "🔹 Loaded 18000 spectrograms...\n",
            "\n",
            "💾 Caching 18000 samples to train_data.npz\n",
            "✅ Done caching!\n",
            "\n",
            "🔄 No cache found. Loading raw data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_valid_augmented_pinknoise\n",
            "🔹 Loaded 500 spectrograms...\n",
            "🔹 Loaded 1000 spectrograms...\n",
            "🔹 Loaded 1500 spectrograms...\n",
            "🔹 Loaded 2000 spectrograms...\n",
            "🔹 Loaded 2500 spectrograms...\n",
            "🔹 Loaded 3000 spectrograms...\n",
            "🔹 Loaded 3500 spectrograms...\n",
            "\n",
            "💾 Caching 3746 samples to val_data.npz\n",
            "✅ Done caching!\n",
            "\n",
            "🔄 No cache found. Loading raw data from: /content/drive/MyDrive/Keyword_spotting_transformer/data/processed_test_augmented_pinknoise\n",
            "🔹 Loaded 500 spectrograms...\n",
            "🔹 Loaded 1000 spectrograms...\n",
            "🔹 Loaded 1500 spectrograms...\n",
            "🔹 Loaded 2000 spectrograms...\n",
            "🔹 Loaded 2500 spectrograms...\n",
            "🔹 Loaded 3000 spectrograms...\n",
            "🔹 Loaded 3500 spectrograms...\n",
            "\n",
            "💾 Caching 3761 samples to test_data.npz\n",
            "✅ Done caching!\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (18000, 40, 101, 1), y = (18000,)\n",
            "Val:   X = (3746, 40, 101, 1), y = (3746,)\n",
            "Test:  X = (3761, 40, 101, 1), y = (3761,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n"
          ]
        }
      ],
      "source": [
        "!python data_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwqm-rsWw5-7",
        "outputId": "16dd86fd-89ab-4791-b13f-f1fec85c74b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-13 08:05:50.511667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "🚀 Starting dataset loading process...\n",
            "📦 Found cache: train_data.npz — Loading...\n",
            "✅ Loaded 18000 samples from cache.\n",
            "📦 Found cache: val_data.npz — Loading...\n",
            "✅ Loaded 3746 samples from cache.\n",
            "📦 Found cache: test_data.npz — Loading...\n",
            "✅ Loaded 3761 samples from cache.\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (18000, 40, 101, 1), y = (18000,)\n",
            "Val:   X = (3746, 40, 101, 1), y = (3746,)\n",
            "Test:  X = (3761, 40, 101, 1), y = (3761,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n",
            "🚀 Starting training...\n",
            "\n",
            "Epoch 1/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 1.4257 - accuracy: 0.5332\n",
            "Epoch 1: val_accuracy improved from -inf to 0.76722, saving model to models/binary_cnn_best.keras\n",
            "1125/1125 [==============================] - 120s 105ms/step - loss: 1.4257 - accuracy: 0.5332 - val_loss: 0.7584 - val_accuracy: 0.7672\n",
            "Epoch 2/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.3790 - accuracy: 0.8799\n",
            "Epoch 2: val_accuracy improved from 0.76722 to 0.82488, saving model to models/binary_cnn_best.keras\n",
            "1125/1125 [==============================] - 112s 100ms/step - loss: 0.3790 - accuracy: 0.8799 - val_loss: 0.5649 - val_accuracy: 0.8249\n",
            "Epoch 3/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.2435 - accuracy: 0.9241\n",
            "Epoch 3: val_accuracy improved from 0.82488 to 0.88548, saving model to models/binary_cnn_best.keras\n",
            "1125/1125 [==============================] - 116s 103ms/step - loss: 0.2435 - accuracy: 0.9241 - val_loss: 0.4144 - val_accuracy: 0.8855\n",
            "Epoch 4/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.1744 - accuracy: 0.9426\n",
            "Epoch 4: val_accuracy did not improve from 0.88548\n",
            "1125/1125 [==============================] - 110s 98ms/step - loss: 0.1744 - accuracy: 0.9426 - val_loss: 0.4316 - val_accuracy: 0.8780\n",
            "Epoch 5/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9516\n",
            "Epoch 5: val_accuracy did not improve from 0.88548\n",
            "1125/1125 [==============================] - 108s 96ms/step - loss: 0.1503 - accuracy: 0.9516 - val_loss: 1.3356 - val_accuracy: 0.7181\n",
            "Epoch 6/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9641\n",
            "Epoch 6: val_accuracy improved from 0.88548 to 0.90176, saving model to models/binary_cnn_best.keras\n",
            "1125/1125 [==============================] - 114s 101ms/step - loss: 0.1140 - accuracy: 0.9641 - val_loss: 0.4860 - val_accuracy: 0.9018\n",
            "Epoch 7/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9639\n",
            "Epoch 7: val_accuracy did not improve from 0.90176\n",
            "1125/1125 [==============================] - 110s 98ms/step - loss: 0.1169 - accuracy: 0.9639 - val_loss: 0.5680 - val_accuracy: 0.8884\n",
            "Epoch 8/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9678\n",
            "Epoch 8: val_accuracy did not improve from 0.90176\n",
            "1125/1125 [==============================] - 114s 102ms/step - loss: 0.1005 - accuracy: 0.9678 - val_loss: 0.4672 - val_accuracy: 0.9002\n",
            "Epoch 9/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0741 - accuracy: 0.9766\n",
            "Epoch 9: val_accuracy improved from 0.90176 to 0.91004, saving model to models/binary_cnn_best.keras\n",
            "1125/1125 [==============================] - 110s 97ms/step - loss: 0.0741 - accuracy: 0.9766 - val_loss: 0.4565 - val_accuracy: 0.9100\n",
            "Epoch 10/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9753\n",
            "Epoch 10: val_accuracy improved from 0.91004 to 0.91057, saving model to models/binary_cnn_best.keras\n",
            "1125/1125 [==============================] - 110s 98ms/step - loss: 0.0788 - accuracy: 0.9753 - val_loss: 0.4976 - val_accuracy: 0.9106\n",
            "Epoch 11/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0702 - accuracy: 0.9777\n",
            "Epoch 11: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 110s 98ms/step - loss: 0.0702 - accuracy: 0.9777 - val_loss: 0.7782 - val_accuracy: 0.8593\n",
            "Epoch 12/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9777\n",
            "Epoch 12: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 114s 101ms/step - loss: 0.0720 - accuracy: 0.9777 - val_loss: 1.2060 - val_accuracy: 0.8289\n",
            "Epoch 13/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9837\n",
            "Epoch 13: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 113s 101ms/step - loss: 0.0532 - accuracy: 0.9837 - val_loss: 0.6745 - val_accuracy: 0.8897\n",
            "Epoch 14/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9858\n",
            "Epoch 14: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 141s 126ms/step - loss: 0.0469 - accuracy: 0.9858 - val_loss: 0.6524 - val_accuracy: 0.8727\n",
            "Epoch 15/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9834\n",
            "Epoch 15: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 115s 103ms/step - loss: 0.0542 - accuracy: 0.9834 - val_loss: 0.6763 - val_accuracy: 0.8905\n",
            "Epoch 16/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9828\n",
            "Epoch 16: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 109s 97ms/step - loss: 0.0548 - accuracy: 0.9828 - val_loss: 0.5972 - val_accuracy: 0.9103\n",
            "Epoch 17/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9844\n",
            "Epoch 17: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 109s 97ms/step - loss: 0.0516 - accuracy: 0.9844 - val_loss: 0.6143 - val_accuracy: 0.9047\n",
            "Epoch 18/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0401 - accuracy: 0.9876\n",
            "Epoch 18: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 109s 97ms/step - loss: 0.0401 - accuracy: 0.9876 - val_loss: 0.6609 - val_accuracy: 0.8932\n",
            "Epoch 19/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9873\n",
            "Epoch 19: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 110s 98ms/step - loss: 0.0410 - accuracy: 0.9873 - val_loss: 0.7476 - val_accuracy: 0.8855\n",
            "Epoch 20/20\n",
            "1125/1125 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9884\n",
            "Epoch 20: val_accuracy did not improve from 0.91057\n",
            "1125/1125 [==============================] - 113s 101ms/step - loss: 0.0395 - accuracy: 0.9884 - val_loss: 0.6284 - val_accuracy: 0.8980\n",
            "✅ Training complete!\n",
            "💾 Saving model architecture and weights...\n",
            "✅ Model saved as JSON + weights\n",
            "📊 Saving sample input/output for hls4ml...\n",
            "1/1 [==============================] - 0s 137ms/step\n",
            "📊 Sample data saved.\n",
            "🛠️ Generating config file for hls4ml...\n",
            "✅ hls4ml config saved.\n"
          ]
        }
      ],
      "source": [
        "!python train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz5f_eWtTkTP",
        "outputId": "2c479206-472a-4fc2-db02-a9531fcc30e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-13 09:05:08.636136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "🚀 Starting dataset loading process...\n",
            "📦 Found cache: train_data.npz — Loading...\n",
            "✅ Loaded 18000 samples from cache.\n",
            "📦 Found cache: val_data.npz — Loading...\n",
            "✅ Loaded 3746 samples from cache.\n",
            "📦 Found cache: test_data.npz — Loading...\n",
            "✅ Loaded 3761 samples from cache.\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (18000, 40, 101, 1), y = (18000,)\n",
            "Val:   X = (3746, 40, 101, 1), y = (3746,)\n",
            "Test:  X = (3761, 40, 101, 1), y = (3761,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n",
            "📦 Loading model...\n",
            "✅ Model loaded and compiled.\n",
            "🧪 Evaluating on test set...\n",
            "118/118 [==============================] - 6s 45ms/step - loss: 0.7285 - accuracy: 0.8974\n",
            "\n",
            "🎯 Test Accuracy: 89.74%\n",
            "📉 Test Loss: 0.7285\n",
            "🔍 Running predictions...\n",
            "118/118 [==============================] - 5s 40ms/step\n",
            "📊 Saving confusion matrix...\n",
            "✅ Saved to /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/plots/pinknoise_cnn_kwt_confusion_matrix.png\n",
            "\n",
            "📄 Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        zero       0.92      0.88      0.90       250\n",
            "         one       0.90      0.89      0.89       248\n",
            "         two       0.87      0.95      0.91       264\n",
            "       three       0.85      0.92      0.88       267\n",
            "        four       0.92      0.92      0.92       253\n",
            "        five       0.96      0.81      0.88       271\n",
            "         six       0.87      0.94      0.91       244\n",
            "       seven       1.00      0.85      0.91       239\n",
            "       eight       0.96      0.86      0.91       257\n",
            "        nine       0.87      0.93      0.90       259\n",
            "          up       0.93      0.91      0.92       272\n",
            "        down       0.98      0.86      0.91       253\n",
            "         yes       0.93      0.95      0.94       256\n",
            "          no       0.88      0.88      0.88       252\n",
            "         bed       0.68      0.95      0.79       176\n",
            "\n",
            "    accuracy                           0.90      3761\n",
            "   macro avg       0.90      0.90      0.90      3761\n",
            "weighted avg       0.90      0.90      0.90      3761\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N35VBE6Ysn8",
        "outputId": "bfc1ccc2-48ff-4e4d-b13e-07851c8c5532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-04-13 09:50:23.640590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "🚀 Starting dataset loading process...\n",
            "📦 Found cache: train_data.npz — Loading...\n",
            "✅ Loaded 18000 samples from cache.\n",
            "📦 Found cache: val_data.npz — Loading...\n",
            "✅ Loaded 3746 samples from cache.\n",
            "📦 Found cache: test_data.npz — Loading...\n",
            "✅ Loaded 3761 samples from cache.\n",
            "\n",
            "📊 Dataset Summary:\n",
            "Train: X = (18000, 40, 101, 1), y = (18000,)\n",
            "Val:   X = (3746, 40, 101, 1), y = (3746,)\n",
            "Test:  X = (3761, 40, 101, 1), y = (3761,)\n",
            "\n",
            "✅ All datasets are loaded and ready!\n",
            "📦 Loading model architecture and weights...\n",
            "✅ Model loaded and compiled.\n",
            "🔊 Loading audio: /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/recorded_audio_files/four.wav\n",
            "🔍 Predicting...\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            "\n",
            "🎙️ Prediction: four\n",
            "📊 Confidence: 0.46\n"
          ]
        }
      ],
      "source": [
        "!python predict_audio.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nth4wyrvAXcS",
        "outputId": "45eeecf9-4462-481b-9a30-ec0ffd4b9134"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLJTqrMD08Mh",
        "outputId": "c5d7fe7f-ecbf-4e45-904d-325736ad4c07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hls4ml\n",
            "  Downloading hls4ml-1.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from hls4ml) (3.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hls4ml) (1.23.5)\n",
            "Collecting pydigitalwavetools==1.1 (from hls4ml)\n",
            "  Downloading pyDigitalWaveTools-1.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from hls4ml) (6.0.2)\n",
            "Downloading hls4ml-1.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyDigitalWaveTools-1.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: pydigitalwavetools, hls4ml\n",
            "Successfully installed hls4ml-1.1.0 pydigitalwavetools-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install hls4ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJ_5vesH8KVu",
        "outputId": "26b0dafd-0ca6-46f9-ceb6-ca891d710982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keras version: 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "print(\"Keras version:\", keras.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT2z6s7SES1q",
        "outputId": "f93aaf98-1d71-4974-d493-ce53613bcc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgHsfSoV8K3G",
        "outputId": "ce2c67ac-06d1-4a42-a2f7-4d5542871ba2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XKaeSj6EjhV",
        "outputId": "479ba0d2-7a7f-4959-dddb-845061f01a62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/import_pb_to_tensorboard\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.11/dist-packages/tensorflow-2.18.0.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/tensorflow/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tensorflow-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "cBVc2RDYE0Y_",
        "outputId": "aedb51a3-1ace-46b3-b806-ca6fe606028f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.12.0\n",
            "  Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.12.0)\n",
            "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.5.2)\n",
            "Collecting keras<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading keras-2.12.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Collecting numpy<1.24,>=1.22 (from tensorflow==2.12.0)\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.12.0)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Collecting tensorboard<2.13,>=2.12 (from tensorflow==2.12.0)\n",
            "  Downloading tensorboard-2.12.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow-estimator<2.13,>=2.12.0 (from tensorflow==2.12.0)\n",
            "  Downloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.12.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "INFO: pip is looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.6.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.6.0,>=0.6.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.6.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting ml_dtypes>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.3,>=0.5.3 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.3-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.5.1-py3-none-any.whl.metadata (22 kB)\n",
            "  Downloading jax-0.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.5.0,>=0.5.0 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.5.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (978 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.38-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.38,>=0.4.38 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.38-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.37-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.37,>=0.4.36 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.36-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.36-py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is still looking at multiple versions of jax to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jax-0.4.35-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.35,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.35-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.34-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.34,>=0.4.34 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.34-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.33-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.33,>=0.4.33 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.33-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.31-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.31,>=0.4.30 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.31-cp311-cp311-manylinux2014_x86_64.whl.metadata (983 bytes)\n",
            "Collecting jax>=0.3.15 (from tensorflow==2.12.0)\n",
            "  Downloading jax-0.4.30-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting jaxlib<=0.4.30,>=0.4.27 (from jax>=0.3.15->tensorflow==2.12.0)\n",
            "  Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.15.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.13,>=2.12->tensorflow==2.12.0)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Downloading tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.0/586.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading jax-0.4.30-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading jaxlib-0.4.30-cp311-cp311-manylinux2014_x86_64.whl (79.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.6/79.6 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, gast, jaxlib, google-auth-oauthlib, tensorboard, jax, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.6.0\n",
            "    Uninstalling gast-0.6.0:\n",
            "      Successfully uninstalled gast-0.6.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.2\n",
            "    Uninstalling google-auth-oauthlib-1.2.2:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 jax-0.4.30 jaxlib-0.4.30 keras-2.12.0 numpy-1.23.5 protobuf-4.25.7 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-estimator-2.12.0 wrapt-1.14.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "3706c1918d6141cfb3c80401368956ab",
              "pip_warning": {
                "packages": [
                  "gast",
                  "jax",
                  "jaxlib",
                  "keras",
                  "numpy",
                  "tensorflow",
                  "wrapt"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqJ3T98p8UNT",
        "outputId": "6b5f3b52-b818-45a9-896d-ab1cef271b51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbiJb9HG1ql0",
        "outputId": "0349c4d0-9102-42f5-f819-ef348b3832f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-07 19:25:32.607652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            " Loading model from:\n",
            " /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts/models/binary_cnn.json\n",
            " /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts/models/binary_cnn.weights.h5\n",
            " Model loaded and compiled.\n",
            "\n",
            "🎯 Preparing sample input/output...\n",
            "1/1 [==============================] - 0s 169ms/step\n",
            " Sample input/output prepared.\n",
            "\n",
            "Saving tb_data manually...\n",
            " tb_data files saved at: /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts/hls4ml_prj_binary_cnn/tb_data\n",
            " HLS4ML config ready.\n",
            "\n",
            " Starting HLS conversion...\n",
            "Interpreting Sequential\n",
            "Topology:\n",
            "Layer name: conv2d_input, layer type: InputLayer, input shapes: [[None, 40, 101, 1]], output shape: [None, 40, 101, 1]\n",
            "Layer name: conv2d, layer type: Conv2D, input shapes: [[None, 40, 101, 1]], output shape: [None, 40, 101, 16]\n",
            "Layer name: batch_normalization, layer type: BatchNormalization, input shapes: [[None, 40, 101, 16]], output shape: [None, 40, 101, 16]\n",
            "Layer name: max_pooling2d, layer type: MaxPooling2D, input shapes: [[None, 40, 101, 16]], output shape: [None, 20, 50, 16]\n",
            "Layer name: conv2d_1, layer type: Conv2D, input shapes: [[None, 20, 50, 16]], output shape: [None, 20, 50, 32]\n",
            "Layer name: batch_normalization_1, layer type: BatchNormalization, input shapes: [[None, 20, 50, 32]], output shape: [None, 20, 50, 32]\n",
            "Layer name: max_pooling2d_1, layer type: MaxPooling2D, input shapes: [[None, 20, 50, 32]], output shape: [None, 10, 25, 32]\n",
            "Layer name: conv2d_2, layer type: Conv2D, input shapes: [[None, 10, 25, 32]], output shape: [None, 10, 25, 64]\n",
            "Layer name: batch_normalization_2, layer type: BatchNormalization, input shapes: [[None, 10, 25, 64]], output shape: [None, 10, 25, 64]\n",
            "Layer name: max_pooling2d_2, layer type: MaxPooling2D, input shapes: [[None, 10, 25, 64]], output shape: [None, 5, 12, 64]\n",
            "Layer name: flatten, layer type: Reshape, input shapes: [[None, 5, 12, 64]], output shape: [None, 3840]\n",
            "Layer name: dense, layer type: Dense, input shapes: [[None, 3840]], output shape: [None, 128]\n",
            "Layer name: dense_1, layer type: Dense, input shapes: [[None, 128]], output shape: [None, 15]\n",
            "Creating HLS model\n",
            "\n",
            " Writing HLS project files...\n",
            "Writing HLS project\n",
            "Done\n",
            " HLS project written to: /content/drive/MyDrive/Keyword_spotting_transformer/Keyword_Spotting_CNN_HLS/scripts/hls4ml_prj_binary_cnn\n",
            "\n",
            " Compiling HLS model...\n",
            "Writing HLS project\n",
            "Done\n",
            " HLS model compiled!\n",
            "\n",
            "Skipping build step in Colab (build requires Vivado/Vitis locally)...\n"
          ]
        }
      ],
      "source": [
        "!python convert_to_hls.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yS5nADP4bFvR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === CONFIG ===\n",
        "SAMPLE_RATE = 16000\n",
        "TARGET_SNR_DB = 0  # Signal-to-noise ratio in dB\n",
        "\n",
        "# === Path Config ===\n",
        "BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/archive/Data/Audio/Male\"\n",
        "NOISE_PATH = \"/path/to/your/noise.wav\"  # Update this to your noise file path\n",
        "\n",
        "# === Utility Functions ===\n",
        "def load_audio(file_path, sr=SAMPLE_RATE):\n",
        "    audio, _ = librosa.load(file_path, sr=sr, mono=True)\n",
        "    return audio\n",
        "\n",
        "def add_background_noise(audio, noise, snr_db):\n",
        "    if len(noise) < len(audio):\n",
        "        repeats = int(np.ceil(len(audio) / len(noise)))\n",
        "        noise = np.tile(noise, repeats)\n",
        "    noise = noise[:len(audio)]\n",
        "\n",
        "    # Scale pink noise to desired SNR\n",
        "    signal_power = np.mean(audio ** 2)\n",
        "    noise_power = np.mean(noise ** 2)\n",
        "    factor = np.sqrt(signal_power / (10**(snr_db / 10) * noise_power))\n",
        "    noisy_audio = audio + noise * factor\n",
        "    return noisy_audio\n",
        "\n",
        "# === Load background noise once ===\n",
        "print(f\" Loading background noise from: {NOISE_PATH}\")\n",
        "background_noise = load_audio(NOISE_PATH)\n",
        "\n",
        "# === Augment each set ===\n",
        "for subfolder in [\"PTDB-TUG\", \"TMIT\"]:  # Loop through the two subfolders\n",
        "    input_dir = os.path.join(BASE_DIR, subfolder)\n",
        "    output_dir = os.path.join(BASE_DIR, f\"{subfolder}_augmented\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n Augmenting: {input_dir} → {output_dir}\")\n",
        "\n",
        "    # Loop through each keyword folder inside PTDB-TUG and TMIT\n",
        "    for keyword in os.listdir(input_dir):\n",
        "        keyword_dir = os.path.join(input_dir, keyword)\n",
        "        if not os.path.isdir(keyword_dir):\n",
        "            continue\n",
        "\n",
        "        save_keyword_dir = os.path.join(output_dir, keyword)\n",
        "        os.makedirs(save_keyword_dir, exist_ok=True)\n",
        "\n",
        "        # Loop through the files in each keyword folder\n",
        "        for filename in tqdm(os.listdir(keyword_dir), desc=f\"Processing {subfolder}/{keyword}\"):\n",
        "            if not filename.endswith(\".wav\"):\n",
        "                continue\n",
        "\n",
        "            filepath = os.path.join(keyword_dir, filename)\n",
        "            try:\n",
        "                clean_audio = load_audio(filepath)\n",
        "                noisy_audio = add_background_noise(clean_audio, background_noise, TARGET_SNR_DB)\n",
        "\n",
        "                # Save noisy version\n",
        "                new_filename = filename.replace(\".wav\", f\"_noise_snr{TARGET_SNR_DB}.wav\")\n",
        "                save_path = os.path.join(save_keyword_dir, new_filename)\n",
        "                sf.write(save_path, noisy_audio, SAMPLE_RATE)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\" Error processing {filename}: {e}\")\n",
        "\n",
        "print(\"\\n All augmented audio files saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lk4_h86KpOls"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === CONFIG ===\n",
        "SAMPLE_RATE = 16000\n",
        "SNR_VALUES = [-5, 0, 10]  # You can adjust or add more SNR values here\n",
        "\n",
        "# === Path Config ===\n",
        "BASE_DIR = \"/content/drive/MyDrive/Colab Notebooks/archive/Data/Audio/Male\"\n",
        "NOISE_FILES = {\n",
        "    \"pink_noise\": \"/content/drive/MyDrive/background_noise/background_noise/pink_noise.wav\",\n",
        "    \"dishes_noise\": \"/content/drive/MyDrive/background_noise/background_noise/doing_the_dishes.wav\",\n",
        "    \"dude_miaowing_noise\": \"/content/drive/MyDrive/background_noise/background_noise/dude_miaowing.wav\",\n",
        "    \"exercise_bike\": \"/content/drive/MyDrive/background_noise/background_noise/exercise_bike.wav\",\n",
        "    \"running_tap\": \"/content/drive/MyDrive/background_noise/background_noise/running_tap.wav\",\n",
        "    \"white_noise\": \"/content/drive/MyDrive/background_noise/background_noise/white_noise.wav\"\n",
        "}\n",
        "\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/augmentation\"\n",
        "\n",
        "# === Utility Functions ===\n",
        "def load_audio(file_path, sr=SAMPLE_RATE):\n",
        "    audio, _ = librosa.load(file_path, sr=sr, mono=True)\n",
        "    return audio\n",
        "\n",
        "def add_noise(audio, noise, snr_db):\n",
        "    if len(noise) < len(audio):\n",
        "        repeats = int(np.ceil(len(audio) / len(noise)))\n",
        "        noise = np.tile(noise, repeats)\n",
        "    noise = noise[:len(audio)]\n",
        "\n",
        "    # Scale noise to desired SNR\n",
        "    signal_power = np.mean(audio ** 2)\n",
        "    noise_power = np.mean(noise ** 2)\n",
        "    factor = np.sqrt(signal_power / (10**(snr_db / 10) * noise_power))\n",
        "    noisy_audio = audio + noise * factor\n",
        "    return noisy_audio\n",
        "\n",
        "# === Load All Noise Files ===\n",
        "noise_data = {}\n",
        "for name, path in NOISE_FILES.items():\n",
        "    print(f\"Loading noise: {name} from {path}\")\n",
        "    noise_data[name] = load_audio(path)\n",
        "\n",
        "# === Process Folders ===\n",
        "for dataset in [\"TMIT\", \"PTDB-TUG\"]:\n",
        "    input_dir = os.path.join(BASE_DIR, dataset)\n",
        "\n",
        "    for noise_name, noise_audio in noise_data.items():\n",
        "        for snr in SNR_VALUES:\n",
        "            output_dir = os.path.join(OUTPUT_DIR, f\"{dataset}_{noise_name}_snr{snr}\")\n",
        "            os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "            print(f\"\\nAugmenting {dataset} with {noise_name} noise at {snr} dB → {output_dir}\")\n",
        "\n",
        "            for filename in tqdm(os.listdir(input_dir), desc=f\"{dataset}/{noise_name}/SNR{snr}\"):\n",
        "                if not filename.endswith(\".wav\"):\n",
        "                    continue\n",
        "\n",
        "                filepath = os.path.join(input_dir, filename)\n",
        "                try:\n",
        "                    clean_audio = load_audio(filepath)\n",
        "                    noisy_audio = add_noise(clean_audio, noise_audio, snr)\n",
        "\n",
        "                    new_filename = filename.replace(\".wav\", f\"_{noise_name}_snr{snr}.wav\")\n",
        "                    save_path = os.path.join(output_dir, new_filename)\n",
        "                    sf.write(save_path, noisy_audio, SAMPLE_RATE)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {filename}: {e}\")\n",
        "\n",
        "print(\"\\n✅ All augmentation completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMZdgLWXZYFO",
        "outputId": "441715fb-8556-429e-c9eb-647b12bccf7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGUTCRk4LN3V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6defb6a4-04cc-40fa-9835-b8627a98e9d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "\n",
            "🚀 Evaluating Model on Test Data...\n",
            "\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            " Test Accuracy: 59.87%\n"
          ]
        }
      ],
      "source": [
        "!python test.py"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1GZaJQsy7SFaEIwQI0O33QMLKMT1V6_IH",
      "authorship_tag": "ABX9TyOJf9lQNfOcBLQSd4nPjdw5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}