{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkat23-hub/VenkatKWT/blob/main/KWT_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRO0qjJIrequ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CH0xPb9rtQv",
        "outputId": "abc7c8d3-46d8-45e8-d5b5-b4230bef0085"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVhCo8DhrvSt",
        "outputId": "fa15089a-da05-4e54-d717-3b33ab097781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
            "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio librosa numpy tqdm pyyaml\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2AT3i_DTtoc"
      },
      "source": [
        "**To check dataset is present or not.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snXBZa-Nsagd",
        "outputId": "843d8679-09b6-46ff-b8b4-46c02478279e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset found!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Define dataset path\n",
        "DATASET_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/speech_commands_dataset\"\n",
        "\n",
        "# Check if dataset exists\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    print(\"✅ Dataset found!\")\n",
        "else:\n",
        "    print(\"❌ Dataset not found. Check the path!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTpsmAmvT-f4"
      },
      "source": [
        "**Organize Train, Validation, and Test Splits**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFVuu17QD8ZI",
        "outputId": "a40e2c65-7f58-43f1-94d6-6d2a9c152919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.61.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba) (0.44.0)\n",
            "Requirement already satisfied: numpy<2.2,>=1.24 in /usr/local/lib/python3.11/dist-packages (from numba) (1.26.4)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade numba librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgdBW7lfYlfW"
      },
      "source": [
        "**Verify Processed data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCg9NZPuYheC",
        "outputId": "7aa876c7-bd1b-40ff-c48e-cd8d61c1c0fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train files: 30\n",
            "Validation files: 30\n",
            "Test files: 30\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Check if preprocessing was successful\n",
        "print(\"Train files:\", len(os.listdir(\"/content/drive/MyDrive/Keyword_spotting_transformer/data/train/\")))\n",
        "print(\"Validation files:\", len(os.listdir(\"/content/drive/MyDrive/Keyword_spotting_transformer/data/valid\")))\n",
        "print(\"Test files:\", len(os.listdir(\"/content/drive/MyDrive/Keyword_spotting_transformer/data/test\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qCJRdXs3fXS"
      },
      "source": [
        "**Converting the .wav files to MEL spectrograms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WYvpDkJAGH0",
        "outputId": "6b3e1f6a-5300-43c6-ad44-616a0446af69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train folders: ['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\n",
            "Valid folders: ['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\n",
            "Test folders: ['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/data\"\n",
        "\n",
        "print(\"Train folders:\", os.listdir(f\"{DATA_PATH}/train\"))\n",
        "print(\"Valid folders:\", os.listdir(f\"{DATA_PATH}/valid\"))\n",
        "print(\"Test folders:\", os.listdir(f\"{DATA_PATH}/test\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EgluwtfAZms",
        "outputId": "cd118b5a-47e1-439e-a559-f1dfc112d03d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Moved .npy files to train_processed/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "TRAIN_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/train\"\n",
        "PROCESSED_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/train_processed\"\n",
        "\n",
        "os.makedirs(PROCESSED_PATH, exist_ok=True)\n",
        "\n",
        "for file in os.listdir(TRAIN_PATH):\n",
        "    if file.endswith(\".npy\"):\n",
        "        shutil.move(os.path.join(TRAIN_PATH, file), os.path.join(PROCESSED_PATH, file))\n",
        "\n",
        "print(\"✅ Moved .npy files to train_processed/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkWTu7KyBulK",
        "outputId": "ca5e6d90-5ec1-45d2-bef3-0070f271e3b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Moved .npy files from valid → valid_processed/\n",
            "✅ Moved .npy files from test → test_processed/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/data\"\n",
        "SPLITS = [\"valid\", \"test\"]\n",
        "\n",
        "for split in SPLITS:\n",
        "    input_folder = os.path.join(DATA_PATH, split)\n",
        "    output_folder = os.path.join(DATA_PATH, f\"{split}_processed\")\n",
        "\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for file in os.listdir(input_folder):\n",
        "        if file.endswith(\".npy\"):\n",
        "            shutil.move(os.path.join(input_folder, file), os.path.join(output_folder, file))\n",
        "\n",
        "    print(f\" Moved .npy files from {split} → {split}_processed/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCpbHpqoC9Xr",
        "outputId": "759d77c4-4bf6-401b-b6d2-f66607ec068e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Processed: 51324\n",
            "Valid Processed: 6798\n",
            "Test Processed: 6835\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/data\"\n",
        "\n",
        "print(\"Train Processed:\", len(os.listdir(f\"{DATA_PATH}/train_processed\")))\n",
        "print(\"Valid Processed:\", len(os.listdir(f\"{DATA_PATH}/valid_processed\")))\n",
        "print(\"Test Processed:\", len(os.listdir(f\"{DATA_PATH}/test_processed\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTBLRaLgYtRA",
        "outputId": "660b7b9b-0d08-4e65-e74c-4763cc64f15d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing train data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing bed: 100%|██████████| 1358/1358 [00:38<00:00, 34.85it/s]\n",
            "Processing bird: 100%|██████████| 1421/1421 [01:08<00:00, 20.66it/s]\n",
            "Processing cat: 100%|██████████| 1409/1409 [01:14<00:00, 18.83it/s]\n",
            "Processing dog: 100%|██████████| 1396/1396 [01:10<00:00, 19.78it/s]\n",
            "Processing down: 100%|██████████| 1852/1852 [01:43<00:00, 17.94it/s]\n",
            "Processing eight: 100%|██████████| 1852/1852 [01:54<00:00, 16.15it/s]\n",
            "Processing five: 100%|██████████| 1861/1861 [01:43<00:00, 17.97it/s]\n",
            "Processing four: 100%|██████████| 1899/1899 [01:42<00:00, 18.50it/s]\n",
            "Processing go: 100%|██████████| 1881/1881 [01:45<00:00, 17.75it/s]\n",
            "Processing happy: 100%|██████████| 1373/1373 [01:10<00:00, 19.45it/s]\n",
            "Processing house: 100%|██████████| 1427/1427 [01:12<00:00, 19.82it/s]\n",
            "Processing left: 100%|██████████| 1839/1839 [01:45<00:00, 17.36it/s]\n",
            "Processing marvin: 100%|██████████| 1444/1444 [01:27<00:00, 16.59it/s]\n",
            "Processing nine: 100%|██████████| 1875/1875 [01:51<00:00, 16.75it/s]\n",
            "Processing no: 100%|██████████| 1853/1853 [01:46<00:00, 17.36it/s]\n",
            "Processing off: 100%|██████████| 1839/1839 [01:48<00:00, 16.94it/s]\n",
            "Processing on: 100%|██████████| 1884/1884 [01:47<00:00, 17.56it/s]\n",
            "Processing one: 100%|██████████| 1892/1892 [02:07<00:00, 14.87it/s]\n",
            "Processing right: 100%|██████████| 1852/1852 [01:44<00:00, 17.64it/s]\n",
            "Processing seven: 100%|██████████| 1876/1876 [01:53<00:00, 16.52it/s]\n",
            "Processing sheila: 100%|██████████| 1382/1382 [01:17<00:00, 17.95it/s]\n",
            "Processing six: 100%|██████████| 1863/1863 [01:46<00:00, 17.52it/s]\n",
            "Processing stop: 100%|██████████| 1895/1895 [01:54<00:00, 16.56it/s]\n",
            "Processing three: 100%|██████████| 1851/1851 [02:08<00:00, 14.40it/s]\n",
            "Processing tree: 100%|██████████| 1374/1374 [01:16<00:00, 17.89it/s]\n",
            "Processing two: 100%|██████████| 1873/1873 [01:48<00:00, 17.26it/s]\n",
            "Processing up: 100%|██████████| 1857/1857 [01:53<00:00, 16.42it/s]\n",
            "Processing wow: 100%|██████████| 1414/1414 [01:21<00:00, 17.36it/s]\n",
            "Processing yes: 100%|██████████| 1860/1860 [01:48<00:00, 17.19it/s]\n",
            "Processing zero: 100%|██████████| 1866/1866 [01:50<00:00, 16.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing valid data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing bed: 100%|██████████| 197/197 [00:10<00:00, 19.23it/s]\n",
            "Processing bird: 100%|██████████| 162/162 [00:05<00:00, 28.66it/s]\n",
            "Processing cat: 100%|██████████| 168/168 [00:06<00:00, 26.61it/s]\n",
            "Processing dog: 100%|██████████| 170/170 [00:05<00:00, 28.88it/s]\n",
            "Processing down: 100%|██████████| 264/264 [00:10<00:00, 24.79it/s]\n",
            "Processing eight: 100%|██████████| 243/243 [00:08<00:00, 27.41it/s]\n",
            "Processing five: 100%|██████████| 242/242 [00:09<00:00, 24.46it/s]\n",
            "Processing four: 100%|██████████| 280/280 [00:11<00:00, 25.15it/s]\n",
            "Processing go: 100%|██████████| 260/260 [00:09<00:00, 27.26it/s]\n",
            "Processing happy: 100%|██████████| 189/189 [00:07<00:00, 24.57it/s]\n",
            "Processing house: 100%|██████████| 173/173 [00:06<00:00, 28.19it/s]\n",
            "Processing left: 100%|██████████| 247/247 [00:09<00:00, 25.29it/s]\n",
            "Processing marvin: 100%|██████████| 160/160 [00:06<00:00, 26.22it/s]\n",
            "Processing nine: 100%|██████████| 230/230 [00:08<00:00, 26.97it/s]\n",
            "Processing no: 100%|██████████| 270/270 [00:11<00:00, 24.16it/s]\n",
            "Processing off: 100%|██████████| 256/256 [00:09<00:00, 26.24it/s]\n",
            "Processing on: 100%|██████████| 257/257 [00:13<00:00, 18.67it/s]\n",
            "Processing one: 100%|██████████| 230/230 [00:10<00:00, 20.96it/s]\n",
            "Processing right: 100%|██████████| 256/256 [00:09<00:00, 27.06it/s]\n",
            "Processing seven: 100%|██████████| 263/263 [00:10<00:00, 25.23it/s]\n",
            "Processing sheila: 100%|██████████| 176/176 [00:06<00:00, 28.97it/s]\n",
            "Processing six: 100%|██████████| 262/262 [00:10<00:00, 23.82it/s]\n",
            "Processing stop: 100%|██████████| 246/246 [00:09<00:00, 27.22it/s]\n",
            "Processing three: 100%|██████████| 248/248 [00:09<00:00, 24.85it/s]\n",
            "Processing tree: 100%|██████████| 166/166 [00:05<00:00, 28.19it/s]\n",
            "Processing two: 100%|██████████| 236/236 [00:10<00:00, 23.42it/s]\n",
            "Processing up: 100%|██████████| 260/260 [00:10<00:00, 25.59it/s]\n",
            "Processing wow: 100%|██████████| 166/166 [00:06<00:00, 24.27it/s]\n",
            "Processing yes: 100%|██████████| 261/261 [00:12<00:00, 20.36it/s]\n",
            "Processing zero: 100%|██████████| 260/260 [00:11<00:00, 23.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing test data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing bed: 100%|██████████| 176/176 [00:10<00:00, 17.24it/s]\n",
            "Processing bird: 100%|██████████| 158/158 [00:06<00:00, 25.89it/s]\n",
            "Processing cat: 100%|██████████| 166/166 [00:06<00:00, 27.39it/s]\n",
            "Processing dog: 100%|██████████| 180/180 [00:06<00:00, 26.34it/s]\n",
            "Processing down: 100%|██████████| 253/253 [00:10<00:00, 25.00it/s]\n",
            "Processing eight: 100%|██████████| 257/257 [00:09<00:00, 27.06it/s]\n",
            "Processing five: 100%|██████████| 271/271 [00:11<00:00, 23.08it/s]\n",
            "Processing four: 100%|██████████| 253/253 [00:09<00:00, 25.72it/s]\n",
            "Processing go: 100%|██████████| 251/251 [00:10<00:00, 24.25it/s]\n",
            "Processing happy: 100%|██████████| 180/180 [00:07<00:00, 24.06it/s]\n",
            "Processing house: 100%|██████████| 150/150 [00:06<00:00, 24.07it/s]\n",
            "Processing left: 100%|██████████| 267/267 [00:13<00:00, 20.13it/s]\n",
            "Processing marvin: 100%|██████████| 162/162 [00:05<00:00, 28.15it/s]\n",
            "Processing nine: 100%|██████████| 259/259 [00:10<00:00, 25.25it/s]\n",
            "Processing no: 100%|██████████| 252/252 [00:10<00:00, 24.29it/s]\n",
            "Processing off: 100%|██████████| 262/262 [00:10<00:00, 24.10it/s]\n",
            "Processing on: 100%|██████████| 246/246 [00:09<00:00, 25.78it/s]\n",
            "Processing one: 100%|██████████| 248/248 [00:10<00:00, 23.18it/s]\n",
            "Processing right: 100%|██████████| 259/259 [00:10<00:00, 24.59it/s]\n",
            "Processing seven: 100%|██████████| 239/239 [00:09<00:00, 25.46it/s]\n",
            "Processing sheila: 100%|██████████| 186/186 [00:07<00:00, 26.48it/s]\n",
            "Processing six: 100%|██████████| 244/244 [00:11<00:00, 21.62it/s]\n",
            "Processing stop: 100%|██████████| 249/249 [00:12<00:00, 19.49it/s]\n",
            "Processing three: 100%|██████████| 267/267 [00:10<00:00, 25.36it/s]\n",
            "Processing tree: 100%|██████████| 193/193 [00:08<00:00, 23.99it/s]\n",
            "Processing two: 100%|██████████| 264/264 [00:10<00:00, 25.84it/s]\n",
            "Processing up: 100%|██████████| 272/272 [00:10<00:00, 25.72it/s]\n",
            "Processing wow: 100%|██████████| 165/165 [00:06<00:00, 26.70it/s]\n",
            "Processing yes: 100%|██████████| 256/256 [00:10<00:00, 23.80it/s]\n",
            "Processing zero: 100%|██████████| 250/250 [00:18<00:00, 13.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Preprocessing complete!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Parameters from the research paper\n",
        "SAMPLE_RATE = 16000\n",
        "N_MELS = 40\n",
        "N_FFT = 400  # Window size (30ms)\n",
        "HOP_LENGTH = 160  # Hop size (10ms)\n",
        "\n",
        "# MelSpectrogram transformation using torchaudio\n",
        "mel_transform = T.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=N_FFT,\n",
        "    hop_length=HOP_LENGTH,\n",
        "    n_mels=N_MELS,\n",
        "    f_min=0.0,\n",
        "    f_max=None,\n",
        "    power=2.0,\n",
        ")\n",
        "\n",
        "# Paths\n",
        "DATA_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/data\"\n",
        "DATA_SPLITS = [\"train\", \"valid\", \"test\"]\n",
        "\n",
        "# Function to process audio into Mel-Spectrogram using torchaudio\n",
        "def process_audio(file_path):\n",
        "    waveform, sr = torchaudio.load(file_path)  # Load audio\n",
        "    if sr != SAMPLE_RATE:\n",
        "        waveform = torchaudio.functional.resample(waveform, sr, SAMPLE_RATE)  # Resample if needed\n",
        "    mel_spec = mel_transform(waveform).squeeze(0)  # Convert to Mel Spectrogram and remove extra channel\n",
        "    mel_spec_db = torch.log(mel_spec + 1e-6).numpy()  # Convert to log scale (avoid log(0) errors)\n",
        "    return mel_spec_db\n",
        "\n",
        "# Iterate over splits and process files\n",
        "for split in DATA_SPLITS:\n",
        "    input_folder = os.path.join(DATA_PATH, split)\n",
        "    output_folder = os.path.join(DATA_PATH, f\"{split}_processed\")\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    print(f\"Processing {split} data...\")\n",
        "\n",
        "    for keyword in os.listdir(input_folder):\n",
        "        keyword_path = os.path.join(input_folder, keyword)\n",
        "        if not os.path.isdir(keyword_path):\n",
        "            continue  # Skip non-folder files\n",
        "\n",
        "        for filename in tqdm(os.listdir(keyword_path), desc=f\"Processing {keyword}\"):\n",
        "            file_path = os.path.join(keyword_path, filename)\n",
        "            if not file_path.endswith(\".wav\"):\n",
        "                continue  # Skip non-audio files\n",
        "\n",
        "            try:\n",
        "                mel_spec = process_audio(file_path)\n",
        "                save_filename = f\"{keyword}_{filename.replace('.wav', '.npy')}\"  # Add keyword to filename\n",
        "                save_path = os.path.join(output_folder, save_filename)\n",
        "\n",
        "                np.save(save_path, mel_spec)\n",
        "               # print(f\"✅ Processed: {file_path} → {save_path}\")  # Debugging print\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing {file_path}: {e}\")\n",
        "\n",
        "print(\"✅ Preprocessing complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME1Kxi2W624C"
      },
      "source": [
        "**Verifying the spectrograms**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVV6Zrsq6cVP",
        "outputId": "26be8ff5-5c73-4c5c-dde3-61db1494f103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Spectrograms: 51318\n",
            "Validation Spectrograms: 6798\n",
            "Test Spectrograms: 6835\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/Keyword_spotting_transformer/data\"\n",
        "\n",
        "print(\"Train Spectrograms:\", len(os.listdir(f\"{data_path}/train_processed\")))\n",
        "print(\"Validation Spectrograms:\", len(os.listdir(f\"{data_path}/valid_processed\")))\n",
        "print(\"Test Spectrograms:\", len(os.listdir(f\"{data_path}/test_processed\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To patch the spectrograms**"
      ],
      "metadata": {
        "id": "OOnXKHGMHWgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/train_processed\"\n",
        "\n",
        "labels = set()\n",
        "for file in os.listdir(DATA_PATH):\n",
        "    if file.endswith(\".npy\"):\n",
        "        label = file.split(\"_\")[0]  # Extracts the word label from filename\n",
        "        labels.add(label)\n",
        "\n",
        "print(\"Available Labels:\", labels)\n",
        "print(\"Total Labels:\", len(labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoBT-kQ3OdfI",
        "outputId": "88d8f32b-d037-4f17-993d-318d011797c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Labels: {'two', 'tree', 'nine', 'six', 'right', 'three', 'no', 'yes', 'one', 'stop', 'wow', 'off', 'sheila', 'bed', 'go', 'seven', 'cat', 'four', 'on', 'eight', 'bird', 'down', 'marvin', 'house', 'happy', 'dog', 'five', 'left', 'zero', 'up'}\n",
            "Total Labels: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Define the fixed spectrogram size (mel-bands x time-frames)\n",
        "N_MELS = 40  # Mel-frequency bands (height)\n",
        "FIXED_TIME_DIM = 101  # Fixed width for spectrograms (adjust based on dataset)\n",
        "\n",
        "# Function to pad or crop spectrograms to a fixed size\n",
        "def pad_or_crop_spectrogram(spectrogram, fixed_size=FIXED_TIME_DIM):\n",
        "    \"\"\"Pads with zeros or crops spectrogram to a fixed time size\"\"\"\n",
        "    time_dim = spectrogram.shape[1]  # Get the time dimension\n",
        "\n",
        "    if time_dim < fixed_size:\n",
        "        # Pad with zeros if too short\n",
        "        pad_width = fixed_size - time_dim\n",
        "        spectrogram = np.pad(spectrogram, ((0, 0), (0, pad_width)), mode=\"constant\")\n",
        "    elif time_dim > fixed_size:\n",
        "        # Crop if too long\n",
        "        spectrogram = spectrogram[:, :fixed_size]\n",
        "\n",
        "    return spectrogram\n",
        "\n",
        "# Custom Dataset class for Keyword Spotting\n",
        "class KeywordDataset(Dataset):\n",
        "    def __init__(self, spectrogram_dir, label_dict):\n",
        "        \"\"\"\n",
        "        spectrogram_dir: Path to processed spectrogram .npy files\n",
        "        label_dict: Dictionary mapping word labels to numerical values\n",
        "        \"\"\"\n",
        "        self.spectrogram_files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Scan the directory for spectrograms\n",
        "        for filename in os.listdir(spectrogram_dir):\n",
        "            if filename.endswith(\".npy\"):\n",
        "                word = filename.split(\"_\")[0]  # Extract the keyword label\n",
        "                if word in label_dict:\n",
        "                    self.spectrogram_files.append(os.path.join(spectrogram_dir, filename))\n",
        "                    self.labels.append(label_dict[word])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spectrogram_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load spectrogram\n",
        "        spectrogram = np.load(self.spectrogram_files[idx])\n",
        "        spectrogram = pad_or_crop_spectrogram(spectrogram)  # Ensure fixed size\n",
        "        spectrogram = torch.tensor(spectrogram, dtype=torch.float32)\n",
        "\n",
        "        # Load label\n",
        "        label = self.labels[idx]\n",
        "        return spectrogram, label\n",
        "\n",
        "# Define label dictionary (ensure it matches your selected classes)\n",
        "label_dict = {\n",
        "    'bed':0, 'bird':1, 'cat':2, 'dog':3, 'down':4,\n",
        "    'eight':5, 'five':6, 'four':7, 'go':8, 'happy':9,\n",
        "    'house':10, 'left':11, 'marvin':12, 'nine':13, 'no':14,\n",
        "    'off':15, 'on':16, 'one':17, 'right':18, 'seven':19,\n",
        "    'sheila':20, 'six':21, 'stop':22, 'three':23, 'tree':24,\n",
        "    'two':25, 'up':26, 'wow':27, 'yes':28,'zero':29\n",
        "}\n",
        "\n",
        "# Paths to preprocessed spectrograms\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/train_processed\"\n",
        "VALID_DIR = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/valid_processed\"\n",
        "TEST_DIR = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/test_processed\"\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = KeywordDataset(TRAIN_DIR, label_dict)\n",
        "valid_dataset = KeywordDataset(VALID_DIR, label_dict)\n",
        "test_dataset = KeywordDataset(TEST_DIR, label_dict)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "# Test DataLoader\n",
        "for spectrograms, labels in train_loader:\n",
        "    print(\"Spectrogram Shape:\", spectrograms.shape)  # Expected: (batch_size, 40, 101)\n",
        "    print(\"Labels:\", labels)\n",
        "    break  # Only print one batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn7SHnjTQGU6",
        "outputId": "80ac292d-53ad-4735-c96d-589cd3bc47e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spectrogram Shape: torch.Size([32, 40, 101])\n",
            "Labels: tensor([18,  5, 19, 24, 28,  6, 14, 22, 17, 27, 14, 10, 18, 28, 29, 23,  4,  6,\n",
            "         0, 19,  3,  2,  6, 29, 26, 25, 15, 14, 22,  8, 12, 19])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try loading a single batch\n",
        "data_iter = iter(train_loader)\n",
        "try:\n",
        "    spectrograms, labels = next(data_iter)\n",
        "    print(\" Successfully loaded a batch!\")\n",
        "    print(\"Spectrogram Shape:\", spectrograms.shape)\n",
        "    print(\"Labels:\", labels)\n",
        "except Exception as e:\n",
        "    print(\" Error in DataLoader:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ULnt8yJOit2",
        "outputId": "72dd277e-1c80-4194-da6f-f2d03115edb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully loaded a batch!\n",
            "Spectrogram Shape: torch.Size([32, 40, 101])\n",
            "Labels: tensor([28,  0, 26,  5, 20,  8, 24,  4, 13, 15,  7, 22, 11,  3, 25, 16, 22,  2,\n",
            "        29, 23, 19, 24, 13, 14,  0, 20,  8,  2, 10, 28, 13, 11])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtWDuNMp6_rc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5c2d5dc-a67c-4003-e99b-e1b9f1ee1fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched Shape: torch.Size([32, 7, 640])\n",
            "Embedded Shape: torch.Size([32, 7, 128])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# === Hyperparameters ===\n",
        "PATCH_SIZE = 16  # Patch size along time axis\n",
        "EMBED_DIM = 128  # Embedding dimension for transformer input\n",
        "N_MELS = 40      # Mel spectrogram height\n",
        "\n",
        "# === 1. Function to Split into Patches ===\n",
        "def split_into_patches(spectrograms, patch_size=PATCH_SIZE):\n",
        "    \"\"\"\n",
        "    Splits spectrograms into non-overlapping patches along the time axis.\n",
        "\n",
        "    Args:\n",
        "        spectrograms (torch.Tensor): (batch_size, n_mels, time_frames)\n",
        "        patch_size (int): Size of each patch along the time axis.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: (batch_size, num_patches, n_mels * patch_size)\n",
        "    \"\"\"\n",
        "    batch_size, n_mels, time_frames = spectrograms.shape\n",
        "\n",
        "    # Pad time axis if it's not divisible by patch_size\n",
        "    pad_len = (patch_size - (time_frames % patch_size)) % patch_size\n",
        "    spectrograms_padded = F.pad(spectrograms, (0, pad_len))  # Pad along time axis\n",
        "\n",
        "    # Reshape into patches: (batch, n_mels, num_patches, patch_size)\n",
        "    num_patches = spectrograms_padded.shape[-1] // patch_size\n",
        "    patches = spectrograms_padded.unfold(dimension=2, size=patch_size, step=patch_size)\n",
        "\n",
        "    # Flatten patches: (batch, num_patches, n_mels * patch_size)\n",
        "    patches = patches.permute(0, 2, 1, 3).reshape(batch_size, num_patches, -1)\n",
        "\n",
        "    return patches\n",
        "\n",
        "# === 2. Linear Projection Layer ===\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Projects patches into an embedding space and adds positional embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_dim, embed_dim, max_patches=64):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Linear(patch_dim, embed_dim)  # Linear mapping\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, max_patches, embed_dim))  # Learnable position embeddings\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch_size, num_patches, patch_dim)\n",
        "        Returns: (batch_size, num_patches, embed_dim)\n",
        "        \"\"\"\n",
        "        x = self.projection(x)  # Apply linear projection\n",
        "        x += self.positional_embedding[:, :x.shape[1], :]  # Add positional embedding\n",
        "        return x\n",
        "\n",
        "# === Example Usage ===\n",
        "batch_size = 32\n",
        "time_frames = 101  # Example time dimension\n",
        "\n",
        "# Create dummy spectrogram batch (32, 40, 101)\n",
        "dummy_spectrograms = torch.randn(batch_size, N_MELS, time_frames)\n",
        "\n",
        "# Split into patches\n",
        "patched_spectrograms = split_into_patches(dummy_spectrograms, PATCH_SIZE)\n",
        "\n",
        "# Initialize Patch Embedding layer\n",
        "patch_embedding_layer = PatchEmbedding(N_MELS * PATCH_SIZE, EMBED_DIM)\n",
        "\n",
        "# Apply Patch Embedding Layer\n",
        "embedded_patches = patch_embedding_layer(patched_spectrograms)\n",
        "\n",
        "print(\"Patched Shape:\", patched_spectrograms.shape)  # (32, num_patches, patch_size * 40)\n",
        "print(\"Embedded Shape:\", embedded_patches.shape)  # (32, num_patches, EMBED_DIM)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single Transformer encoder block with MHSA, LayerNorm, and FeedForward layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, expansion_factor=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim * expansion_factor),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim * expansion_factor, embed_dim),\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through a Transformer encoder block.\n",
        "        Args:\n",
        "            x: (batch_size, num_patches, embed_dim)\n",
        "        Returns:\n",
        "            x: (batch_size, num_patches, embed_dim)\n",
        "        \"\"\"\n",
        "        # Multi-Head Self Attention\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_output))  # Add & Norm\n",
        "\n",
        "        # Feed Forward Network\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_output))  # Add & Norm\n",
        "\n",
        "        return x\n",
        "\n",
        "class KeywordTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Keyword Transformer (KWT) model for keyword spotting.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim=128, num_heads=4, num_layers=6, num_classes=10, patch_dim=640, max_patches=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch Embedding\n",
        "        self.patch_embedding = nn.Linear(patch_dim, embed_dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(1, max_patches, embed_dim))\n",
        "\n",
        "        # Transformer Encoder\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, dropout=dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Classification Head\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through KWT model.\n",
        "        Args:\n",
        "            x: (batch_size, num_patches, patch_dim)\n",
        "        Returns:\n",
        "            logits: (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        x = self.patch_embedding(x)  # Project patches to embedding space\n",
        "        x += self.positional_embedding[:, :x.shape[1], :]  # Add positional encoding\n",
        "\n",
        "        # Add CLS Token\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        # Pass through Transformer Encoder\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Classification: Use CLS token output\n",
        "        cls_output = x[:, 0, :]\n",
        "        logits = self.mlp_head(cls_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# === Example Usage ===\n",
        "batch_size = 32\n",
        "num_patches = 6  # Example number of patches\n",
        "patch_dim = 640  # 40x16 flattened\n",
        "\n",
        "dummy_patches = torch.randn(batch_size, num_patches, patch_dim)  # Fake input data\n",
        "kwt_model = KeywordTransformer(embed_dim=128, num_heads=4, num_layers=6, num_classes=10)\n",
        "\n",
        "logits = kwt_model(dummy_patches)  # Forward pass\n",
        "print(\"Output Shape:\", logits.shape)  # Expected: (batch_size, num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a019eneEK72o",
        "outputId": "8642399b-9317-43a7-941e-5360e7ae4471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Shape: torch.Size([32, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class KeywordDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset to load spectrogram patches from train_processed, valid_processed, and test_processed.\n",
        "    \"\"\"\n",
        "    def __init__(self, spectrogram_dir, label_dict):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            spectrogram_dir: Path to folder containing .npy spectrograms.\n",
        "            label_dict: Dictionary mapping keywords to label indices.\n",
        "        \"\"\"\n",
        "        self.spectrogram_files = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Check if the directory exists and has files\n",
        "        if not os.path.exists(spectrogram_dir):\n",
        "            raise FileNotFoundError(f\"Directory not found: {spectrogram_dir}\")\n",
        "        if not os.listdir(spectrogram_dir):\n",
        "             raise FileNotFoundError(f\"Directory is empty: {spectrogram_dir}\")\n",
        "\n",
        "\n",
        "        # Scan the directory and collect spectrogram file paths and labels\n",
        "        for filename in os.listdir(spectrogram_dir):\n",
        "            if filename.endswith(\".npy\"):\n",
        "                word = filename.split(\"_\")[0]  # Extract keyword from filename\n",
        "                if word in label_dict:\n",
        "                    self.spectrogram_files.append(os.path.join(spectrogram_dir, filename))\n",
        "                    self.labels.append(label_dict[word])  # Convert word to numerical label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spectrogram_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and returns a spectrogram tensor and its corresponding label.\n",
        "        \"\"\"\n",
        "        spectrogram = np.load(self.spectrogram_files[idx])  # Load spectrogram\n",
        "        spectrogram = torch.tensor(spectrogram, dtype=torch.float32)  # Convert to tensor\n",
        "        label = self.labels[idx]\n",
        "        return spectrogram, label  # Return spectrogram & label tensors\n",
        "\n",
        "\n",
        "# Define label mapping\n",
        "label_dict = {\n",
        "   'bed':0, 'bird':1, 'cat':2, 'dog':3, 'down':4,\n",
        "    'eight':5, 'five':6, 'four':7, 'go':8, 'happy':9,\n",
        "    'house':10, 'left':11, 'marvin':12, 'nine':13, 'no':14,\n",
        "    'off':15, 'on':16, 'one':17, 'right':18, 'seven':19,\n",
        "    'sheila':20, 'six':21, 'stop':22, 'three':23, 'tree':24,\n",
        "    'two':25, 'up':26, 'wow':27, 'yes':28,'zero':29\n",
        "}\n",
        "\n",
        "# Define dataset paths\n",
        "TRAIN_DIR = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/train_processed\"\n",
        "VALID_DIR = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/valid_processed\"\n",
        "\n",
        "# Create dataset instances\n",
        "try:\n",
        "    print(f\"Checking {TRAIN_DIR}\")\n",
        "    train_dataset = KeywordDataset(TRAIN_DIR, label_dict)\n",
        "    print(f\"Number of files in training dataset: {len(train_dataset)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating training dataset: {e}\")\n",
        "    raise\n",
        "try:\n",
        "    print(f\"Checking {VALID_DIR}\")\n",
        "    valid_dataset = KeywordDataset(VALID_DIR, label_dict)\n",
        "    print(f\"Number of files in validation dataset: {len(valid_dataset)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating validation dataset: {e}\")\n",
        "    raise\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0y_k8V5SONZ",
        "outputId": "450831a1-902f-43ae-b8b4-702f1266b562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking /content/drive/MyDrive/Keyword_spotting_transformer/data/train_processed\n",
            "Number of files in training dataset: 51318\n",
            "Checking /content/drive/MyDrive/Keyword_spotting_transformer/data/valid_processed\n",
            "Number of files in validation dataset: 6798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, n, _, h = *x.shape, self.heads\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), qkv)\n",
        "\n",
        "        dots = torch.einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        out = torch.einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        return self.to_out(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "            ]) for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x\n",
        "            x = ff(x) + x\n",
        "        return x\n",
        "\n",
        "class KeywordTransformer(nn.Module):\n",
        "    def __init__(self, *, spectrogram_size, patch_size, num_classes, dim, depth, heads, mlp_dim, dropout=0.1, emb_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.spectrogram_size = spectrogram_size\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Calculate the number of patches based on the *padded* size\n",
        "        max_time_frames = spectrogram_size[1]\n",
        "\n",
        "        # Calculate the maximum possible number of patches\n",
        "        num_patches = (max_time_frames + (patch_size[1] - 1)) // patch_size[1] # ceiling division\n",
        "\n",
        "        patch_dim = spectrogram_size[0] * patch_size[1]  # (40 * patch_size)\n",
        "\n",
        "        self.to_patch_embedding = nn.Sequential(\n",
        "            # Rearrange('b c f (t p2) -> b t (c f p2)', p2=patch_size[1]),  # Keep 'c' in the output\n",
        "            nn.Linear(patch_dim, dim),\n",
        "        )\n",
        "        self.rearrange = Rearrange('b c f (t p2) -> b t (c f p2)', p2=patch_size[1]) # Rearrange layer\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
        "        self.dropout = nn.Dropout(emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head=dim // heads, mlp_dim=mlp_dim, dropout=dropout)\n",
        "\n",
        "        self.mlp_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "         # Pad the time dimension if it's not divisible by patch_size\n",
        "        b, c, f, t = x.shape\n",
        "        pad_len = (self.patch_size[1] - (t % self.patch_size[1])) % self.patch_size[1]\n",
        "        x = F.pad(x, (0, pad_len))  # Pad along time axis\n",
        "\n",
        "        x = self.rearrange(x) # Rearrange after padding\n",
        "        x = self.to_patch_embedding(x)\n",
        "\n",
        "        b, n, _ = x.shape\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embedding[:, :(n + 1)] # positional embedding scaled by the patch embedding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        x = x[:, 0]  # CLS token output\n",
        "        return self.mlp_head(x)\n",
        "\n",
        "\n",
        "# === Example Usage ===\n",
        "model = KeywordTransformer(\n",
        "    spectrogram_size=(40, 101),  # (Mel bands, time frames)\n",
        "    patch_size=(40, 16),  # Only split along time\n",
        "    num_classes=30,  # Based on label_dict\n",
        "    dim=128,\n",
        "    depth=6,\n",
        "    heads=8,\n",
        "    mlp_dim=256,\n",
        "    dropout=0.1,\n",
        "    emb_dropout=0.1\n",
        ")\n",
        "\n",
        "dummy_input = torch.randn(1, 1, 40, 101)  # (batch, channels, mel, time)\n",
        "output = model(dummy_input)\n",
        "print(\"Output Shape:\", output.shape)  # Expected: (1, 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80w4PSVDGtFy",
        "outputId": "190f63a9-5d3c-4b43-d21f-c28647db6d1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output Shape: torch.Size([1, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Keyword_spotting_transformer/data/train_augmented_pinknoise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57g5OC0ogB4s",
        "outputId": "7d29841a-b1db-418e-9939-1971ad843622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Keyword_spotting_transformer/data/train_augmented_pinknoise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Example path (adjust if needed)\n",
        "sample_path = \"/content/drive/MyDrive/Keyword_spotting_transformer/data/train_augmented_pinknoise/zero_xyz123.npy\"\n",
        "\n",
        "# Load and inspect\n",
        "sample = np.load(sample_path)\n",
        "print(\"Shape:\", sample.shape)\n",
        "print(\"Dtype:\", sample.dtype)\n",
        "print(\"Min value:\", np.min(sample))\n",
        "print(\"Max value:\", np.max(sample))\n"
      ],
      "metadata": {
        "id": "m_VTOCUYaugu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "aa345dee-1262-475f-e7f1-76ac50b234d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Keyword_spotting_transformer/data/train_augmented_pinknoise/zero_xyz123.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-62255c21ca96>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load and inspect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dtype:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Keyword_spotting_transformer/data/train_augmented_pinknoise/zero_xyz123.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-r-icXMjf43q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FTQoxK2svB-QieqhIQvCurI5aUunHled",
      "authorship_tag": "ABX9TyMJbfJ4QbNnoJ29IUEQP5l1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}